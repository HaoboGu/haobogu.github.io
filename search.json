[{"categories":null,"content":" B站视频教程链接：https://www.bilibili.com/video/BV1G5411C7mu\nRust + RTOS 概况 在Rust中，RTOS分为两类：\n基于C bindings的RTOS zephyr: https://github.com/tylerwhall/zephyr-rust FreeRTOS: https://github.com/lobaro/FreeRTOS-rust Pure Rust 偏传统的RTOS https://github.com/tock/tock https://github.com/oxidecomputer/hubris 异步运行时 👍 https://github.com/embassy-rs/embassy https://github.com/rtic-rs/rtic Rust异步编程 简介 async 编程入门 - Rust语言圣经(Rust Course)\n异步运行时 embassy-rs Embassy\n示例代码（点灯 + button） main.rs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 #![no_std] #![no_main] use defmt::*; use embassy_executor::Spawner; use embassy_stm32::{ exti::ExtiInput, gpio::{AnyPin, Input, Level, Output, Pin, Pull}, }; use embassy_time::Timer; use {defmt_rtt as _, panic_probe as _}; // Declare async tasks #[embassy_executor::task] async fn blink(pin: AnyPin) { let mut led = Output::new(pin, Level::Low, embassy_stm32::gpio::Speed::High); loop { // Timekeeping is globally available, no need to mess with hardware timers. led.set_high(); Timer::after_millis(150).await; led.set_low(); Timer::after_millis(150).await; } } // Main is itself an async task as well. #[embassy_executor::main] async fn main(spawner: Spawner) { // Initialize the embassy-stm32 HAL. let p = embassy_stm32::init(Default::default()); // Spawned tasks run in the background, concurrently. spawner.spawn(blink(p.PE3.degrade())).unwrap(); let mut button = ExtiInput::new(Input::new(p.PC13, Pull::Down), p.EXTI13); loop { // Asynchronously wait for GPIO events, allowing other tasks // to run, or the core to sleep. button.wait_for_rising_edge().await; info!(\"Button pressed!\"); button.wait_for_falling_edge().await; info!(\"Button released!\"); } } build.rs:\n1 2 3 4 5 fn main() { println!(\"cargo:rustc-link-arg-bins=--nmagic\"); println!(\"cargo:rustc-link-arg-bins=-Tlink.x\"); println!(\"cargo:rustc-link-arg-bins=-Tdefmt.x\"); } .cargo/config.toml\n1 2 3 4 5 6 7 8 [target.thumbv7em-none-eabihf] runner = \"probe-rs run --chip STM32H7B0VBTx\" [build] target = \"thumbv7em-none-eabihf\" [env] DEFMT_LOG = \"info\" Cargo.toml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 [package] edition = \"2021\" name = \"embassy-stm32h7-examples\" version = \"0.1.0\" license = \"MIT OR Apache-2.0\" [dependencies] embassy-stm32 = { version = \"0.1.0\", features = [\"defmt\", \"stm32h7b0vb\", \"time-driver-any\", \"exti\", \"memory-x\", \"unstable-pac\", \"chrono\"] } embassy-executor = { version = \"0.5.0\", features = [\"task-arena-size-32768\", \"arch-cortex-m\", \"executor-thread\", \"defmt\", \"integrated-timers\"] } embassy-time = { version = \"0.3.0\", features = [\"defmt\", \"defmt-timestamp-uptime\", \"tick-hz-32_768\"] } defmt = \"0.3\" defmt-rtt = \"0.4\" cortex-m = { version = \"0.7.6\", features = [\"inline-asm\", \"critical-section-single-core\"] } cortex-m-rt = \"0.7.0\" panic-probe = { version = \"0.3\", features = [\"print-defmt\"] } # cargo build/run [profile.dev] codegen-units = 1 debug = 2 debug-assertions = true # \u003c- incremental = false opt-level = 3 # \u003c- overflow-checks = true # \u003c- # cargo build/run --release [profile.release] codegen-units = 1 debug = 2 debug-assertions = false # \u003c- incremental = false lto = 'fat' opt-level = 3 # \u003c- overflow-checks = false # \u003c- ","description":"","tags":["rust"],"title":"Rust嵌入式教程（7）- RTOS概况","uri":"/posts/keyboard/7-%E5%BC%82%E6%AD%A5rust-rtos/"},{"categories":null,"content":" B站视频教程链接：https://www.bilibili.com/video/BV1pb4y1A72W\n安装工具 cargo-binutil https://github.com/rust-embedded/cargo-binutils\n1 2 cargo install cargo-binutils rustup component add llvm-tools cargo-bloat 1 cargo install cargo-bloat 使用 --release 默认cargo build是debug模式，debug信息会占用大量固件内容。打包生产需要使用 --release。\n1 cargo build --release 下面的所有命令，都可以加上 --release。如果不加，默认都是对debug模式下的固件执行相关命令。\n查看固件大小 默认生成的固件是 elf 文件，其大小不能真实地反映固件大小。\n查看固件/text/data/bss段的大小 1 cargo size 查看每个section的大小 1 2 3 4 cargo size -- -A # 十六进制展示 cargo size -- -A -x 配置 Cargo.toml 文档：https://doc.rust-lang.org/cargo/reference/profiles.html#profiles\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [profile.dev] codegen-units = 1 # better optimizations debug = true opt-level = 1 overflow-checks = true lto = false panic = 'unwind' [profile.release] codegen-units = 1 # better optimizations debug = true # no overhead for bare-metal opt-level = \"z\" # optimize for binary size opt-level = \"s\" # overflow-checks = false lto = \"fat 分析固件大小 1 cargo bloat --release 使用 nm 分析\n1 cargo nm --release -- --print-size --size-sort 编译生成 .bin 文件 下面的命令会自动重新编译工程，并且生成 .bin 文件\n1 cargo objcopy --release -- -O binary rust-embedded-demo.bin 在tasks.json里面配置如下：\n1 2 3 4 5 6 7 8 9 10 11 { \"label\": \"build .bin firmware\", \"type\": \"shell\", \"command\": \"cargo objcopy --release -- -O binary \\\"${workspaceFolder}/target/release/rust-embedded-demo.bin\\\"\", \"problemMatcher\": [ \"$rustc\" ], \"group\": { \"kind\": \"build\", }, }, 注意：bin 无法调试。最好 elf/bin 文件都生成出来\n","description":"","tags":["rust"],"title":"Rust嵌入式教程（6）- 优化固件体积","uri":"/posts/keyboard/6%E4%BC%98%E5%8C%96%E5%9B%BA%E4%BB%B6%E4%BD%93%E7%A7%AF/"},{"categories":null,"content":" B站视频教程链接：https://www.bilibili.com/video/BV1nN41177ct\n相关链接 embedded-hal 文档：https://docs.rs/embedded-hal/latest/embedded_hal embedded-hal 代码仓库：https://github.com/rust-embedded/embedded-hal 示例代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 #![no_main] #![no_std] mod rtt_logger; use cortex_m_rt::entry; use log::{debug, error, info, warn}; use panic_rtt_target as _; use stm32h7xx_hal::{ hal::digital::v2::{InputPin, OutputPin}, pac, prelude::*, }; fn control_led\u003cIn: InputPin\u003cError = E\u003e, Out: OutputPin\u003cError = E\u003e, E\u003e( input_pin: \u0026In, led_pin: \u0026mut Out, ) -\u003e Result\u003c(), E\u003e { if input_pin.is_high()? { info!(\"high\"); led_pin.set_high()?; } else { info!(\"low\"); led_pin.set_low()?; } Ok(()) } #[entry] fn main() -\u003e ! { rtt_logger::init(log::LevelFilter::Debug); // 获取cortex核心外设和stm32h7的所有外设 let cp = cortex_m::Peripherals::take().unwrap(); let dp = pac::Peripherals::take().unwrap(); // Power 设置 let pwr = dp.PWR.constrain(); let pwrcfg = pwr.freeze(); // 初始化RCC let rcc = dp.RCC.constrain(); let ccdr = rcc.sys_ck(200.MHz()).freeze(pwrcfg, \u0026dp.SYSCFG); // 设置LED对应的GPIO let gpioe = dp.GPIOE.split(ccdr.peripheral.GPIOE); let mut led = gpioe.pe3.into_push_pull_output(); let gpioc = dp.GPIOC.split(ccdr.peripheral.GPIOC); let key = gpioc.pc13.into_pull_down_input(); // cortex-m已经实现好了delay函数，直接拿到，下面使用 let mut delay = cp.SYST.delay(ccdr.clocks); loop { // 点灯 control_led(\u0026key, \u0026mut led).unwrap(); // 延时50ms delay.delay_ms(50_u16); } } ","description":"","tags":["rust"],"title":"Rust嵌入式教程（5）- embedded-hal \u0026 gpio点灯","uri":"/posts/keyboard/5embedded-hal-gpio/"},{"categories":null,"content":" B站视频教程链接：https://www.bilibili.com/video/BV14N4y1U7XE\n必选插件 rust analyzer cortex debug VSCode配置 .vscode/tasks.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 { // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format \"version\": \"2.0.0\", \"tasks\": [ { /* * This is the default cargo build task, * but we need to provide a label for it, * so we can invoke it from the debug launcher. */ \"label\": \"Cargo Build (debug)\", \"type\": \"process\", \"command\": \"cargo\", \"args\": [\"build\"], \"problemMatcher\": [ \"$rustc\" ], \"group\": { \"kind\": \"build\", } }, { \"label\": \"Cargo Build (release)\", \"type\": \"process\", \"command\": \"cargo\", \"args\": [\"build\", \"--release\"], \"problemMatcher\": [ \"$rustc\" ], \"group\": \"build\" }, { \"label\": \"Flash\", \"type\": \"shell\", \"command\": \"openocd -f ${workspaceFolder}/openocd.cfg -c \\\"program target/thumbv7em-none-eabihf/debug/rust-embedded-demo preverify verify reset exit\\\"\", \"dependsOn\": [ \"Cargo Build (debug)\" ], \"group\": \"build\" }, { \"label\": \"Cargo Clean\", \"type\": \"process\", \"command\": \"cargo\", \"args\": [\"clean\"], \"problemMatcher\": [], \"group\": \"build\" }, ] } .vscode/launch.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { /* * Requires the Rust Language Server (rust-analyzer) and Cortex-Debug extensions * https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer * https://marketplace.visualstudio.com/items?itemName=marus25.cortex-debug */ \"version\": \"0.2.0\", \"configurations\": [ { /* Configuration for the STM32F303 Discovery board */ \"type\": \"cortex-debug\", \"request\": \"launch\", \"name\": \"Debug (OpenOCD)\", \"servertype\": \"openocd\", \"cwd\": \"${workspaceRoot}\", \"preLaunchTask\": \"Flash\", \"runToEntryPoint\": \"main\", \"executable\": \"./target/thumbv7em-none-eabihf/debug/rust-embedded-demo\", \"svdFile\": \"${workspaceRoot}/STM32H7B0x.svd\", \"configFiles\": [ \"${workspaceRoot}/openocd.cfg\" ], \"rttConfig\": { \"enabled\": true, \"address\": \"auto\", \"clearSearch\": false, \"polling_interval\": 20, \"rtt_start_retry\": 2000, \"decoders\": [ { \"label\": \"RTT channel 0\", \"port\": 0, \"type\": \"console\" } ] }, } ] } 工程配置 Cargo.toml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 [package] authors = [\"Haobo Gu \u003chaobogu@outlook.com\u003e\"] edition = \"2018\" readme = \"README.md\" name = \"rust-embedded-demo\" version = \"0.1.0\" [dependencies] cortex-m = \"0.7.7\" cortex-m-rt = \"0.7.3\" panic-rtt-target = { version = \"0.1.2\", features = [\"cortex-m\"] } rtt-target = \"0.4.0\" stm32h7xx-hal = { version = \"0.15.1\", features = [\"rt\", \"stm32h7b0\"] } log = \"0.4.20\" # Uncomment for the panic example. # panic-itm = \"0.4.1\" # Uncomment for the allocator example. # alloc-cortex-m = \"0.4.0\" # Uncomment for the device example. # Update `memory.x`, set target to `thumbv7em-none-eabihf` in `.cargo/config`, # and then use `cargo build --examples device` to build it. # [dependencies.stm32f3] # features = [\"stm32f303\", \"rt\"] # version = \"0.7.1\" # this lets you use `cargo fix`! [[bin]] name = \"rust-embedded-demo\" test = false bench = false [profile.release] codegen-units = 1 # better optimizations debug = true # symbols are nice and they don't increase the size on Flash lto = true # better optimizations 代码 main.rs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 #![no_main] #![no_std] mod rtt_logger; use cortex_m_rt::entry; use log::{info, warn, error}; use panic_rtt_target as _; use stm32h7xx_hal::{pac, prelude::*}; #[entry] fn main() -\u003e ! { rtt_logger::init(log::LevelFilter::Debug); // 获取cortex核心外设和stm32h7的所有外设 let cp = cortex_m::Peripherals::take().unwrap(); let dp = pac::Peripherals::take().unwrap(); // Power 设置 let pwr = dp.PWR.constrain(); let pwrcfg = pwr.freeze(); // 初始化RCC let rcc = dp.RCC.constrain(); let ccdr = rcc.sys_ck(200.MHz()).freeze(pwrcfg, \u0026dp.SYSCFG); // 设置LED对应的GPIO let gpioe = dp.GPIOE.split(ccdr.peripheral.GPIOE); let mut led = gpioe.pe3.into_push_pull_output(); // cortex-m已经实现好了delay函数，直接拿到，下面使用 let mut delay = cp.SYST.delay(ccdr.clocks); let mut cnt: u32 = 0; loop { // 点灯 if cnt == 1 { info!(\"current cnt: {}\", cnt); } else if cnt == 2 { warn!(\"current cnt: {}\", cnt); } else if cnt == 3 { error!(\"current cnt: {}\", cnt); } else if cnt \u003e 100000 { panic!(\"panicked!\") } else { debug!(\"current cnt: {}\", cnt); } cnt += 1; led.toggle(); // 延时500ms delay.delay_ms(500_u16); } } rtt_logger.rs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 use panic_rtt_target as _; use log::{Level, LevelFilter, Metadata, Record}; use rtt_target::{rprintln, rtt_init_print}; pub struct Logger { level: Level, } static LOGGER: Logger = Logger { level: Level::Info }; pub fn init(level: LevelFilter) { rtt_init_print!(); log::set_logger(\u0026LOGGER) .map(|()| log::set_max_level(level)) .unwrap(); } impl log::Log for Logger { fn enabled(\u0026self, metadata: \u0026Metadata) -\u003e bool { metadata.level() \u003c= self.level } fn log(\u0026self, record: \u0026Record) { let prefix = match record.level() { Level::Error =\u003e \"\\x1B[1;31m\", Level::Warn =\u003e \"\\x1B[1;33m\", Level::Info =\u003e \"\\x1B[1;32m\", Level::Debug =\u003e \"\\x1B[1;30m\", Level::Trace =\u003e \"\\x1B[2;30m\", }; rprintln!(\"{}{} - {}\\x1B[0m\", prefix, record.level(), record.args()); } fn flush(\u0026self) {} } ","description":"","tags":["rust"],"title":"Rust嵌入式教程（4）- VSCode开发环境配置","uri":"/posts/keyboard/4%E6%8A%8Avscode%E6%89%93%E9%80%A0%E6%88%90%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%BC%80%E5%8F%91%E7%9A%84%E5%88%A9%E5%99%A8/"},{"categories":null,"content":" B站视频教程链接：https://www.bilibili.com/video/BV1g94y1V75c\nRust嵌入式工作组 https://github.com/rust-embedded/wg\n软件生态 学习路径 Rust基础/嵌入式基础 Rust嵌入式官方book： 英文：https://docs.rust-embedded.org/book/intro/index.html 中文：https://xxchang.github.io/book/ 选择MCU，在https://crates.io/上搜索对应HAL库，https://docs.rs/上面浏览对应文档 Github上面查看HAL库的源代码，阅读对应examples 实践 ","description":"","tags":["rust"],"title":"Rust嵌入式教程（3）- 嵌入式生态环境 \u0026 学习路径","uri":"/posts/keyboard/3rust%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%94%9F%E6%80%81%E7%8E%AF%E5%A2%83%E5%AD%A6%E4%B9%A0%E8%B7%AF%E5%BE%84/"},{"categories":null,"content":" B站视频教程链接：https://www.bilibili.com/video/BV1jC4y1H7k8\n从模板创建Rust嵌入式工程 1 2 cargo install cargo-generate cargo generate --git https://github.com/rust-embedded/cortex-m-quickstart 修改Cargo.toml，添加 hal 依赖 1 2 3 4 cortex-m = \"0.7.7\" cortex-m-rt = \"0.7.3\" panic-halt = \"0.2.0\" stm32h7xx-hal = { version = \"0.15.0\", features = [\"stm32h7b0\", \"rt\"] } 修改.cargo/config.toml 1 2 3 4 5 6 7 8 9 [build] # Pick ONE of these default compilation targets # target = \"thumbv6m-none-eabi\" # Cortex-M0 and Cortex-M0+ # target = \"thumbv7m-none-eabi\" # Cortex-M3 # target = \"thumbv7em-none-eabi\" # Cortex-M4 and Cortex-M7 (no FPU) target = \"thumbv7em-none-eabihf\" # Cortex-M4F and Cortex-M7F (with FPU) # target = \"thumbv8m.base-none-eabi\" # Cortex-M23 # target = \"thumbv8m.main-none-eabi\" # Cortex-M33 (no FPU) # target = \"thumbv8m.main-none-eabihf\" # Cortex-M33 (with FPU) 修改main.rs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #![no_main] #![no_std] use cortex_m_rt::entry; use panic_halt as _; use stm32h7xx_hal::{pac, prelude::*}; #[entry] fn main() -\u003e ! { // 获取cortex核心外设和stm32h7的所有外设 let cp = cortex_m::Peripherals::take().unwrap(); let dp = pac::Peripherals::take().unwrap(); // Power 设置 let pwr = dp.PWR.constrain(); let pwrcfg = pwr.freeze(); // 初始化RCC let rcc = dp.RCC.constrain(); let ccdr = rcc.sys_ck(200.MHz()).freeze(pwrcfg, \u0026dp.SYSCFG); // 设置LED对应的GPIO let gpioe = dp.GPIOE.split(ccdr.peripheral.GPIOE); let mut led = gpioe.pe3.into_push_pull_output(); // cortex-m已经实现好了delay函数，直接拿到，下面使用 let mut delay = cp.SYST.delay(ccdr.clocks); loop { // 点灯 led.toggle(); // 延时500ms delay.delay_ms(500_u16); } } 构建 1 cargo build 烧录 首先修改openocd.cfg\n1 2 3 source [find interface/cmsis-dap.cfg] source [find target/stm32h7x.cfg] 1 openocd -f openocd.cfg -c \"program target/thumbv7em-none-eabihf/debug/cargo-embedded-demo preverify verify reset exit 0x08000000\" ","description":"","tags":["rust"],"title":"Rust嵌入式教程（2）- 点亮开发板","uri":"/posts/keyboard/2%E4%BD%BF%E7%94%A8rust%E7%82%B9%E4%BA%AE%E5%BC%80%E5%8F%91%E6%9D%BF/"},{"categories":null,"content":" B站视频教程链接：https://www.bilibili.com/video/BV19c411Z7YF/\n前言 使用Rust开发嵌入式软件的优点：\n现代的开发体验 方便的包管理 丰富的语言特性：Iterator、async/await、trait 常见MCU都有现成的HAL可用 说明 本教程假设你已经对Rust有了初步的了解，熟悉Rust的相关语法、配置；并且了解嵌入式编程，熟悉gcc系列开源工具链\n❗️ 现阶段，除非你个人具有超强的能力，否则，嵌入式领域的生产项目，请优先考虑 C\n环境配置 安装Rust https://rustup.rs/\n安装MCU对应的Target 英文：https://docs.rust-embedded.org/book/intro/install.html 中文：https://xxchang.github.io/book/intro/install.html\n1 rustup target add thumbv7em-none-eabihf 安装OpenOCD MAC 1 brew install openocd Windows https://github.com/xpack-dev-tools/openocd-xpack/releases\n安装VSCode插件 rust-analyzer cortex-debug 可选 安装 probe-rs https://probe.rs/docs/getting-started/installation/\n1 cargo install probe-rs --features cli VSCode插件：\nDebugger for probe-rs ","description":"","tags":["rust"],"title":"Rust嵌入式教程（1）- 前言\u0026环境配置","uri":"/posts/keyboard/1%E5%89%8D%E8%A8%80-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"Why? Rust作为一门新兴语言，其安全、可靠、运行效率高等特点让它成为一门非常适合嵌入式开发的语言。本文主要介绍如何搭建Rust嵌入式开发环境，然后使用stm32h7开发板点个灯。\n在嵌入式开发领域，C语言的地位是无法被撼动的（至少在2023年是这样）。用Rust开发嵌入式目前就两个目的：\n玩\n战未来 :)\n适用对象 如果你没有接触过嵌入式编程，或者完全不懂gcc系列的开源工具链，或者完全没有接触过Rust，那么建议先了解一下相关的背景知识。\nOK，Let's GO!\n搭建Rust开发环境 安装Rust 首先第一步是安装Rust的开发环境。Rust开发环境的安装非常简单，参考https://rustup.rs/，一条命令即可：\n1 curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh 安装之后，可以使用下面的命令查看当前的Rust版本：\n1 2 3 cargo --version rustup --version rustc --version 这里，几个工具简单了解一下：\ncargo：是Rust的构建系统和包管理器，安装、配置三方包、项目构建你都需要用到他 rustc：Rust编译器，相当于gcc rustup：Rust工具链管理工具，比如更新Rust版本、添加target等，都可以使用rustup 默认情况下，安装完Rust之后，编译的目标架构都是本机的架构。由于我们是嵌入式开发，因此需要交叉编译到MCU对应的架构，以stm32h7为例，它是ARM Cortex-m系列的MCU，其对应的target是：thumbv7em-none-eabihf。对于cortex-m系列的MCU来说，每种核心对应的target可以参考：https://logiase.github.io/The-Embedded-Rust-Book-CN/intro/install.html。\n我们使用rustup来添加对应的交叉编译支持：\n1 rustup target add thumbv7em-none-eabihf 运行完这个命令之后，你本地的Rust已经具备了编译对应MCU程序的能力。\n安装嵌入式相关工具链 使用Rust开发嵌入式代码，还需要一系列其他的工具链，比如openocd、gdb等等。这些工具的安装和使用C语言时并没有太大的区别，同样可以参考：https://logiase.github.io/The-Embedded-Rust-Book-CN/intro/install.html，左侧选择你当前的PC平台，按照说明安装即可。我使用的是MacOS，安装相关工具非常简单，使用homebrew安装即可：\n1 2 3 4 5 6 7 8 $ # GDB $ brew install armmbed/formulae/arm-none-eabi-gcc $ # OpenOCD $ brew install openocd $ # QEMU $ brew install qemu 配置IDE 我们使用VSCode来开发Rust。VSCode的安装就不再废话了，这里着重讲一下需要安装的插件：\nrust-analyzer：使用VSCode开发Rust必备 cortex-debug：调试、debug嵌入式程序 crates：提升编辑Cargo.toml的体验，辅助包管理 直接VSCode插件市场搜索安装即可。\n一些背景知识 在配置完开发环境之后，我们缓一缓，简单了解一下使用Rust开发嵌入式工程的一些背景知识。\n和C语言不同，Rust官方提供了一套标准的硬件抽象层embedded-hal，几乎所有的MCU厂家都会基于这套hal来开发自己的sdk。 各个厂商的相关SDK命名都遵循xxx-rs的方式，比如stm32就是stm32-rs，ESP是esp-rs，rp2040是rp-rs。如果我们想要找相关的SDK，就去对应的github组下面去找就好了。 在正式开发中，我们不会直接和embedded-hal打交道，而是使用各个厂家MCU对应的上层hal实现。我们使用的MCU是stm32h7b0，因此，直接去stm32-rs下面搜索stm32h7，就能看到对应的hal库stm32h7xx-hal了。当然也可以去crates.io搜索，一样的。使用对应的hal库也非常简单，在Cargo.toml的[dependencies]下面添加一行 1 stm32h7xx-hal = { version = \"0.14.0\", features = [\"stm32h7b0\", \"rt\", \"log-rtt\"] } 即可。上面的配置说明我们使用的hal库版本是0.14.0，我们的芯片是stm32h7b0，并且开启了log-rtt特性。features中的rt代表着 runtime，一般都默认加上。\n创建Rust嵌入式工程 在了解了相关背景知识之后，我们就可以去创建Rust嵌入式工程了。\n方法1 直接使用官方模板创建，一条命令即可：\n1 2 # 使用 rust 官方的 cortex-m-quickstart 作为我们的项目模板 cargo generate --git https://github.com/rust-embedded/cortex-m-quickstart 方法2 上面的创建方式会默认创建一个QEMU模拟器工程，如果你对Rust嵌入式开发不熟悉，把这个工程改成你的目标板的程序可能会有些麻烦。所以下面我们就从0开始，一步一步地搭建整个Rust嵌入式工程，在走完整个流程之后，你就可以了解每一个文件的具体功能，后面用项目模板一键创建工程改改就行了。下面详细按步骤介绍：\n1. 创建空的Rust工程 这一步非常简单，使用cargo创建即可\n1 cargo new project-name --bin --edition 2021 --bin表示创建的是一个可执行程序的工程（而不是一个库），--edition表示使用2021标准（也是最新版本）。使用VSCode打开工程，默认是一个本机的HelloWorld工程，命令行运行cargo run，运行。\n2. 配置.cargo/config.toml cargo new默认生成的工程的target是本机的架构，我们需要把target改成我们的目标MCU。\nVSCode根目录下创建.cargo/config.toml文件，并且填入以下的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 [target.thumbv7m-none-eabi] # uncomment this to make `cargo run` execute programs on QEMU # runner = \"qemu-system-arm -cpu cortex-m3 -machine lm3s6965evb -nographic -semihosting-config enable=on,target=native -kernel\" [target.'cfg(all(target_arch = \"arm\", target_os = \"none\"))'] # uncomment ONE of these three option to make `cargo run` start a GDB session # which option to pick depends on your system runner = \"arm-none-eabi-gdb -q -x openocd.gdb\" # runner = \"gdb-multiarch -q -x openocd.gdb\" # runner = \"gdb -q -x openocd.gdb\" rustflags = [ # Previously, the linker arguments --nmagic and -Tlink.x were set here. # They are now set by build.rs instead. The linker argument can still # only be set here, if a custom linker is needed. # By default, the LLD linker is used, which is shipped with the Rust # toolchain. If you run into problems with LLD, you can switch to the # GNU linker by uncommenting this line: # \"-C\", \"linker=arm-none-eabi-ld\", # If you need to link to pre-compiled C libraries provided by a C toolchain # use GCC as the linker by uncommenting the three lines below: # \"-C\", \"linker=arm-none-eabi-gcc\", # \"-C\", \"link-arg=-Wl,-Tlink.x\", # \"-C\", \"link-arg=-nostartfiles\", ] [build] # Pick ONE of these default compilation targets # target = \"thumbv6m-none-eabi\" # Cortex-M0 and Cortex-M0+ # target = \"thumbv7m-none-eabi\" # Cortex-M3 # target = \"thumbv7em-none-eabi\" # Cortex-M4 and Cortex-M7 (no FPU) target = \"thumbv7em-none-eabihf\" # Cortex-M4F and Cortex-M7F (with FPU) # target = \"thumbv8m.base-none-eabi\" # Cortex-M23 # target = \"thumbv8m.main-none-eabi\" # Cortex-M33 (no FPU) # target = \"thumbv8m.main-none-eabihf\" # Cortex-M33 (with FPU) 每一个选项注释里面已经说的很明白了，注意核心是[build]下的target，这里就配置了使用cargo build构建工程的时候，默认的target。\n3. 使用rust-toolchain.toml配置默认工具链 在很多情况下，我们想要使用最新的rust的feature，这些feature往往只在nightly版本的rust中生效。那么我们就可以在项目的根目录下创建一个rust-toolchain.toml文件，来配置当前项目使用的Rust工具链：\n1 2 3 [toolchain] channel = \"nightly\" components = [ \"rust-src\", \"rustfmt\", \"llvm-tools\" ] 这里，我们使用了nightly版本的Rust，同时激活了下面三个component。\n4. 创建build.rs构建脚本 build.rs是Rust的一个特殊文件，它主要用于配置构建的流程和相关的参数。对比gcc，可以理解成CFLAGS、compile options这些玩意都可以在这里配置。其功能可以参考：https://doc.rust-lang.org/cargo/reference/build-scripts.html。\n同时，如果你读的比较仔细，可以发现在上面的.cargo/config.toml中，有一个rustflags字段，也可以配置相关的选项。这两个地方的配置都可以生效。\n针对cortex-m的默认build.rs如下，暂时可以直接复制一份到项目根目录下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 //! This build script copies the `memory.x` file from the crate root into //! a directory where the linker can always find it at build time. //! For many projects this is optional, as the linker always searches the //! project root directory -- wherever `Cargo.toml` is. However, if you //! are using a workspace or have a more complicated build setup, this //! build script becomes required. Additionally, by requesting that //! Cargo re-run the build script whenever `memory.x` is changed, //! updating `memory.x` ensures a rebuild of the application with the //! new memory settings. //! //! The build script also sets the linker flags to tell it which link script to use. use std::env; use std::fs::File; use std::io::Write; use std::path::PathBuf; fn main() { // Put `memory.x` in our output directory and ensure it's // on the linker search path. let out = \u0026PathBuf::from(env::var_os(\"OUT_DIR\").unwrap()); File::create(out.join(\"memory.x\")) .unwrap() .write_all(include_bytes!(\"memory.x\")) .unwrap(); println!(\"cargo:rustc-link-search={}\", out.display()); // By default, Cargo will re-run a build script whenever // any file in the project changes. By specifying `memory.x` // here, we ensure the build script is only re-run when // `memory.x` is changed. println!(\"cargo:rerun-if-changed=memory.x\"); // Specify linker arguments. // `--nmagic` is required if memory section addresses are not aligned to 0x10000, // for example the FLASH and RAM sections in your `memory.x`. // See https://github.com/rust-embedded/cortex-m-quickstart/pull/95 println!(\"cargo:rustc-link-arg=--nmagic\"); // Set the linker script to the one provided by cortex-m-rt. println!(\"cargo:rustc-link-arg=-Tlink.x\"); } 5. 创建链接脚本memory.x Rust和C语言一样，在编译之后都需要一个链接脚本把所有的.o文件链接成可执行文件，这个链接脚本就是memory.x。Rust的链接脚本和gcc的ld文件一模一样。如果你不想自己写，可以从自己之前的gcc工程中直接把ld文件的内容复制过来，或者直接参考你开发板芯片对应的hal库的默认链接脚本。比如我使用的stm32h7，那么就在这个代码库里面找：https://github.com/stm32-rs/stm32h7xx-hal。\n6. 配置Cargo.toml K，现在一个基础的Rust嵌入式开发环境已经搭建完成了。整个工程的结构大概长这样子：\n1 2 3 4 5 6 7 .cargo - config.toml src - main.rs build.rs Cargo.toml rust-toolchain.toml 在上面这些外围的东西配置好了之后，我们就可以配置Cargo.toml了。通过Cargo使用三方包非常简单，在Cargo.toml的[dependencies]下面，把你想要用的三方包加上，就可以了：\n1 2 3 4 5 [dependencies] cortex-m = \"0.7.7\" cortex-m-rt = \"0.7.3\" stm32h7xx-hal = {version = \"0.14.0\", features = [\"stm32h7b0\", \"rt\", \"log-rtt\"]} panic-halt = \"0.2.0\" 作为最基础的程序，我们添加了上面四个crate，cortex-m用来操作cortex-m核心，cortex-m-rt是ARM Cortex-m核的运行时，stm32h7xx-hal是我们用的芯片的hal库，panic-halt可以理解成是hardfault处理程序，会直接halt暂停程序。如果不加这个，就需要手动实现hardfault处理代码。\n7. 修改main.rs 在配置完三方包之后，就可以修改我们的主程序main.rs了。下面先把主程序粘贴过来，后面逐行讲解：\n1 2 3 4 5 6 7 8 9 10 11 12 #![no_main] #![no_std] use panic_halt as _; use cortex_m_rt::entry; #[entry] fn main() -\u003e ! { loop { panic!(\"hello world!\"); } } 首先是两个声明#![no_main]和#![no_std]，这两句话说明我们的rust程序没有默认的main函数，也不使用std库。Rust中默认的main函数是std标准库中的函数，由于我们的target是嵌入式MCU，因此我们默认不使用std标准库，默认的main函数也不用了。\n下面一行use panic_halt as _;表示我们使用panic-halt包提供的错误处理。\n上面说我们没有使用的标准库的main函数，那我们的程序入口在哪里呢？看下面一行use cortex_m_rt::entry，意思就是我们会使用cortex-m-rt包提供的主函数入口，并且在对应的函数入口用#[entry]标识。下面就是我们的主函数了，我在主函数里面写了一个循环，并且调用了panic!，让程序panic，同时输出hello world。\n8. 编译！ 到这里，我们整个Rust的最简单的嵌入式工程就已经搭建完毕了。接下来，就编译我们的第一个Rust嵌入式程序：\n1 2 # 编译程序 cargo build 因为我们已经在.cargo/config.toml里面配置好了target，所以cargo build命令的默认target就会是我们的MCU。如果一切正常的话，就可以在target/thumbv7em-none-eabihf/debug目录下看到你的固件了！需要注意的是，固件的文件名和Cargo.toml中的name完全一致：\n9. Hello World程序 到上一步为止，你已经可以使用写出并且编译第一个固件程序了。但是这个程序会直接panic。下面我们就再写一些代码，完成第一个Hello World程序，并且通过RTT打印出来。\n首先，在Cargo.toml中添加RTT相关依赖：\n1 2 3 rtt-target = \"0.4.0\" panic-rtt-target = { version = \"0.1.2\", features = [\"cortex-m\"] } log = \"0.4.19\" 第一个rtt-target让MCU可以实现RTT输出，第二个panic-rtt-target用来替代panic-halt，也就是在panic的时候，通过rtt-target来输出相关信息，而不是直接停止MCU运行。第三个是Rust的log包，提供了logging相关的trait。\n然后，修改main.rs代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #![no_main] #![no_std] use cortex_m_rt::entry; use panic_rtt_target as _; use rtt_target::{rprintln, rtt_init_print}; use stm32h7xx_hal::{pac, prelude::*}; #[entry] fn main() -\u003e ! { // 初始化RTT rtt_init_print!(); // 获取cortex核心外设和stm32h7的所有外设 let cp = cortex_m::Peripherals::take().unwrap(); let dp = pac::Peripherals::take().unwrap(); // Power 设置 let pwr = dp.PWR.constrain(); let pwrcfg = pwr.freeze(); // 初始化RCC let rcc = dp.RCC.constrain(); let ccdr = rcc.sys_ck(200.MHz()).freeze(pwrcfg, \u0026dp.SYSCFG); // 设置LED对应的GPIO let gpioe = dp.GPIOE.split(ccdr.peripheral.GPIOE); let mut led = gpioe.pe3.into_push_pull_output(); // cortex-m已经实现好了delay函数，直接拿到，下面使用 let mut delay = cp.SYST.delay(ccdr.clocks); loop { // 点灯并且输出RTT日志 led.toggle(); rprintln!(\"Hello World!\"); // 延时500ms delay.delay_ms(500_u16); } } 简单解释一下main函数。首先，使用rtt_init_print!()初始化RTT功能。然后，通过cortex_m和stm32h7xx的PAC，获取cortex核的外设对象还有stm32h7的外设对象。后面就是初始化PWR和RCC，并且设置主频为200MHZ。注意在这里，每一个设置最后都需要调用freeze函数，表示把所有的设置写入对应寄存器。如果你知道builder模式，那你应该对这种操作很熟悉。\n接着，就从MCU外设中拿到GPIO，并且设置LED对应的GPIO状态。最后，在主循环里面点灯并且使用RTT打印。\n运行cargo build，编译一切OK。\nDebug Rust嵌入式程序 编译好了第一个固件之后，下一步就是烧录 \u0026 调试了。实话实说，烧录和调试和Rust本身关系不太大，使用的还是openocd 那一套。这里就主要写一下如何在VSCode下面调试Rust嵌入式程序。\n首先，还是安装cortex-debug插件，并且配置.vscode/launch.json。具体可以参考我之前的blog：使用OpenOCD+VSCode一键烧录Boot+App到内置+外置flash的VSCode配置部分即可。这里我也贴一下我使用的launch.json：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Cortex Debug\", \"cwd\": \"${workspaceFolder}\", \"executable\": \"${workspaceFolder}/target/thumbv7em-none-eabihf/debug/你的固件名称\", \"request\": \"launch\", \"type\": \"cortex-debug\", \"servertype\": \"openocd\", \"showDevDebugOutput\": \"parsed\", \"runToEntryPoint\": \"main\", \"device\": \"stlink\", \"preLaunchTask\": \"flash\", \"configFiles\": [ \"openocd.cfg\" ], \"svdFile\": \"STM32H7B0x.svd\", \"rttConfig\": { \"enabled\": true, \"address\": \"auto\", \"clearSearch\": false, \"polling_interval\": 20, \"rtt_start_retry\": 2000, \"decoders\": [ { \"label\": \"RTT channel 0\", \"port\": 0, \"type\": \"console\" } ] }, } ] } 需要注意的是，executable需要改成你的固件路径，然后配置好对应的openocd.cfg和svd文件即可。\n另外，这里只是启动调试，我配置了一个preLaunchTask，会在每一次调试之前自动烧录固件。这个task是配置在.vscode/tasks.json中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"Cargo Build (debug)\", \"type\": \"process\", \"command\": \"cargo\", \"args\": [\"build\"], \"problemMatcher\": [ \"$rustc\" ], \"group\": { \"kind\": \"build\", \"isDefault\": true } }, { \"label\": \"flash\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"openocd -f openocd.cfg -c \\\"program target/thumbv7em-none-eabihf/debug/你的固件名称 preverify verify reset exit\\\"\", \"dependsOn\": [ \"Cargo Build (debug)\" ], \"dependsOrder\": \"sequence\" }, ] } 需要注意的是，这里的烧录的openocd command，同样需要把固件路径改成你自己的。\n配置完这两个json文件之后，点击F5，VSCode就会自动编译、烧录固件、开启调试。默认情况下，调试的时候会首先停在程序入口处，需要手动点击运行：\n点击运行之后，就可以看到点灯成功，并且在下面终端Tab的RTT Channel中，可以看到MCU发过来的实时日志了：\n到现在为止，终于使用Rust在stm32上面点灯成功了，完结撒花🎉\n","description":"","tags":["keyboard","embedded"],"title":"使用Rust开发STM32嵌入式程序入门教程","uri":"/posts/keyboard/rust-embedded-tutorial/"},{"categories":null,"content":"Learn Tauri Tauri is a desktop application framework which has a Rust core and can be used with any frontend framework. It's similar with Electron, but tauri uses system's built-in web render, so the application created by tauri is much smaller than Electron.\nInstallation Tauri can be used with any frontend frameworks. We'll use vite in this article. If you don't have a project, use the following to create one:\n1 2 cargo install create-tauri-app cargo create-tauri-app Another thing you need to do is to install tauri cli tool to cargo or npm:\n1 2 3 cargo install tauri-cli # OR npm install --save-dev @tauri-apps/cli Then, you can use cargo or npm command like:\n1 2 3 cargo tauri dev # OR npm run tauri dev tauri-api npm package is also needed for calling Rust APIs in frontend:\n1 npm install @tauri-apps/api Vue 项目结构\npublic: 不会被编译的资源文件 src/assets: 资源文件，会被编译 src/components: 组件 src/App.vue: 入口文件 SFC 单文件组件 一个文件就是一个组件，一般分层几个部分：\nscript: setup只能有一个，其他的script可以有多个 template: 模板，只能有一个 style: css vue 模板语法 在\u003cscript setup\u003e里面定义一个变量v，然后就可以在\u003ctemplate\u003e里面使用{{ v }}用这个变量了。这是最简单的vue的模板语法：\n1 2 3 4 5 6 7 8 9 \u003ctemplate\u003e \u003cdiv\u003e {{ v }} \u003c/div\u003e \u003c/template\u003e \u003cscript setup lang=\"ts\"\u003e const v:number = 1; \u003c/script\u003e vue 指令 vue提供的语法糖指令。比如，v-text：直接显示text。使用方式：\n1 2 3 4 5 6 7 8 \u003ctemplate\u003e \u003cdiv v-text=\"v\"\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup lang=\"ts\"\u003e const v:string = \"str\"; \u003c/script\u003e 每种vue指令，都有其用法。\nv-on 在某事件发生的时候，触发\n1 \u003cbutton v-on:click='callback'\u003e v-on有一个简写，使用@代替v-on：\n1 \u003cbutton @click='callback'\u003e v-bind 绑定属性，简写是:\n1 \u003cdiv :id=\"my_id\"\u003e v-model 一般是绑定表单元素，比如输入、用户选择等\n1 \u003cinput v-model=\"a\" type=\"text\"\u003e 注意，这种变量需要用 ref() 声明，才能让对应的变量变成响应式的，否则变量不会自动更新\n","description":"","tags":["rust"],"title":"Learn Tauri","uri":"/posts/engineering/tauri/"},{"categories":null,"content":"一直以来，我都用的是openocd + vscode来开发stm32，相比传统的KEIL MDK，VSCode无论是各种插件（Copilot！）还是人性化方面都更胜一筹。用习惯之后，不论是Python还是Rust还是C，所有的语言都在一个地方开发，真的很爽。在开发stm32的时候，由于引入了RTOS，所以仅仅是断点单步调试显得有些不够用了，就想着可不可以像其他高级语言一样，电脑端命令行中实时打印日志。几番搜索下来，发现VSCode上面的这套开源工具链（OpenOCD + CortexDebug）已经把这个事情做了！先展示一下效果：\n单步调试 + 彩色日志，直接IDE内命令行展示，全齐，且不需要额外硬件，一个stlink足够。真的很方便。下面就简单介绍一下如何实现。\n安装RTT 首先是要从SEGGER网站上面下载JLink全家桶，并且把RTT相关代码复制到工程下面，在makefile中添加对应的文件。参见：https://zhuanlan.zhihu.com/p/163771273 这篇文章。其中，如果你要使用stlink，那么环境变量也不用配置，只需要复制RTT源码即可。然后，添加log文件，同样是从上面的文章中复制得到，感谢文章作者：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 /* * Author: Jayant Tang * Email: jayant97@foxmail.com */ #ifndef _LOG_H_ #define _LOH_H_ #include \"SEGGER_RTT.h\" #define LOG_DEBUG 1 #if LOG_DEBUG #define LOG_PROTO(type,color,format,...) \\ SEGGER_RTT_printf(0,\" %s%s\"format\"\\r\\n%s\", \\ color, \\ type, \\ ##__VA_ARGS__, \\ RTT_CTRL_RESET) /* 清屏*/ #define LOG_CLEAR() SEGGER_RTT_WriteString(0, \" \"RTT_CTRL_CLEAR) /* 无颜色日志输出 */ #define LOG(format,...) LOG_PROTO(\"\",\"\",format,##__VA_ARGS__) /* 有颜色格式日志输出 */ #define LOGI(format,...) LOG_PROTO(\"INFO: \", RTT_CTRL_TEXT_BRIGHT_GREEN , format, ##__VA_ARGS__) #define LOGW(format,...) LOG_PROTO(\"WARN: \", RTT_CTRL_TEXT_BRIGHT_YELLOW, format, ##__VA_ARGS__) #define LOGE(format,...) LOG_PROTO(\"ERROR: \", RTT_CTRL_TEXT_BRIGHT_RED , format, ##__VA_ARGS__) #else #define LOG_CLEAR() #define LOG #define LOGI #define LOGW #define LOGE #endif #endif // !_LOG_H_ 配置Cortex-Debug OpenOCD原生是支持RTT的，在默认情况下需要去修改openocd.cfg来配置RTT。不过在VSCode里面，Cortex-Debug插件已经帮你配置好了一切（当然底层还是使用的OpenOCD的能力）！本来Cortex-Debug也是在VSCode下调试stm32必装的软件，所以说我们实际上只需要改改配置就可以使用RTT了。\n具体Cortex-Debug的配置可以参见我之前的文章：https://haobogu.github.io/posts/keyboard/openocd-ospi-flash/ 中的launch.json配置环节。\n在这里，我们只需要在原本的launch.json中，增加如下配置：\n然后，按F5烧录代码并启动调试。你就会在下面的终端页面上看到新增了一个窗口：\n这里就是RTT日志显示的地方。这样，不管是单步调试还是彩色日志显示，在一个地方全齐。Cortex-Debug甚至还支持RTT的其他特性，比如Graph：\n更多的使用方法，可以参考Cortex-Debug的WIKI：https://github.com/Marus/cortex-debug/wiki/SEGGER-RTT-support\n","description":"","tags":["keyboard","embedded"],"title":"使用VSCode + RTT方便地调试STM32","uri":"/posts/keyboard/rtt/"},{"categories":null,"content":"RTOS 什么是RTOS RTOS指实时操作系统（Real-Time Operation System）。是广泛用在嵌入式中的操作系统，其主要的特征是基于优先级的任务执行，以及快速、实时的响应。\nAzure RTOS Azure RTOS的前身是ThreadX，是微软收购并且开源的一个RTOS。Azure RTOS包含一整套的嵌入式系统，实现了全家桶式的嵌入式RTOS解决方案。Azure RTOS主要包括如下组件：\nThreadX: 高性能实时操作系统 NetX: 嵌入式网络协议栈 FileX: 嵌入式FAT文件系统 LevelX: NAND 和 NOR 闪存磨损均衡 GuiX: 嵌入式图形库 USBX: 嵌入式USB协议栈 TraceX: 基于主机的嵌入式分析工具 RTOS有很多，为什么我会选择Azure RTOS作为我学习使用的对象呢？原因主要有如下三点：\n我主要使用STM32，ST官方支持比较好的RTOS只有FreeRTOS和AzureRTOS（ThreadX） ThreadX的设计很棒，代码写得非常好，且非常规范。如所有ThreadX相关的函数都是tx_开头，更加符合我的代码审美 官方提供完善的中文文档 提供了从文件系统、USB到网络协议栈的全家桶，并且全都可以使用CubeMX生成和配置 当然，其他RTOS如FreeRTOS也是非常好的选择，选自己喜欢的就行，一通百通。\n我们主要会用到ThreadX、FileX、LevelX、USBX等，下面我们会从Azure RTOS的核心，ThreadX，开始讲起。\nThreadX 中文文档：https://learn.microsoft.com/zh-cn/azure/rtos/threadx\n下面我们会学习整个ThreadX的中文文档。文档主要有6个章节，分别是：\n第 1 章 - 简要概述 Azure RTOS ThreadX 及其与实时嵌入式开发的关系\n第 2 章 - 介绍在应用程序中安装和使用 Azure RTOS ThreadX 的基本步骤（开箱即用）\n第 3 章 - 详细介绍 Azure RTOS ThreadX（高性能实时内核）的功能操作\n第 4 章 - 详细介绍如何将应用程序的接口应用到 Azure RTOS ThreadX\n第 5 章 - 介绍如何编写 Azure RTOS ThreadX 应用程序的 I/O 驱动程序\n第 6 章 - 介绍随每个 Azure RTOS ThreadX 处理器支持包一起提供的演示应用程序\nThreadX简介 首先了解一些基础概念。\n首先是实时软件的概念。实时软件实际上就是需要实时地和外部进行交互的软件，大多数嵌入式软件都属于实时软件。在出现RTOS之前，大多数实时软件都是使用C main函数内部的主循环来分配各个任务的处理时间（现在在一些简单的程序中仍然是这么做的）。这么做的问题在于由于每个事件的响应时间不一，对于大型或者复杂的程序，其时序特性就会发生变化，整个程序就会变得不稳定、难以维护。\n而RTOS的引入可以解决这个问题，基于优先级的响应方式可以让重要的外部事件处理变得确定和快速。当然，现代操作系统基于进程和线程的调度方式是实时操作系统的进一步延伸，不过对于嵌入式系统，一般来说RTOS已经足够用了。\n第二是线程的概念。由于在ThreadX中，并没有复杂的线程、进程之分，因此为了避免混淆，ThreadX统一使用“线程”来描述一项程序任务。\n安装ThreadX 有两种方式：\n直接克隆源码 CubeMX 我们主要是使用CubeMX的方式。参见下面的文章：https://blog.csdn.net/wallace89/article/details/114941859。大多数的配置保持默认即可，唯一需要注意的是，TX_TIMER_TICKS_PER_SECOND需要改为1000，即系统Tick的时间为1ms，这也是大多数RTOS的默认设置。然后Memory Pool大小，即堆栈大小设置大一些就可以了。\n在设置了FileX和USBX之后，生成的代码会把CubeMX自带的FatFS和USB生成的代码覆盖掉，说明在ThreadX系统中，会由FileX和USBX替代对应的模块。注意可能需要重新移植一下对应的接口。\n第一个ThreadX程序 在使用CubeMX生成了ThreadX代码之后，我们首先看一下main.c:\n可以看到CubeMX在主循环之前增加了一个MX_ThreadX_Init()，并且增加了一句提示，说MX_ThreadX_Init()后面的代码永远不会执行。这就说明从MX_ThreadX_Init()这个函数，系统就进入了ThreadX的实时操作系统中。点进MX_ThreadX_Init()看看代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * @brief MX_ThreadX_Init * @param None * @retval None */ void MX_ThreadX_Init(void) { /* USER CODE BEGIN Before_Kernel_Start */ /* USER CODE END Before_Kernel_Start */ tx_kernel_enter(); /* USER CODE BEGIN Kernel_Start_Error */ /* USER CODE 可以看到实际上，只调用了一个ThreadX的函数tx_kernel_enter()，这就是ThreadX的入口。这个函数相当于是裸机程序的主循环，它永远不会返回。\nOK，我们现在知道了ThreadX的入口在哪里。那么下一步就是初始化系统资源。根据官方文档，我们可以使用tx_application_define来初始化你的第一个线程。搜索tx_application_define，可以发现这个函数被生成在了AZURE_RTOS\\App\\app_azure_rtos.c文件下。这个文件非常长，简单来说就是根据在CubeMX里面设置的Azure Application的设置，来初始化对应的资源：\n然后在资源创建完成之后，你就可以在对应的区域添加你自己的实时任务代码了。如下图框起来的就是ThreadX核心的线程代码和FileX的线程代码。\n​\t一般可以使用tx_thread_create来创建。下面这段代码就是官方文档里面的最最简单的单线程创建代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #include \"tx_api.h\" unsigned long my_thread_counter = 0; TX_THREAD my_thread; main( ) { /* Enter the ThreadX kernel. */ tx_kernel_enter( ); } void tx_application_define(void *first_unused_memory) { /* Create my_thread! */ tx_thread_create(\u0026my_thread, \"My Thread\", my_thread_entry, 0x1234, first_unused_memory, 1024, 3, 3, TX_NO_TIME_SLICE, TX_AUTO_START); } void my_thread_entry(ULONG thread_input) { /* Enter into a forever loop. */ while(1) { /* Increment thread counter. */ my_thread_counter++; /* Sleep for 1 tick. */ tx_thread_sleep(1); } } 可以看到整个的框架和CubeMX生成的实际上是一样的，入口就是main -\u003e tx_kernel_enter，然后初始化就是tx_application_define，然后里面定义了一个线程my_thread_entry。这个线程会增加计数器 -\u003e 休眠1Tick，就这样一直无限循环下去。\n当然，你也可以创建一个线程，然后执行完毕之后就退出，这也是可以的。\n基础配置 ThreadX中有很多配置项，这些配置项都定义在Core/Inc/tx_user.h中，只有当定义了TX_INCLUDE_USER_DEFINE_FILE后这个文件里面的设置才会生效。不过不用担心，CubeMX已经帮我们设置好了。具体都有哪些设置，这里看文档吧，也没什么捷径，唯一需要注意的是可以在CubeMX里面定义的尽量在CubeMX里面定义，否则重新生成代码可能会把原先的设置覆盖掉。\nThreadX的内核组件 ThreadX应用程序包含四种类型的程序执行：\n初始化 线程执行 中断服务程序（Interrupt Service Routine，ISR） 应用程序计时器 这四类程序的执行如下图：\n下面简单介绍一下这四类程序。\n初始化\n就是初始化\n线程执行\n在初始化完毕之后，ThreadX就会进入线程计划循环，简单来说就是\n查找ready的最高优先级的线程 把控制权转交给该线程 执行完毕（或者更高优先级的线程Ready） 执行权交回线程计划循环 继续查找下一个最高优先级的线程 这样一个无限循环的过程。\n中断服务程序（ISR）\n中断是实时系统的基础。在检测到更高优先级的中断时，系统会把当前程序的执行信息保存在堆栈上，然后去执行中断服务程序。\n应用程序计时器\n和中断服务程序类似，区别在于其硬件实现是对程序隐藏的，通常被用来执行超时、定时任务或者监视器服务。此外，应用程序计时器无法相互中断，这一点也和ISR不同。\n这就是ThreadX中定义的四类主要程序。每一类程序的详细讲解，可以参考这里的文档。\nThreadX的设备驱动程序 USBX USBX是Azure RTOS提供的高性能USB主机和设备的嵌入式堆栈，其特点是\n内存占用小 和ThreadX、FileX等Azure RTOS体系完美适配和集成 支持绝大多数MCU，以及绝大多数USB设备和主机类别 提供直观且一致的API，遵循动词 - 名词的命名约定，所有API均带有ux_前缀 所有阻塞API都带有可选的超时 通过了各种认证 下面就介绍一下USBX的功能，以及使用\nUSBX功能 USBX 同时支持主机端和设备端。 每一端都由三个层组成。\n控制器层 堆栈层 类层 如下图所示：\nUSBX安装 Cube已经把USBX所需要的文件生成在了USBX目录下，在代码中只需要按需引入ux_api.h或者ux_port.h即可。\n此外，要想在项目中使用USBX还需要如下步骤：\n在tx_application_define的开头添加ux_system_initialize，完成USB资源的初始化 ux_system_initialize这个函数定义了USBX的内存池，具体使用可以看后续章节 添加对ux_host_stack_initialize的调用 初始化所需的 USBX 类（主机和/或设备类） 初始化系统中可用的设备控制器 （optional）修改 tx_low_level_initialize.c 文件，以添加低级别硬件初始化和中断向量路由地址 USBX的配置 和ThreadX类似，USBX的所有配置都在ux_user.h文件中。可以按照这里的文档修改相应配置。修改之前注意检查CubeMX。\nUSBX的初始化 初始化 \u0026 反初始化 首先是USBX的初始化。由于USBX有自己的内存管理器，因此可以为USBX单独申请内存池，这个步骤必须在初始化主机或者设备端之前完成。下面是初始化内存池的例子：\n1 2 // USBX 内存资源初始化为128K的普通内存，无缓存安全内存池 ux_system_initialize(memory_pointer,(128*1024),UX_NULL,0); 注意：当常规内存不是缓存安全的，且MCU需要使用DMA时，需要定义缓存安全内存池。在不定义缓存安全内存池时，USBX会使用普通内存池代替。\n要想终止使用USBX（反初始化），则需要首先保证所有的USB类和控制器资源已经终止，然后执行ux_system_uninitialize();即可。\nUSBX的设备堆栈（Device Stack） 对于我们来说，主要是使用设备堆栈，即让我们的MCU成为一个USB设备。USBX的设备堆栈的结构可以参考下图：\n简单理解，可以把整个USBX的设备堆栈划分为4个部分（3层，上面已经介绍过）：\n最上层是设备类（Device Classes） 中间层是设备堆栈（Device Stack） 底层是设备控制器（Device Controller） 中间还有一个是VBUS控制器 下面会先讲一下初始化和接口调用，然后简单介绍一下\n初始化设备堆栈 要想使用USBX的设备堆栈，需要在调用ux_system_initialize初始化完毕内存池之后，调用ux_device_stack_initialize来初始化设备堆栈所使用的所有资源。然后，就可以使用ux_device_stack_class_register去注册对应是USB设备类。还需要注意的是，USB设备控制器（即Device Controller），需要单独初始化。\n说了这么多，下面给一个初始化的checklist：\nUSBX内存池：ux_system_initialize USB设备堆栈：ux_device_stack_initialize 注册USB设备类到设备堆栈：ux_device_stack_class_register USB设备控制器：ux_dcd_controller_initialize(这个函数似乎已depreciated，查找ux_dcd_*获取最新更新) 下面是一个Demo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // USBX内存池初始化 ux_system_initialize(memory_pointer,(128*1024), 0, 0); // 设备堆栈初始化 status = ux_device_stack_initialize(\u0026device_framework_high_speed, DEVICE_FRAMEWORK_LENGTH_HIGH_SPEED, \u0026device_framework_full_speed, DEVICE_FRAMEWORK_LENGTH_FULL_SPEED, \u0026string_framework, STRING_FRAMEWORK_LENGTH, \u0026language_id_framework, LANGUAGE_ID_FRAMEWORK_LENGTH, UX_NULL); // 存储设备类的初始化参数 /* Store the number of LUN in this device storage instance: single LUN. */ storage_parameter.ux_slave_class_storage_parameter_number_lun = 1; /* Initialize the storage class parameters for reading/writing to the Flash Disk. */ storage_parameter.ux_slave_class_storage_parameter_lun[0].ux_slave_class_storage_media_last_lba = 0x1e6bfe; storage_parameter.ux_slave_class_storage_parameter_lun[0].ux_slave_class_storage_media_block_length = 512; storage_parameter.ux_slave_class_storage_parameter_lun[0].ux_slave_class_storage_media_type = 0; storage_parameter.ux_slave_class_storage_parameter_lun[0].ux_slave_class_storage_media_removable_flag = 0x80; storage_parameter.ux_slave_class_storage_parameter_lun[0].ux_slave_class_storage_media_read = tx_demo_thread_flash_media_read; storage_parameter.ux_slave_class_storage_parameter_lun[0].ux_slave_class_storage_media_write = tx_demo_thread_flash_media_write; storage_parameter.ux_slave_class_storage_parameter_lun[0].ux_slave_class_storage_media_status = tx_demo_thread_flash_media_status; // 注册上面的存储设备类到interface 0 status = ux_device_stack_class_register(ux_system_slave_class_storage_name ux_device_class_storage_entry, ux_device_class_storage_thread,0, (VOID *)\u0026storage_parameter); // 初始化设备控制器 status = ux_dcd_controller_initialize(0x7BB00000, 0, 0xB7A00000); 应用层的接口调用 USBX的接口分为两层：\nUSB Device Stack API：负责注册 USBX 设备组件（例如类和设备框架）。 USB Device Class API：这些API随着具体的USB设备类的不同而不同，比如MSC/HID等。 大多数情况下，只需要调用Class API即可，并不需要调用Stack API。\nUSB的设备框架 设备框架主要是用来定义USB设备，分成4个部分：\n设备描述符： 配置描述符 接口描述符 终结点描述符 这些都遵循USB协议。定义方式是这样的，下面是一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #define DEVICE_FRAMEWORK_LENGTH_HIGH_SPEED 60 UCHAR device_framework_high_speed[] = { /* Device descriptor */ 0x12, 0x01, 0x00, 0x02, 0x00, 0x00, 0x00, 0x40, 0x0a, 0x07, 0x25, 0x40, 0x01, 0x00, 0x01, 0x02, 0x03, 0x01, /* Device qualifier descriptor */ 0x0a, 0x06, 0x00, 0x02, 0x00, 0x00, 0x00, 0x40, 0x01, 0x00, /* Configuration descriptor */ 0x09, 0x02, 0x20, 0x00, 0x01, 0x01, 0x00, 0xc0, 0x32, /* Interface descriptor */ 0x09, 0x04, 0x00, 0x00, 0x02, 0x08, 0x06, 0x50, 0x00, /* Endpoint descriptor (Bulk Out) */ 0x07, 0x05, 0x01, 0x02, 0x00, 0x02, 0x00, /* Endpoint descriptor (Bulk In) */ 0x07, 0x05, 0x82, 0x02, 0x00, 0x02, 0x00 }; FileX Azure FileX是一个FAT格式的文件管理系统，支持各种设备、各种FAT格式和exFAT拓展格式，并且针对性地做了很多优化。\nFileX的安装 和上面ThreadX、USBX的安装一样，要安装FileX只需要在CubeMX中开启对应的功能，并设置相关参数即可。FileX的相关文件基本都是fx_为开头，和上面ThreadX/USBX类似，主要引入的头文件是fx_api.h和fx_port.h。\n需要额外注意的是，FileX依赖ThreadX的计时器（timer）和信号量（semaphores）功能。\n要想在项目中使用FileX，还需要如下配置：\ninclude fx_api.h文件 在tx_application_define 或应用程序线程中调用 fx_system_initialize，来初始化FileX文件系统 然后使用 fx_media_open 来添加文件系统FileX媒体，这个函数必须在应用程序线程中调用 编译代码 FileX的配置项 配置项在这里。注意使用CubeMX配置。整个Azure RTOS的配置都是类似的，参考上面其他模块的配置即可。\nFileX的功能组件 FileX的文件系统 \u0026 FAT文件系统基础 FileX会把物理的存储看作是一个逻辑扇区的数组，至于扇区和底层物理的映射需要在I/O驱动程序中来做，这些驱动程序在调用fx_media_open的时候被初始化。在FileX中，底层物理存储被称为介质（media）。\nFAT12/16/32的介质的逻辑扇区布局一般长这样：\n可以看到：\n逻辑扇区的起始ID为1，扇区1指向的是保留扇区（reserved sector） 第0扇区为启动扇区 不过，当介质有隐藏扇区的时候，这些隐藏扇区会在启动扇区之前，因此启动扇区的偏移量必须要考虑是否有隐藏扇区 作为区分，exFAT的逻辑扇区长这样：\n在exFAT中， 启动块和 FAT 区域属于系统区域。 其余群集属于用户区域。同时，作为对FAT12/16/32的兼容，exFAT中的0x0B 和 0x40 之间的区域（其中包含 FAT12/16/32 中的各种介质参数）在 exFAT 中标记为“保留”，必须全都置为0。\nFAT12/16/32以及exFAT的启动记录，可以参考文档中的详细说明：https://learn.microsoft.com/zh-cn/azure/rtos/filex/chapter3，这里不再一一介绍。\nFileX 的 I/O 驱动程序 FileX支持多介质（media），每一个FileX的介质实例都有其对应的唯一的I/O的驱动程序。在FileX中，FX_MEDIA结构定义了管理对应的存储介质所需要的一切。用户需要做的，就是为FileX的I/O驱动程序，写一个函数作为整个驱动的入口：\n1 void my_driver_entry(FX_MEDIA *media_ptr); 这个函数的唯一一个参数是FX_MEDIA，它会根据FX_MEDIA中的请求fx_media_driver_request和具体的参数，来判断当前要做的事情，然后调用对应的函数/硬件接口。在这里可以看到常用的fx_media_driver_request的值，以及对应请求会用到的参数：https://learn.microsoft.com/zh-cn/azure/rtos/filex/chapter5#io-driver-requests。\nmy_driver_entry可以理解成是对硬件存储驱动程序的一个统一入口，在这里去分发不同的硬件I/O请求。需要注意的是，在CubeMX生成的LevelX + FileX代码中，入口函数名为：fx_stm32_levelx_nor_driver\n这里是一个针对RAM的Driver fx_ram_driver.c，作为参考。\nFileX的容错模块 FileX包含了一个容错模块，用于在文件写入过程中介质断电或者弹出时的容错。这个模块不是默认开启的，如果想要开启，需要在定义了FX_ENABLE_FAULT_TOLERANT的情况下，生成FileX。这样，在调用fx_media_open初始化FileX之后，FileX会自动调用fx_fault_tolerant_enable从而启动容错服务。\n容错模块会开启容错日志，容错日志会在介质中占用一个cluster的大小。容错日志里面会记录文件操作，当操作被异常终止的时候，日志就可以把当前操作记录下来，后面再初始化FileX之后就会根据容错日志来完成上次中断的操作。当所有数据都成功写入并且提交到介质之后，FileX会删除当前的日志条目文件更新操作完成。\nLevelX LevelX是Azure RTOS提供的用于NAND和NOR闪存的磨损均衡工具。需要注意的是，LevelX不依赖FileX，但是会依赖ThreadX。\nLevelX的安装 和上面基本一样，不再赘述。生成的文件中，api都在lx_api.h中。此外，LevelX生成的文件可以分成两类，lx_nand_xxxxx和lx_nor_xxxxx，分别用于NAND Flash和NOR Flash的磨损均衡。按需取用即可。\n在官方库里面，还有LevelX结合FileX的示例，在这些文件中\n# nand flash demo_filex_nand_flash.c fx_nand_flash_simulated_driver.c lx_nand_flash_simulator.c # nor flash demo_filex_nor_flash.c fx_nor_flash_simulated_driver.c lx_nor_flash_simulator.c LevelX的配置选项 同上，见：https://learn.microsoft.com/zh-cn/azure/rtos/levelx/chapter2#configuration-options\nLevelX对NOR Flash的支持 我们主要会使用NOR Flash，因此主要看一下LevelX对NOR Flash的支持就好。LevelX在初始化的时候，会调用lx_nor_flash_open，这个函数会把NOR Flash的驱动程序指定给LevelX。NOR Flash的驱动函数如下：\n1 INT nor_driver_initialize(LX_NOR_FLASH *instance); 其中instance就是LevelX的实例。在这个驱动函数里面，会把具体的硬件驱动和LevelX实例关联起来。每个LevelX的NOR Flash的实例需要实现如下几个服务：\n读取扇区 写入扇区 块擦除 验证擦除的块 系统错误处理程序 上面的这些服务是通过在上面的初始化函数中设置LX_NOR_FLASH的instance的相应函数指针来设置的，此外，驱动程序的初始化函数还负责\n指定Flash的基础地址 指定块的总数和每个块的字节数 RAM缓冲区设置 下面介绍一下需要在驱动程序中实现的服务。\n读取扇区 顾名思义，就是读取NOR Flash中的特定扇区。如果成功，返回LX_SUCCESS。失败则返回LX_ERROR。其原型函数为：\n1 2 3 4 INT nor_driver_read_sector( ULONG *flash_address, ULONG *destination, ULONG words); 其中，flash_address就是读取的逻辑扇区的地址，destination是读出来的数据位置，而words则是读取大小（32位字节的数量）。\n写入扇区 和读取类似，如果成功，返回LX_SUCCESS。失败则返回LX_ERROR。原型函数为：\n1 2 3 4 INT nor_driver_write_sector( ULONG *flash_address, ULONG *source, ULONG words); flash_address为写入逻辑扇区的地址，source为数据，words为写入的大小。\n注意，在写入时，需要自行判断写入是否成功。这个步骤可以通过读取回写入值并校验实现。\n块擦除 负责擦除指定的块（Block）。如果成功，返回LX_SUCCESS。失败则返回LX_ERROR。原型函数为：\n1 2 INT nor_driver_block_erase(ULONG block, ULONG erase_count); block就是要擦除的块，而erase_count则主要用于诊断\n验证擦除的块 LevelX需要验证对应的块是否已经成功擦除，如果已被擦除，则返回LX_SUCCESS，否则返回LX_ERROR。函数原型如下：\n1 INT nor_driver_block_erased_verify(ULONG block); block就是要验证的块。\n驱动程序系统错误 这个函数负责处理驱动程序发生的错误，函数原型入下：\n1 INT nor_driver_system_error(UINT error_code); error_code表示发生的错误码。\nNOR的模拟驱动程序 LevelX提供了使用RAM来模拟NOR闪存的驱动程序，默认情况下，这个驱动程序会提供8个闪存块（Block），每个块有16个扇区（Sector），每个扇区512字节。这个模拟驱动程序在lx_nor_flash_simulator.c中定义，可以作为自定义驱动程序的很好的参考。其初始化函数为lx_nor_flash_simulator_initialize。\nFileX集成 fx_nor_flash_simulated_driver.c文件提供了NOR模拟 + FileX的集成驱动程序，可以作为很好的参考。\n另外，官方问题还有如下提示，可以用来提升读写效率：\nFileX NOR 闪存格式应该是小于 NOR 闪存提供的扇区的一个完整块大小。 这将有助于确保在损耗均衡处理期间获得最佳性能。 在 LevelX 损耗均衡算法中提高写入性能的其他技巧包含：\n确保所有写入大小都恰好是一个或多个群集（cluster），并且在确切的群集边界上启动。 通过 FileX 的API fx_file_allocate 类执行大文件写入操作之前，请先预分配群集。 定期使用 lx_nor_flash_defragment 释放尽可能多的 NOR 块，从而提高写入性能。 确保已启用 FileX 驱动程序以接收释放扇区信息，并通过调用 lx_nor_flash_sector_release 在驱动程序中处理对驱动程序发出的释放扇区的请求。 CubeMX的坑 在使用CubeMX生成ThreadX工程的时候，需要注意如下几个坑：\n生成的ThreadX，默认的porting是IAR，即使在CubeMX中选择makefile，也一样。因此，需要去官方库，把port/cortex_m7/gnu目录下的内容复制到CubeMX工程下的Middlewares/ST/threadx/ports/cortex_m7/gnu。\n把makefile里面的IAR相关的路径干掉，换成gnu的\n在makefile里面添加.S文件（即ThreadX的gnu porting）相关的编译选项，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 # 添加所有的 .S 文件到 S_SOURCES，主要是 gnu porting。 # 别忘记 tx_initialize_low_level.S 这个文件。 S_SOURCES = $(wildcard Middlewares/ST/threadx/ports/cortex_m7/gnu/src/*.S) \\ Core/Src/tx_initialize_low_level.S # .. 中间省略，下面到生成的 OBJECTS 的地方，添加 .S 文件对应的 OBJECTS OBJECTS += $(addprefix $(BUILD_DIR)/,$(notdir $(sort $(S_SOURCES:.S=.o)))) vpath %.S $(sort $(dir $(sort $(S_SOURCES)))) # .. 然后到构建命令这边，添加 .S 对应的构建任务（参照 .s 即可） $(BUILD_DIR)/%.o: %.S Makefile | $(BUILD_DIR) $(AS) -c $(CFLAGS) $\u003c -o $@ ","description":"","tags":["keyboard","embedded"],"title":"RTOS","uri":"/posts/keyboard/rtos/"},{"categories":null,"content":"使用OpenOCD+VSCode一键烧录Boot+App到内置+外置flash 背景 在开发stm32系列的时候，大多数情况下会使用Windows系统+MDK/IAR来开发。不过对于主力是MacOS+Windows的我来说，一套兼容两个系统的开发方案就成了刚需。在中文互联网上面类似的资料非常稀少，不过实际上，使用gcc+OpenOCD的方案实际上已经非常成熟了，在此就记录一下我的折腾历程。\n最终效果 在讲具体配置步骤之前，先看一下最终的效果：\nVSCode中，shift+cmd+b，选择flash merged firmware，然后VSCode会帮你把搞定以下所有：\n自动编译App+Bootloader两个工程 使用srec_cat合并bootloader.hex和application.hex（同时兼容MacOS和Windows） 把bootloader.hex烧写到stm32内置Flash，把application.hex烧写到外置OSPI Flash 一键搞完，相当爽：\n如果你选择flash bootloader或者flash application，也一样，VSCode会首先编译对应的hex，然后自动烧录到对应Flash。\nDebug 按F5就可以开始debug，如果默认配置的是Debug Bootloader，那么会先编译烧录bootloader，然后开始debug；Debug Application也一样。且VSCode会自动地在bootloader和application之间跳转：\n开发环境配置 首先，开发环境的配置可以参考这里：https://haobogu.github.io/posts/keyboard/develop-stm32-using-vscode/。需要注意的是我们需要选择gcc+makefile的方案而不是PlatformIO。\n需要注意的是，如果你想要安装最新版的OpenOCD，在Mac上面可以直接使用\nbrew install openocd --HEAD 另外，还需要使用homebrew安装srecord备用：\nbrew install srecord 在Windows下，这两者都需要手动安装，srecord需要一个srec_cat.exe。\n创建工程 由于我们的工程是App + Bootloader形式的，因此需要在根目录下创建两个工程。创建完之后，可以把打包编译都加到VSCode的task里。然后，直接使用shift+cmd+b快捷键就可以选择任务：\n下面是一个我的task.json，供参考\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 { // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"make bootloader\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"cd bootloader \u0026\u0026 make all -j 16\", \"problemMatcher\": { \"owner\": \"cpp\", \"fileLocation\": [ \"relative\", \"${workspaceFolder}/application\" ], \"pattern\": { \"regexp\": \"^(.*):(\\\\d+):(\\\\d+):\\\\s+(warning|error):\\\\s+(.*)$\", \"file\": 1, \"line\": 2, \"column\": 3, \"severity\": 4, \"message\": 5 } } }, { \"label\": \"make application\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"cd application \u0026\u0026 make all -j 16\", \"problemMatcher\": { \"owner\": \"cpp\", \"fileLocation\": [ \"relative\", \"${workspaceFolder}/application\" ], \"pattern\": { \"regexp\": \"^(.*):(\\\\d+):(\\\\d+):\\\\s+(warning|error):\\\\s+(.*)$\", \"file\": 1, \"line\": 2, \"column\": 3, \"severity\": 4, \"message\": 5 } }, }, { \"label\": \"make all\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"cd application \u0026\u0026 make all -j 16 \u0026\u0026 cd ../bootloader \u0026\u0026 make all -j 16\" }, { \"label\": \"make clean\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"cd bootloader \u0026\u0026 make clean \u0026\u0026 cd ../application \u0026\u0026 make clean\" }, { \"label\": \"flash bootloader\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"cd bootloader \u0026\u0026 openocd -f openocd.cfg -c \\\"program build/bootloader.elf preverify verify reset exit\\\"\", \"dependsOn\": [ \"make bootloader\" ], \"dependsOrder\": \"sequence\" }, { \"label\": \"flash application\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"cd application \u0026\u0026 openocd -f openocd.cfg -c \\\"program build/application.hex preverify verify reset exit 0x00000000\\\"\", \"dependsOn\": [ \"make application\" ], \"dependsOrder\": \"sequence\" }, { \"label\": \"flash merged firmware\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"openocd -f bootloader/openocd.cfg -c \\\"program firmware.hex preverify verify reset exit\\\"\", \"dependsOn\": [ \"make all\", \"merge hex\" ], \"dependsOrder\": \"sequence\" }, { \"label\": \"merge hex\", \"group\": \"none\", \"type\": \"shell\", \"command\": \"resources/srec_cat.exe\", \"args\": [ \"bootloader/build/bootloader.hex\", \"-Intel\", \"application/build/application.hex\", \"-Intel\", \"-o\", \"firmware.hex\", \"-Intel\" ], \"osx\":{ // needs `brew install srecord` \"command\": \"srec_cat\", \"args\": [ \"bootloader/build/bootloader.hex\", \"-Intel\", \"application/build/application.hex\", \"-Intel\", \"-o\", \"firmware.hex\", \"-Intel\" ] }, \"problemMatcher\": [] } ] } Debug Debug的配置也类似，只不过是配置launch.json，具体可以参考上面环境配置的文章。下面是我的launch.json，供参考：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 { // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Debug Bootloader\", \"cwd\": \"${workspaceFolder}\", \"executable\": \"bootloader/build/bootloader.elf\", \"loadFiles\": [ \"bootloader/build/bootloader.elf\", \"application/build/application.elf\" ], \"symbolFiles\": [ { \"file\": \"bootloader/build/bootloader.elf\", }, { \"file\": \"application/build/application.elf\" } ], \"request\": \"launch\", \"type\": \"cortex-debug\", \"runToEntryPoint\": \"main\", \"servertype\": \"openocd\", \"showDevDebugOutput\": \"parsed\", \"configFiles\": [ \"bootloader/openocd.cfg\" ], \"svdFile\": \"bootloader/STM32H7B0x.svd\", \"device\": \"stlink\", \"preLaunchTask\": \"flash bootloader\" }, { \"name\": \"Debug Application\", \"cwd\": \"${workspaceFolder}\", \"executable\": \"bootloader/build/bootloader.elf\", \"loadFiles\": [ \"bootloader/build/bootloader.elf\", \"application/build/application.elf\" ], \"symbolFiles\": [ { \"file\": \"bootloader/build/bootloader.elf\", }, { \"file\": \"application/build/application.elf\" } ], \"request\": \"launch\", \"type\": \"cortex-debug\", \"runToEntryPoint\": \"main\", \"servertype\": \"openocd\", \"showDevDebugOutput\": \"parsed\", \"configFiles\": [ \"bootloader/openocd.cfg\" ], \"svdFile\": \"bootloader/STM32H7B0x.svd\", \"device\": \"stlink\", \"preLaunchTask\": \"flash application\" } ] } Linker Script配置 由于我们的application运行在外置的OSPI Flash，要想让OpenOCD默认把application烧录到OSPI Flash，需要首先修改一下Linker Script。其实修改也非常简单，就是把Flash区域的地址由修改到0x90000000，大小修改为4096K（即Flash的大小设置为了4M，这个可以根据你使用的spi flash芯片来定）。\n为什么需要把Flash区域的地址修改到0x90000000呢？其实这一点和使用MDK/IAR并没有什么不同，简单来说就是stm32要想在直接在OSPI内运行（即XIP），那么OSPI必须运行在memory mapped模式下。而memory mapped模式下的默认地址就是0x90000000。\n注意这里有个坑就是，在OSPI Flash打开memory mapped模式之后，正常的OSPI Flash的读写通信就全都会失败。如果你想使用驱动里面的读写或者其他函数（比如ReadID），那么必须关掉memory mapped模式。\nOpenOCD的配置 下面就来到了重点，也是最难的地方：OpenOCD的配置。\n对于大部分程序来说，其实只需要使用OpenOCD官方提供的默认配置就行了。但是，由于我们要在OSPI Flash上面XIP运行程序，官方的配置大概率是不能用的，就需要我们自己写openocd.cfg。\n在openocd.cfg中，实际上需要做以下几件事：\n指定debugger，在这里我们使用的是stlink 指定芯片，source芯片的基础配置 配置时钟 配置GPIO 配置ospi 其中，1和2都比较简单，不再展开，参考下面的代码即可。那为什么在OpenOCD中还需要配置时钟、GPIO和OSPI呢？这是因为我们想要直接把application烧录到OSPI Flash，在OpenOCD烧录的过程中，它需要知道你的OSPI的GPIO配置，以及OSPI Flash的配置，这样OpenOCD才能正确地和OSPI Flash通信，从而进行烧录。\n3/4/5的配置，本质上就是配置stm32的寄存器。在OpenOCD中可以使用mww和mmw对寄存器进行设置，mww命令在OpenOCD文档里面就有，而mmw是OpenOCD额外封装的一个命令，具体逻辑可以去OpenOCD的scripts里面找到。配置代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 、 # Choose debugger source [find interface/stlink.cfg] # Choose transport interfance transport select hla_swd # Set chip name set CHIPNAME stm32h7b0xx # Enable stmqspi if {![info exists OCTOSPI1]} { set OCTOSPI1 1 set OCTOSPI2 0 } # Use built-in stm32h7 openocd configs source [find target/stm32h7x.cfg] # OCTOSPI initialization proc octospi_init { octo } { global a b mmw 0x58024540 0x000006FF 0\t;# RCC_AHB4ENR |= GPIOAEN-GPIOKEN (enable clocks) mmw 0x58024534 0x00284000 0\t;# RCC_AHB3ENR |= IOMNGREN, OSPI2EN, OSPI1EN (enable clocks) sleep 1\t;# Wait for clock startup mww 0x5200B404 0x03010111\t;# OCTOSPIM_P1CR: assign Port 1 to OCTOSPI1 mww 0x5200B408 0x00000000\t;# OCTOSPIM_P2CR: disable Port 2 # AF mapping can be found in datasheet. For h7b0, see DS13196, page 54 # PB02: OCSPI1_CLK, PB06: OCSPI1_NCS, PD11: OCSPI1_IO0, PD12: OCSPI1_IO1, PE2: OCSPI1_IO2, PD13: OCSPI1_IO3 # Generate the GPIO config: perl resources/gpio_gen.pl -c \"PB06:AF10:V, PB02:AF09:V, PD13:AF09:V, PD12:AF09:V, PD11:AF09:V, PE02:AF09:V\" # mmw command: \"memory modify word, modify only given bits\" # usage: mmw \"address setbits clearbits\" # PB06:AF10:V, PB02:AF09:V, PD13:AF09:V, PD12:AF09:V, PD11:AF09:V, PE02:AF09:V # Port B: PB06:AF10:V, PB02:AF09:V mmw 0x58020400 0x00002020 0x00001010 ;# MODER mmw 0x58020408 0x00003030 0x00000000 ;# OSPEEDR mmw 0x5802040C 0x00000000 0x00003030 ;# PUPDR mmw 0x58020420 0x0A000900 0x05000600 ;# AFRL # Port D: PD13:AF09:V, PD12:AF09:V, PD11:AF09:V mmw 0x58020C00 0x0A800000 0x05400000 ;# MODER mmw 0x58020C08 0x0FC00000 0x00000000 ;# OSPEEDR mmw 0x58020C0C 0x00000000 0x0FC00000 ;# PUPDR mmw 0x58020C24 0x00999000 0x00666000 ;# AFRH # Port E: PE02:AF09:V mmw 0x58021000 0x00000020 0x00000010 ;# MODER mmw 0x58021008 0x00000030 0x00000000 ;# OSPEEDR mmw 0x5802100C 0x00000000 0x00000030 ;# PUPDR mmw 0x58021020 0x00000900 0x00000600 ;# AFRL # OCTOSPI1: memory-mapped 4-line read mode with 3-byte(24bits) addresses mww 0x52005130 0x00001000\t;# OCTOSPI_LPTR: deactivate CS after 4096 clocks when FIFO is full # Enter Memory mapped mode mww 0x52005000 0x3040000B\t;# OCTOSPI_CR: FMODE=0x11, APMS=1, FTHRES=0, FSEL=0, DQM=0, TCEN=0 mww 0x52005008 0x00160100\t;# OCTOSPI_DCR1: MTYP=0x0, FSIZE=0x16=22=2^(22+1), CSHT=0x00, CKMODE=0, DLYBYP=0 mww 0x5200500C 0x00000001\t;# OCTOSPI_DCR2: WRAPSIZE=0x00, PRESCALER=0+1 mww 0x52005108 0x00000008\t;# OCTOSPI_TCR: SSHIFT=0, DHQC=0, DCYC=0x8 mww 0x52005100 0x03002303\t;# OCTOSPI_CCR: SIOO=0, DMODE=011, ABMODE=0x0, ADSIZE=10, ADMODE=011, ISIZE=0x0, IMODE=011 mww 0x52005110 0x000000EB\t;# OCTOSPI_IR: INSTR=FastRead, 0xeb sleep 1 flash probe $a\t;# load configuration from CR, TCR, CCR, IR register values } # RCC配置，然后调用octospi_init $_CHIPNAME.cpu0 configure -event reset-init { global OCTOSPI1 global OCTOSPI2 mmw 0x52002000 0x00000004 0x0000000B\t;# FLASH_ACR: 4 WS for 64MHZ HCLK mmw 0x58024400 0x00000001 0x00000018\t;# RCC_CR: HSIDIV=1, HSI on mww 0x58024418 0x00000040\t;# RCC_CDCFGR1: CDCPRE=1, CDPPRE=2, HPRE=1 mww 0x5802441C 0x00000440\t;# RCC_CDCFGR2: CDPPRE2=2, CDPPRE1=2 mww 0x58024420 0x00000040\t;# RCC_SRDCFGR: SRDPPRE=2 mww 0x58024428 0x00404040\t;# RCC_PLLCKSELR: DIVM3=4, DIVM2=4, DIVM1=4, PLLSRC=HSI mww 0x5802442C 0x01ff0ccc\t;# RCC_PLLCFGR: PLLxRGE=8MHz to 16MHz, PLLxVCOSEL=wide mww 0x58024430 0x01010207\t;# RCC_PLL1DIVR: 64MHz: DIVR1=2, DIVQ1=2, DIVP1=2, DIVN1=8 mww 0x58024438 0x01010207\t;# RCC_PLL2DIVR: 64MHz: DIVR2=2, DIVQ2=2, DIVP2=2, DIVN2=8 mww 0x58024440 0x01010207\t;# RCC_PLL3DIVR: 64MHz: DIVR3=2, DIVQ3=2, DIVP3=2, DIVN3=8 mmw 0x58024400 0x01000000 0\t;# RCC_CR: PLL1ON=1 sleep 1 mmw 0x58024410 0x00000003 0\t;# RCC_CFGR: PLL1 as system clock sleep 1 adapter speed 4000 if { $OCTOSPI1 } { octospi_init 0 } } reset_config none separate 首先看$_CHIPNAME.cpu0 configure -event reset-init部分，这里是时钟的配置。具体哪一条命令配置的是哪个寄存器，注释里面已经很明白了。这里时钟最好和Bootloader里面配置的时钟一致，否则可能出现奇怪的问题（比如OSPI Flash获取不到ID之类的）。\n在时钟配置完毕之后，会判断是否开启了OCTOSPI1，如果开启，则使用octospi_init函数对OSPI进行配置。而OSPI的配置在上面proc octospi_init { octo }函数里。\n然后就是OSPI的具体配置了。OSPI的配置分两步，GPIO配置和OSPI本身的寄存器配置。对于GPIO配置，OpenOCD官方提供了一个perl脚本可以直接生成对应引脚的GPIO，perl脚本的链接在这里。具体使用可以参考我上面写的注释，需要配置好对应的接口和AF。而OSPI配置没有什么技巧，就是对照着OSPI的寄存器一个一个地配置。需要注意的是，这里的OSPI Flash的配置必须和Bootloader里面的OSPI Flash的配置完全一致，这样的话在烧录完Bootloader之后，OpenOCD才能够正确地使用这个配置去烧写Application到OSPI Flash。总而言之，在Cube里面的配置、OpenOCD的配置和Flash的配置最好都要一致，才不会出现奇怪的问题。\n验证配置 在配置完毕OpenOCD之后，可以手动验证一下配置。这一步非常重要，防止后面出问题不知道去哪里排查。首先进入你的application文件夹，然后使用命令打开OpenOCD监听4444端口：\n1 openocd -f openocd.cfg 然后，使用telnet(windows/linux)或者nc连接到OpenOCD：\n# Windows/Linux telnet localhost 4444 # MacOS nc localhost 4444 连接到之后，就可以任意输入OpenOCD的命令了，在这里我们使用\n1 flash probe 1 来查看OSPI Flash的信息（你也可以尝试一下flash probe 0，看看输出的信息）：\n\u003e flash probe 1 flash probe 1 valid SFDP detected flash1 'sfdp' id = 0x333333 size = 8192 KiB flash 'stmqspi' found at 0x90000000 可以看到OpenOCD已经识别出了在0x90000000位置的stmqspi。然后我们可以使用flash info 1来查看Flash的详细信息：\n可以验证一下下面的Flash信息配置和你使用的Flash的DataSheet中的配置是否一致。如果一致，那么应该是没有问题的。\n看到这里，你可能会有一个问题就是，为什么这里的Flash名称显示的是sfdp，而前面烧录的时候会正确显示Flash的名称w25q64jv？\n这里原理我也没有弄明白，但是我猜测是因为OSPI的Memory mapped模式已经打开，这样的话GetID的命令就会失效。不过这个时候，OpenOCD只需要读取sfdp的寄存器就可以知道用那些命令操作Flash了。\n编译工程，合并Hex，烧录 由于我们是Bootloader + Application两个工程，在编译的时候需要两个工程一起编译，编译出来是两个固件文件。想要实现一键烧录，有两个方案：\n分别烧录两个固件到对应位置 把两个固件合并成一个烧录 其实两个方案没太大区别，为了后续发布简单，我选择了方案2。方案2有一个问题就是，不能使用bin格式的固件。这是因为bin格式在内存上是连续的，而我们的固件实际上是烧录在两个位置：内置Flash（0x08000000)和OSPI Flash（0x90000000）。如果使用bin格式的话，在合并之后，就会产生一个好几个G的超大固件，这显然是不对的。所以在这里我们使用hex格式，hex格式在文件内有存储每一个块烧录的内存位置，因此我们只需要使用三方工具把Application和Bootloader的固件合并成一个，然后在烧录的时候就会自动地把Bootloader烧录到0x08000000，把OSPI Flash烧录到0x90000000。\n我们选择的合并工具是srecord，也是非常著名的hex合并工具，全平台都有对应版本。安装上面已经讲过，不再赘述。想要合并hex，使用以下命令即可：\n1 srec_cat bootloader.hex -Intel application.hex -Intel -o merged.hex -Intel 代码实际上都写在了VSCode的task中，执行task就会自动完成hex的合并。\n烧录就简单了：\n1 openocd -f bootloader/openocd.cfg -c \\\"program firmware.hex preverify verify reset exit\\\" 这里需要注意的有两点：\n使用的是bootloader文件夹下的openocd.cfg，bootloader和application的openocd.cfg最好保持一致 使用的是OpenOCD的program命令，而不是很常见的flash erase_xxxx。program命令是对flash命令的额外一层封装，还提供了预校验、烧写之后的校验、自动退出重启等功能，强烈建议使用。 ","description":"","tags":["keyboard","stm32","embedding"],"title":"使用OpenOCD+VSCode一键烧录Boot+App到内置+外置flash","uri":"/posts/keyboard/openocd-ospi-flash/"},{"categories":null,"content":"OpenOCD OpenOCD是一款开源的针对嵌入式设备的调试器，可以用来烧录、调试很多嵌入式设备。\n安装 在mac上面安装OpenOCD非常简单，使用brew安装就好。需要注意的是，brew里面默认的版本比较老旧，在安装时最好使用:\n1 brew intall openocd --HEAD 在Windows上面安装则更为简单：去OpenOCD的Github的Release页面下载最新版本的OpenOCD安装文件即可。\nOpenOCD入门 OpenOCD内置了对很多MCU和开发板的支持，这些都可以去${安装目录}/share/openocd/scripts下获取。一般来说，只需要在interface文件夹下面找到你使用的debugger，然后在board文件夹下面找到你所使用的板子或者在target目录下找到你使用的MCU，即可以不做任何改动，直接使用OpenOCD进行烧录和调试：\n1 openocd -f interface/cmsis-dap.cfg -f target/stm32h7x.cfg 这是默认选项，也是大多数人的用法。不过在这里，如果我们想要对OpenOCD的烧录、调试选项做更加深入细致地定制的话，就必须熟悉OpenOCD的配置文件，也就是-f选项后面的文件。OpenOCD自带的这些文件是很好的参考，后面我们也会参考这些文件，针对我们自己的板子写出对应的配置文件。\nOpenOCD的配置文件 OK，到现在为止我们已经入门了OpenOCD，现在就进一步学习OpenOCD的配置文件。默认情况下（即不使用-f指定配置文件时），OpenOCD会使用当前目录下的openocd.cfg文件作为配置文件。\n在写我们自己的配置文件时，我们仍然可以复用scripts 目录下的配置文件，像这样：\n# 引用stlink.cfg source [find interface/stlink.cfg] 大多数情况下，你所使用的debugger的配置文件都会随OpenOCD提供，引用即可。\n下面，针对我们使用的debugger，需要去指定通信类型。比如，对于stlink，我们使用hla_swd：\n# Choose transport interfance transport select hla_swd 或者对于cmsis-dap，我们使用swd：\nsource [find interface/cmsis-dap.cfg] transport select swd 再然后，就可以看一下我们使用的MCU是否在targets里面了。绝大多数情况下，是在的，那么我们可能需要进行一些些的配置。比如，指定芯片名称：\nset CHIPNAME stm32h7x 再比如，打开octo-spi的开关：\n# Enable stmqspi if {![info exists OCTOSPI1]} { set OCTOSPI1 1 set OCTOSPI2 0 } 然后直接引用对应的内置配置文件即可：\n# Use built-in stm32h7 openocd configs source [find target/stm32h7x.cfg] source [find board/stm32h7x_dual_qspi.cfg] 这里，由于我们已经事先看了这两个文件，知道打开octo-spi要把OCTOSPI1配置为1。对于不同的MCU，最好去看一下对应target下的配置文件，这样你就能够知道可以配置哪些东西，需要配置哪些东西。\nOpenOCD提供了非常非常多的配置选项，其内置的各种target的选项就是非常好的参考。如果发现有命令不太明白，可以去官方文档查询。当你能够看明白一个MCU的完整的配置文件，那么相信自己写一写也不在话下了。\nOpenOCD的命令 在写完配置文件之后，我们就可以使用OpenOCD进行实际的调试了。在调试之前，我们需要学习一下最常用的OpenOCD的命令，这些命令在调试中都非常常用。\nhalt：暂停CPU运行，在执行烧录命令之前必须先halt，否则CPU不会理你的\nflash：flash实际上包含很多子命令，常用的有flash info，flash list，falsh banks, flash write_image，flash verify_image等。具体用法参见官方手册\nflash write_image erase [filename] [address]：烧录指定文件到对应地址。对于stm32h7，如果烧录elf文件，不需要加地址。而如果烧录bin文件，则需要加地址： flash write_image erase build/h7b0.bin 0x08000000 # 或者 flash write_image erase build/h7b0.elf reset：复位\ninit：初始化MCU\nreset halt：复位并且立刻暂停CPU\nmdx(mdd/mdw/mdh/mdb)：显示对应地址的数据（memory display），mdd是64bit，mdw是32bit，mdh是16bit，mdb是8bit\nmwx(mwd/mww/mwh/mwb)：往对应地址写入，各个命令和上面类似\nprogram: OpenOCD提供了program命令，相当于是flash命令的高级封装。可以直接使用如下一条命令，完成初始化、停止、烧录、重启、退出等一系列命令可以完成的事情：\n1 2 3 4 5 6 7 8 9 # 一条命令完成烧写 program build/h7b0.elf verify reset exit # 下面一堆命令也一样 init halt flash write_image erase build/h7b0.elf reset shutdown stmqspi OpenOCD支持stm32系列MCU的qual-spi和octo-spi，可以使用spi把外部flash映射到内部的地址空间，这些flash可以自动地被OpenOCD检测到。在这种模式下，MCU可以直接读取对应的内存，执行flash里面的代码。不过需要注意的是，MCU不能直接从外置flash区域启动。因此，在具体的实现代码里面，必须在内置flash区（或者说boot代码里面），配置好qspi或者ospi的memory mapping。这样OpenOCD才能使用地址映射去操作这部分内存。下面是测试命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 首先，进入OpenOCD nc localhost 4444 # 之后，可以使用如下命令查看spi-flash # 查看flash列表，可以看到有内置和外置两个flash $ flash list \u003e {name stm32h7b0xx.bank1.cpu0 driver stm32h7x base 134217728 size 0 bus_width 0 chip_width 0 target stm32h7b0xx.cpu0} {name stm32h7b0xx.octospi1 driver stmqspi base 2415919104 size 0 bus_width 0 chip_width 0 target stm32h7b0xx.cpu0} # 获取内置flash的信息 $ flash info 0 \u003e Device: STM32H7Ax/7Bx flash size probed value 128k STM32H7 flash has a single bank Bank (0) size is 128 kb, base address is 0x08000000 #0 : stm32h7x at 0x08000000, size 0x00020000, buswidth 0, chipwidth 0 # 0: 0x00000000 (0x8000 32kB) not protected # 1: 0x00008000 (0x8000 32kB) not protected # 2: 0x00010000 (0x8000 32kB) not protected # 3: 0x00018000 (0x8000 32kB) not protected STM32H7Ax/7Bx - Rev: unknown (0x1001) # 获取外部flash信息 $ flash info 1 \u003e flash1 'win w25q64fv/jv' id = 0x1740ef size = 8192 KiB #1 : stmqspi at 0x90000000, size 0x00800000, buswidth 0, chipwidth 0 # 0: 0x00000000 (0x10000 64kB) not protected # 1: 0x00010000 (0x10000 64kB) not protected ... 省略 #126: 0x007e0000 (0x10000 64kB) not protected #127: 0x007f0000 (0x10000 64kB) not protected flash1 'win w25q64fv/jv', device id = 0x1740ef, flash size = 8192Ki B (page size = 256, read = 0x03, qread = 0xeb, pprog = 0x02, mass_erase = 0xc7, sector size = 64 KiB, sector_erase = 0xd8) 可以看到，OpenOCD甚至连外部flash的芯片型号（w25q64）都能都读取到。除了list和info命令外，还可以使用banks和probe命令：\n这里需要注意的是，在OpenOCD进行烧录的时候，通常不会启动芯片。因此对于外置SPI Flash，一般还需要通过手动写寄存器的方式来进行时钟的初始化和SPI外设的初始化。这样才能让OpenOCD正确地识别到目标芯片。如果你使用的芯片在OpenOCD已经内置，那么一般可以直接使用，直接source即可。如果没有内置或者你有自己的设置（比如SPI GPIO配置不同），那么就需要自己写了。下面是一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # OCTOSPI 初始化 proc octospi_init { octo } { global a b mmw 0x58024540 0x000006FF 0\t;# RCC_AHB4ENR |= GPIOAEN-GPIOKEN (enable clocks) mmw 0x58024534 0x00284000 0\t;# RCC_AHB3ENR |= IOMNGREN, OSPI2EN, OSPI1EN (enable clocks) sleep 1\t;# Wait for clock startup mww 0x5200B404 0x03010111\t;# OCTOSPIM_P1CR: assign Port 1 to OCTOSPI1 mww 0x5200B408 0x00000000\t;# OCTOSPIM_P2CR: disable Port 2 # AF mapping can be found in datasheet. For h7b0, see DS13196, page 54 # PB02: OCSPI1_CLK, PB06: OCSPI1_NCS, PD11: OCSPI1_IO0, PD12: OCSPI1_IO1, PE2: OCSPI1_IO2, PD13: OCSPI1_IO3 # Generate the GPIO config: perl resources/gpio_gen.pl -c \"PB06:AF10:V, PB02:AF09:V, PD13:AF09:V, PD12:AF09:V, PD11:AF09:V, PE02:AF09:V\" # mmw command: \"memory modify word, modify only given bits\" # usage: mmw \"address setbits clearbits\" # PB06:AF10:V, PB02:AF09:V, PD13:AF09:V, PD12:AF09:V, PD11:AF09:V, PE02:AF09:V # Port B: PB06:AF10:V, PB02:AF09:V mmw 0x58020400 0x00002020 0x00001010 ;# MODER mmw 0x58020408 0x00003030 0x00000000 ;# OSPEEDR mmw 0x5802040C 0x00000000 0x00003030 ;# PUPDR mmw 0x58020420 0x0A000900 0x05000600 ;# AFRL # Port D: PD13:AF09:V, PD12:AF09:V, PD11:AF09:V mmw 0x58020C00 0x0A800000 0x05400000 ;# MODER mmw 0x58020C08 0x0FC00000 0x00000000 ;# OSPEEDR mmw 0x58020C0C 0x00000000 0x0FC00000 ;# PUPDR mmw 0x58020C24 0x00999000 0x00666000 ;# AFRH # Port E: PE02:AF09:V mmw 0x58021000 0x00000020 0x00000010 ;# MODER mmw 0x58021008 0x00000030 0x00000000 ;# OSPEEDR mmw 0x5802100C 0x00000000 0x00000030 ;# PUPDR mmw 0x58021020 0x00000900 0x00000600 ;# AFRL # OCTOSPI1: memory-mapped 4-line read mode with 3-byte(24bits) addresses mww 0x52005130 0x00001000\t;# OCTOSPI_LPTR: deactivate CS after 4096 clocks when FIFO is full # Enter Memory mapped mode mww 0x52005000 0x3040000B\t;# OCTOSPI_CR: FMODE=0x11, APMS=1, FTHRES=0, FSEL=0, DQM=0, TCEN=0 mww 0x52005008 0x00160100\t;# OCTOSPI_DCR1: MTYP=0x0, FSIZE=0x16=22=2^(22+1), CSHT=0x00, CKMODE=0, DLYBYP=0 mww 0x5200500C 0x00000001\t;# OCTOSPI_DCR2: WRAPSIZE=0x00, PRESCALER=0+1 mww 0x52005108 0x00000008\t;# OCTOSPI_TCR: SSHIFT=0, DHQC=0, DCYC=0x8 mww 0x52005100 0x03002303\t;# OCTOSPI_CCR: SIOO=0, DMODE=011, ABMODE=0x0, ADSIZE=10, ADMODE=011, ISIZE=0x0, IMODE=011 mww 0x52005110 0x000000EB\t;# OCTOSPI_IR: INSTR=FastRead, 0xeb flash probe $a\t;# load configuration from CR, TCR, CCR, IR register values } # RCC时钟配置，然后调用octospi_init $_CHIPNAME.cpu0 configure -event reset-init { global OCTOSPI1 global OCTOSPI2 mmw 0x52002000 0x00000004 0x0000000B\t;# FLASH_ACR: 4 WS for 64MHZ HCLK mmw 0x58024400 0x00000001 0x00000018\t;# RCC_CR: HSIDIV=1, HSI on mww 0x58024418 0x00000040\t;# RCC_CDCFGR1: CDCPRE=1, CDPPRE=2, HPRE=1 mww 0x5802441C 0x00000440\t;# RCC_CDCFGR2: CDPPRE2=2, CDPPRE1=2 mww 0x58024420 0x00000040\t;# RCC_SRDCFGR: SRDPPRE=2 mww 0x58024428 0x00404040\t;# RCC_PLLCKSELR: DIVM3=4, DIVM2=4, DIVM1=4, PLLSRC=HSI mww 0x5802442C 0x01ff0ccc\t;# RCC_PLLCFGR: PLLxRGE=8MHz to 16MHz, PLLxVCOSEL=wide mww 0x58024430 0x01010207\t;# RCC_PLL1DIVR: 64MHz: DIVR1=2, DIVQ1=2, DIVP1=2, DIVN1=8 mww 0x58024438 0x01010207\t;# RCC_PLL2DIVR: 64MHz: DIVR2=2, DIVQ2=2, DIVP2=2, DIVN2=8 mww 0x58024440 0x01010207\t;# RCC_PLL3DIVR: 64MHz: DIVR3=2, DIVQ3=2, DIVP3=2, DIVN3=8 mmw 0x58024400 0x01000000 0\t;# RCC_CR: PLL1ON=1 sleep 1 mmw 0x58024410 0x00000003 0\t;# RCC_CFGR: PLL1 as system clock sleep 1 adapter speed 4000 if { $OCTOSPI1 } { octospi_init 0 } } OpenOCD官方有提供根据你的Pin的配置自动生成SPI GPIO配置的工具，在官方库的gpio_gen.pl文件中，需要perl执行。这样就不用对照着手册一个一个地自己去算寄存器的值了，非常的方便。\n","description":"","tags":["keyboard","embedding"],"title":"OpenOCD","uri":"/posts/keyboard/openocd/"},{"categories":null,"content":"Makefile 程序的编译和链接 首先需要了解一下程序的编译和链接。编译就是把源代码编译成目标（obj）文件，而链接是把所有的目标文件（包括函数、全局变量等）链接生成可执行文件。\nMakefile介绍 使用make命令就需要Makefile文件。Makefile文件会告诉make命令如何编译、链接程序。\nMakefile规则 首先看一下Makefile中最基础的规则：\n目标：依赖 命令 规则很简单：当依赖更新时，执行命令生成目标。这个简单的规则就是Makefile最核心的内容。下面稍微介绍一下这三个概念：\n目标：目标文件，可以是编译产物（obj）、链接产物（可执行文件），还可以是标签（Label，后面会解释） 依赖：生成目标所依赖的文件或者其他目标 命令：一般是shell命令，用于从依赖生成目标。make会比较目标和依赖的生成时间，如果依赖比较新，那么就会执行命令 下面是一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 最终目标文件--可执行文件 edit : main.o kbd.o cc -o edit main.o kbd.o # 中间目标文件 main.o : main.c defs.h cc -c main.c kbd.o : kbd.c defs.h command.h cc -c kbd.c # 伪目标文件 clean : rm edit main.o kbd.o 可以看到里面定义的3个目标，第一个是生成edit可执行文件，第二个是编译中间文件，第三个是clean。需要提到的是，这里目标clean并不是一个文件，而是一个label。可以看到这个目标没有任何依赖，那就说明只要运行make clean，下面的命令一定会被执行。\nmake是如何工作的 在默认情况下， 直接执行make那么：\nmake会去找Makefile Makefile中，找到第一个目标，作为终极目标 然后嵌套地去根据终极目标的所有依赖去找其他目标，然后根据需要执行命令 需要注意的是，make不会管命令的错误，它只管文件的依赖性，在依赖找不到的情况下直接报错退出，而命令报错则不会。\nMakefile中的变量 和C语言的宏类似，在Makefile中可以定义变量，以提升Makefile的可读性和可维护性。变量的定义和使用很简单：\n1 2 3 4 # 定义objects变量 objects = main.o kbd.o # 使用objects变量 edit : $(objects) make的自动推导 make有自动推导的能力，即它会记住前面的命令，这样就不用每次再写同样的命令了。 另外，make还有一个功能就是，每当make看到一个.o文件，它都会把对应的.c文件加入到依赖里面。 使用这两个特性，我们的Makefile就变成了这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 变量定义 objects = main.o kbd.o # 最终目标文件 edit : $(objects) cc -o edit $(objects) # 中间目标文件 main.o : defs.h kbd.o : defs.h command.h # 伪目标文件 .PHONY : clean clean : rm edit $(objects) 清空目标文件的规则 每个Makefile中都应该写一个清空目标文件（.o和执行文件）的规则，这不仅便于重编译，也很利于保持文件的清洁。还记的之前例子里面的clean目标吗？没错就是这个。不过更好的写法是：\n1 2 3 .PHONY : clean clean : -rm edit $(objects) 这里，.PHONY表示clean是一个伪目标，而rm前面的-表示也许某些文件出现问题，但不要管，继续做后面的事。 另外，还有一个不成文的规矩是，clean一般都放在Makefile的最后（相对应地，第一个目标是默认目标或者说终极目标）。\nMakefile总述 上面介绍了关于make和Makefile的基础知识，下面我们就深入了解Makefile。\n选择执行的Makefile 默认情况下， make会使用当前目录下的Makefile或者makefile。当然你也可以手动地指定Makefile：\n1 2 make -f your_make_file make --file your_make_file 引用其他的Makefile 在一个Makefile中，你也可以引用其它的Makefile，只需要使用include关键字即可：\n1 2 3 4 # 引入其他makefile include foo.make *.mk $(many_makefiles) # 如果文件不存在，也继续 -include not_exist.make 被引入的Makefile会原样地放在这里（当然，也是递归地搜索所有依赖的Makefile）。如果在include前面有一个-，则说明即使有问题也继续（和上面命令里面的含义一样）。\nMakefile的书写规则 在Makefile里，规则的顺序是非常重要的，这是因为Makefile中只有一个终极目标。一般来说，第一个规则里面的目标就是终极目标，如果第一个规则里有很多目标，那其中第一个目标是终极目标。整个make命令所要完成的也是这个目标。 在上面介绍中，我们已经介绍了规则的写法。实际上，规则还有一种写法：\n1 targets : prerequisites ; command 目标和依赖都可以是多个文件，以空格分隔。而命令如果和目标/依赖在同一行，那么必须分号隔开；如果不在同一行，那么新行必须以TAB开头：\n1 2 3 targets : prerequisites command # ↑ 注意这里是tab 通配符 Makefile支持3种通配符（wildcard）：*, ?,[...]。定义和linux里面一样，不再细说了。\n伪目标 在上面的代码中，我们提到了.PHONY定义的是一个伪目标。为啥叫伪目标呢，这是因为实际上clean那个命令并不是一个目标文件，而只是一个标签。执行make clean也不生成任何目标文件，所以叫伪目标。 伪目标在Makefile中很有用，比如如果你想使用make一下子生成好多文件，那么就可以这样写：\n1 2 3 4 5 6 7 8 9 10 11 all : prog1 prog2 prog3 .PHONY : all prog1 : prog1.o utils.o cc -o prog1 prog1.o utils.o prog2 : prog2.o cc -o prog2 prog2.o prog3 : prog3.o sort.o utils.o cc -o prog3 prog3.o sort.o utils.o 这里，我们的终极目标all实际上就是一个伪目标，然后它依赖了三个目标。这样，在执行make all的时候，下面三个目标会总是被执行。\n静态模式规则 在很多时候，我们的目标和依赖都是很相似的，如下面的规则：\n1 2 3 4 a.o : a.c $(CC) -c $(CFLAGS) a.c -o a.o b.o : b.c $(CC) -c $(CFLAGS) b.c -o b.o 这个时候，我们就可以使用模式规则：\n1 2 targets: target_pattern : prereq_pattern commands 这里，目标还是保持不变，但是后面的依赖变为了由模式生成。一般最常用的就是%.o: %.c，即把target中所有的.o文件都替换成.c文件：\n1 2 objects = foo.o bar.o $(objects): %.o: %.c 结合Makefile中的filter函数，就可以批量生成成千上万的.o文件了：\n1 2 3 files = foo.elc bar.o lose.o $(filter %.o,$(files)): %.o: %.c $(CC) -c $(CFLAGS) $\u003c -o $@ 这里，又有了三个新知识点：filter函数、$\u003c和$@。filter是make自带的函数，而的和$\u003c和$@则是自动化变量，$\u003c表示所有的依赖目标集（也就是foo.c、bar.c），$@表示目标集（也就是foo.o、bar.o）。关于函数和自动化变量，我们后面还会详细说明。\n","description":"","tags":["makefile","c","c++"],"title":"Learn Makefile - part 1","uri":"/posts/engineering/makefile/"},{"categories":null,"content":"使用FMC驱动8080接口屏幕 FMC STM32中的FMC主要是用来控制外接存储。由于8080接口的读写时序和很多外接存储很相似，所以FMC可以用来直接驱动8080接口的屏幕。CubeMX中也提供了相应的功能，非常简单方便。\nSTM32会把外接RAM映射为4个bank，每个bank对应一个地址区。FMC可以直接写数据到这些地址，而不用在手写复杂的外接RAM时序：\n8080接口 FMC和8080屏幕的接口对应如下：\nFMC D[0:15]：数据线D0~D15 FMC NEx：片选信号 FMC NOE：读使能 FMC NWE：写使能 FMC Ax：地址线，命令/数据选择 下图是FMC和8080接口的典型连接：\n时序 在配置FMC驱动8080接口屏幕时，时序的配置非常重要。首先我们可以了解下8080接口的时序（一般具体的时序值请查阅驱动芯片的datasheet）：\n在驱动芯片的datasheet中找到这些值，后面在配置FMC时序时会用到。\n在我使用是st7789v驱动芯片中，这些值为：\nt_ah: 10 ns t_as: 0 ns t_cyc: 66 ns t_cyc(read): 160/450 ns（读屏幕RAM/读ID） t_wrlw: 15ns t_wrlr: 355 ns/45 ns（读屏幕RAM/读ID） t_wrhw: 15 ns t_wrhr: 90 ns t_ds: 10 ns t_dh: 10ns t_acc: \u003c 340ns/40ns（读屏幕RAM/读ID）。t_acc：10ns对于读来说 t_od: 20~80ns 根据这些时间，另外可以在CubeMX中，看到FMC运行的时钟频率（如280MHZ），那么一个tick就是3.57ns。根据这些参数，我们就可以去设置FMC的时间参数。计算方法如下：\n当然这里是针对F103的FSMC。对于我们用的h7b0 + st7789v的组合，经过我好长时间的调试，发现应该在代码里面这么设置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /* Timing */ Timing.AddressSetupTime = 0; Timing.AddressHoldTime = 0; Timing.DataSetupTime = 18; Timing.BusTurnAroundDuration = 0; Timing.CLKDivision = 2; Timing.DataLatency = 2; Timing.AccessMode = FMC_ACCESS_MODE_A; /* ExtTiming */ ExtTiming.AddressSetupTime = 0; ExtTiming.AddressHoldTime = 0; ExtTiming.DataSetupTime = 10; ExtTiming.BusTurnAroundDuration = 0; ExtTiming.CLKDivision = 2; ExtTiming.DataLatency = 2; ExtTiming.AccessMode = FMC_ACCESS_MODE_A; 其中，Timing是读时序，而ExtTiming是写时序。\n","description":"","tags":["stm32"],"title":"FMC","uri":"/posts/keyboard/fmc/"},{"categories":null,"content":"TouchGFX 文档链接：https://support.touchgfx.com/4.20/zh-CN/docs/category/introduction\n学习TouchGFX的步骤 首先，开发基于stm32的TouchGFX应用可以分成四个步骤如下图：\n前两个步骤硬件选择和开发板配置和TouchGFX关系不大，因此我们直接跳过，从TouchGFX本身的开发开始。首先我们需要学习TouchGFX的抽象层的开发，即TouchGFX AL\nTouchGFX的抽象层（AL） TouchGFX AL由两个部分组成，分别是硬件抽象层（HAL）和操作系统抽象层（OSAL）。顾名思义，这两个抽象层实际上就充当了TouchGFX和硬件以及操作系统连接的桥梁，把硬件、TouchGFX引擎以及操作系统连接起来，从而构建在单片机上运行的图形界面程序。\n抽象层的职责 那么，抽象层具体负责哪些事情呢？这里有一个表格，里面列举了抽象层的具体职责。简单来说，就是如下几件事情：\n在TouchGFX引擎的主循环中，会自动地调用AL层的若干个钩子函数（或者触发中断），从而完成上述的这些事情：\n我们需要做的就是去开发这些钩子函数，从而搭建起TouchGFX到硬件以及操作系统的桥梁。下面我们会介绍每个钩子的具体职责和用法。\n将TouchGFX Engine主循环与显示器传输同步 此步骤背后的主要思想是，在渲染完成后阻塞TouchGFX Engine主循环，从而确保在显示设备准备好之前不再产生其他帧。 一旦显示设备准备就绪，OSAL向被阻塞的Engine主循环发出信号，以继续产生（生产、渲染）显示帧。\n开发者可以通过Rendering done钩子以及Display Ready中断来完成这个步骤，然后通过使用OSAL中的OSWrappers::signalVSync来完成同步。\n下面会介绍Rendering done和Display Ready。\n渲染完成（Rendering done） 渲染完成钩子即OSWrappers::waitForVSync会在渲染完成之后被TouchGFX引擎自动调用。开发者在实现这个AL方法的时候，必须阻塞图形引擎，直到渲染下一帧。标准的实现方式是从一个消息队列中读取，读不到则阻塞。当OSWrapper::signalVSync发出信号时，OSWrappers::waitForVSync会收到信号并且解除阻塞，然后TouchGFX会开始渲染下一帧。\n显示就绪（Display Ready） Display Ready中断应该来自显示屏（或者相关controller、定时器），这个中断会触发主循环停止阻塞并继续渲染下一帧。在中断触发之后，就应该去调用OSWrappers::signalVsync来使上个section介绍的OSWrappers::waitForVsync解除阻塞。\n下面是一个基于RTOS的实现示例，如果使用TouchGFX Generator，RTOS的代码会自动地生成好。其他OS的需要参考这些代码自行实现OSWrappers：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // RTOS_OSWrappers.cpp // static osMessageQId vsync_queue = 0; //Queue identifier is assigned elsewhere // 在中断中调用该函数，往vsync_queue中放一个值，waitForVSync收到之后会通知引擎继续渲染下一帧 void OSWrappers::signalVSync() { if (vsync_queue) { osMessagePut(vsync_queue, dummy, 0); } } // 在渲染完一帧之后，在这里阻塞，直到收到队列中的值，继续下一帧渲染 void OSWrappers::waitForVSync() { uint32_t dummyGet; // First make sure the queue is empty, by trying to remove an element with 0 timeout. osMessageQueueGet(vsync_queue, \u0026dummyGet, 0, 0); // Then, wait for next VSYNC to occur. osMessageQueueGet(vsync_queue, \u0026dummyGet, 0, osWaitForever); } 触摸与其他外部事件 比较简单，看看就行。说白了就两种方式：轮询/中断。\n见：https://support.touchgfx.com/4.20/zh-CN/docs/development/touchgfx-hal-development/touchgfx-architecture#report-touch-and-physical-button-events\n同步帧缓冲访问 帧缓冲（framebuffer）是整个图形系统要操作的核心数据。在整个显示过程中，会有很多个硬件访问帧缓冲，如CPU、DMA2D、LTDC等，因此TouchGFX AL必须提供一种保护该存储器的方式，从而不让数据读写出现问题（比如CPU写一半，DMA2D就去读取了）。\nTouchGFX通过OSWrappers中的接口来同步帧缓冲的访问，最常规的实现方式是通过信号量来保护帧缓冲的访问，当然也可以自行实现其他方式。下表定义了OSWrappers中对访问帧缓冲需要用到的函数列表，这些函数可以由TouchGFX Generator生成（for RTOS），也可以自行实现：\n报告下一个可用的帧缓冲区 TouchGFX必须在每个Tick知道下一个可用的帧缓冲区。帧缓冲的策略可以有多种，使用单帧缓冲或双帧缓冲时，TouchGFX Engine将根据帧缓冲的全宽、高度和位宽将像素数据写入存储区。而对于部分缓存，可以参见这里。\n执行渲染操作 很多stm32的MCU都提供了把内容渲染到帧缓冲的外设，如DMA2D。在渲染内容到帧缓冲之前，TouchGFX引擎会首先检查HAL层是否实现了渲染功能。如果实现了，那么这个操作就会直接交给HAL层完成，否则会交给CPU处理。\nTouchGFX引擎会调用HAL::getBlitCaps()来获取硬件的能力描述，开发者需要实现对应的HAL子类来把MCU的硬件渲染能力添加进来。然后，TouchGFX引擎会使用HAL类中定义的具体操作，如HAL::blitCopy()等，去执行具体的渲染工作。如果HAL没有实现对应的函数，TouchGFX会默认使用CPU完成这些事情。\n把帧缓冲传输到显示设备 每当一部分帧缓冲渲染完成之后，TouchGFX引擎会调用Rendering of area complete钩子来通知AL层去把渲染完成的那部分帧缓冲传输到显示设备。\nRendering of area complete钩子 在代码里面，这个钩子是一个虚函数：HAL::flushFrameBuffer(Rect\u0026 rect)。在LTDC中我们不用管这个东西，因为LTDC会自动把这些事情做了。这个函数留空即可。而对于其他的接口，如SPI/8080，则需要开发者手动实现该函数实现对帧缓冲的传输。\n对于带GRAM的显示设备（即显示设备里面也有RAM，并口屏常见），这个函数的实现允许手动发起向GRAM的帧缓冲区域的传输：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void TouchGFXHAL::flushFrameBuffer(const touchgfx::Rect\u0026 r) { HAL::flushFrameBuffer(rect); //call superclass //start transfer if not running already! if (!IsTransmittingData()) { const uint8_t* pixels = ...; // Calculate pixel address SendFrameBufferRect((uint8_t*)pixels, r.x, r.y, r.width, r.height); } else { ... // Queue rect for later or wait here } } TouchGFX的硬件抽象层（HAL） 了解了抽象层各个钩子的职责之后，首先看一下硬件抽象层（HAL）。\nTouchGFX Generator TouchGFX Generator是开发TouchGFX应用的必备工具，和CubeMX类似，TouchGFX Generator是用来生成TouchGFX代码的工具，里面提供了有关显示的各种配置。和CubeMX不同的是，TouchGFX生成的是C++代码，通过继承向开发者提供灵活性。\n在CubeMX里面开启TouchGFX Generator之后，生成的代码会自动创建一个TouchGFX文件夹并且把相关代码文件都生成进去。下面是TouchGFX项目的项目结构：\n│ .mxproject │ myproject.ioc ├───Core ├───Drivers ├───EWARM ├───Middlewares └───TouchGFX │ ApplicationTemplate.touchgfx.part ├───App │ app_touchgfx.c │ app_touchgfx.h └───target │ STM32TouchController.cpp │ STM32TouchController.hpp │ TouchGFXGPIO.cpp │ TouchGFXHAL.cpp │ TouchGFXHAL.hpp │ └───generated OSWrappers.cpp TouchGFXConfiguration.cpp TouchGFXGeneratedHAL.cpp TouchGFXGeneratedHAL.hpp 在这里我们只关心TouchGFX文件夹下的内容：\nApplicationTemplate.touchgfx.part：TouchGFX Designer工程相关的一些配置，比如屏幕尺寸、位深等\nApp：X-Cube接口，其中app_touchgfx.c包含MX_TouchGFX_Process(void)和MX_TouchGFX_Init(void)函数，这些函数用于在CubeMX生成的主函数中初始化TouchGFX以及开始主循环\ntarget/generated：注意这个目录是只读的，包含根据相关配置生成的TouchGFX源文件。简单来说，TouchGFXGeneratedHAL是自动生成的HAL层，OSWrappers是OSAL层。而TouchGFXConfiguration是TouchGFX的相关配置，包含一个用于构建HAL的函数以及启动TouchGFX主函数的函数。\ntarget：这个目录是开发者的代码所在，可以继承target/generated里面的类从而实现对HAL/OSAL的覆写。也可以拓展HAL，添加新功能等等。生成的STM32TouchController.cpp文件包含了触摸控制的空接口，而TouchGFXHAL.cpp定义了TouchGFXGeneratedHAL的子类。\n需要注意的是，对于HAL来说，TouchGFXHAL会继承TouchGFXGeneratedHAL，用户可能需要修改TouchGFXHAL来完成HAL的其他配置。HAL的一般架构如下所示：\n","description":"","tags":["keyboard","stm32"],"title":"TouchGFX","uri":"/posts/keyboard/touchgfx/"},{"categories":null,"content":"QMK - 2 QMK's source code can be found at Github: https://github.com/qmk/qmk_firmware.\nLearn QMK's source code Basic project structure There are three logic levels in QMK project:\nCore(_quantum) -- Keyboard/Revision(_kb) -- Keymap(_user) In QMK, many custom functions have a _kb or _user suffix. By convention, when you customize your keyboard or a revision of your keyboard, using _kb functions. And when you customize your keymap, use _user functions.\nRemember to call _user function at the beginning of your _kb functions. Otherwise, those _user functions won't be execute any more.\nProgram Entry Like other C programs, QMK's entry is a main() function. QMK's main function is at quantum/main.c, which is the entrance of all the QMK firmware.\nQMK's main() function is quite simple: setup platform/protocol/keyboard and then run the infinite main loop. The main loop will call protocol_task(), then the keyboard_task() in quantum/keyboard.c is called. keyboard_task() is where the keyboard specific functionality is dispatched, such as matrix scanning, mouse handling and controlling keyboard status LEDs.\nMatrix Scanning Matrix scanning is the core of a keyboard firmware. QMK provides built in scanning algorithm, you just need to define your matrix layout.\nTo declare your own key matrix, a C macro is used. For example, to define a 2*2 matrix, you can use the following code\n1 2 3 4 5 6 7 #define LAYOUT( \\ k00, k01, \\ k10, k11 \\ ) { \\ {k00, k01}, \\ {k10, k11} \\ } Note that you may not have a key at every position in the matrix, you can use keycode KC_NO in the second part of the macro. A typical numpad layout can be defined using the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 #define LAYOUT( \\ k00, k01, k02, k03, \\ k10, k11, k12, k13, \\ k20, k21, k22, \\ k30, k31, k32, k33, \\ k40, k42 \\ ) { \\ { k00, k01, k02, k03 }, \\ { k10, k11, k12, k13 }, \\ { k20, k21, k22, KC_NO }, \\ { k30, k31, k32, k33 }, \\ { k40, KC_NO, k42, KC_NO } \\ } In keymap, you can use this macro to map keycodes of actual physical keys to matrix keys:\n1 2 3 4 5 6 7 8 9 const uint16_t PROGMEM keymaps[][MATRIX_ROWS][MATRIX_COLS] = { [0] = LAYOUT( KC_NUM, KC_PSLS, KC_PAST, KC_PMNS, KC_P7, KC_P8, KC_P9, KC_PPLS, KC_P4, KC_P5, KC_P6, KC_P1, KC_P2, KC_P3, KC_PENT, KC_P0, KC_PDOT ) } You can see the keymap has 3 dimensions. The first dimension is actual layer. Each layer has MATRIX_ROWS * MATRIX_COLS keys. In the given example, we defined only one layer.\nDetect key strokes At each matrix scanning loop, the matrix scanning function returns the current state of the matrix. QMK stores the result of last matrix scan, and compares with the current scanning result to determine which key is pressed or released. The the key code is dispatched to process_record() function.\nProcess Record Function process_record() is not complex, it contains a chain of events(c functions). Many of then depends on rules defined in rules.mk. The full events list is here. If any of them returns false, the following functions won't be executed.\nCustomize keymap You can create your own key code with QMK. To create your key code, you need to define an enum in keymap.c first:\n1 2 3 4 enum my_keycodes { FOO = SAFE_RANGE, BAR }; QMK provides a macro SAFE_RANGE which ensure that you got a unique and correct key code.\nProgram a key When you want to overwrite your key's function or define the functionality of your new keycode, you can use process_record_kb() or process_record_user(). Remember this process_record() function? Your customized process_record_kb/user() function is similar: return false if you want to overwrite this key or attach new functionality to the key, otherwise just return true. Here is an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // Example of process_record_ bool process_record_user(uint16_t keycode, keyrecord_t *record) { switch (keycode) { case FOO: // Set your new key if (record-\u003eevent.pressed) { // 按下时做些什么 } else { // 抬起时做些什么 } // 覆盖已有功能 return false; // 跳过此键的所有进一步处理 case KC_ENTER: // 给回车增加新的功能 // 当按下回车时播放音符 if (record-\u003eevent.pressed) { PLAY_SONG(tone_qwerty); } // 返回true则不覆盖其原来的功能，返回false则为覆盖 return true; // 让QMK响应回车按下/抬起事件 default: // 其他键位都保持不变（返回true） return true; // 正常响应其他键码 } } The definition of input param record：\n1 2 3 4 5 6 7 8 9 10 keyrecord_t record { keyevent_t event { keypos_t key { uint8_t col uint8_t row } bool pressed uint16_t time } } record has the input key's col/row, whether it's pressed and the press time.\nKeyboard Initialization You can also customize the initialization process of the keyboard. There are 3 functions that you can overwrite:\nkeyboard_pre_init_*: happens at the early start of the firmware's setup process, can be used to initialize your hardware matrix_init_*: happens midway of the firmware's setup process. At this moment, hardware is initialized, but features may not be yet keyboard_post_init_*: happens at the end of the firmware's setup process. Hardware and most features are ready, you should put most of your customization code here Keyboard Pre Initialization keyboard_pre_init_* is used to initialize your own extra hardwares. Note that this process starts very early -- even earlier than the USB starts. The following is an example to set up LEDs using keyboard_pre_init_*:\n1 2 3 4 5 6 7 8 9 10 void keyboard_pre_init_user(void) { // Call the keyboard pre init code. // Set our LED pins as output setPinOutput(B0); setPinOutput(B1); setPinOutput(B2); setPinOutput(B3); setPinOutput(B4); } Matrix Initialization Code matrix_init_* is called when the matrix is initialized, you can overwrite the low-level matrix configuration here. If you want to change the default pin initialization method, you can use the following method:\nvoid matrix_init_pins(void): GPIO pin initialization. By default it will initialize pins set in MATRIX_ROW_PINS and MATRIX_COL_PINS, and the setup method is defined using ROW2COL, COL2ROW or DIRECT_PINS. Keyboard Post Initialization Code You most customization code should be wrote here. At this moment, most hardware and features are initialized, you can take changes to certain features as you want.\nThe following is an example about setting up RGB lights:\n1 2 3 4 5 6 void keyboard_post_init_user(void) { // Call the post init code. rgblight_enable_noeeprom(); // enables Rgb, without saving settings rgblight_sethsv_noeeprom(180, 255, 255); // sets the color to teal/cyan without saving rgblight_mode_noeeprom(RGBLIGHT_MODE_BREATHING + 3); // sets mode to Fast breathing without saving } QMK configuration files We've already knew that there are some configuration files in QMK when you're going to create your own keyboard, such as info.json, rules.mk. In this section, we'll learn how to customize those files.\nNOTE: there are many configs that you can set at multiple files. In this case, when you try to compile the firmware, qmk would complain and tell you which config will be used:\nPrune your configurations accordingly.\ninfo.json You can config the hardware and enabled features in info.json. The full reference of info.json can be found here: https://docs.qmk.fm/#/reference_info_json.\nHardware configuration At the top of info.json, there are some configs about the manufacturer and hardware that you can customize:\n1 2 3 4 5 6 7 8 9 10 { \"keyboard_name\": \"your_keyboard_name\", \"manufacturer\": \"You\", \"maintainer\": \"You\", \"usb\": { \"vid\": \"0xFEED\", \"pid\": \"0x0000\", \"device_version\": \"1.0.0\" } } in which, the manufacturer and keyboard_name will be displayed in the list of USB devices on Windows and MacOS. You can also choose your USB's vid and pid.\nMatrix configuration You can also define the GPIO pins which are used for matrix scanning in info.json:\n1 2 3 4 5 6 { \"matrix_pins\": { \"cols\": [\"C1\", \"C2\", \"C3\", \"C4\"], \"rows\": [\"D1\", \"D2\", \"D3\", \"D4\"] }, } Then declare your diode direction in your PCB:\n1 \"diode_direction\": \"ROW2COL\" Layout configuration Next you can config the layout of your keyboard:\n1 2 3 4 5 6 7 8 9 10 \"layouts\": { \"LAYOUT_fancer\": { \"layout\": [ { \"matrix\": [0, 0], \"x\": 0, \"y\": 0 }, { \"matrix\": [0, 1], \"x\": 1, \"y\": 0 }, { \"matrix\": [1, 0], \"x\": 0, \"y\": 1 }, { \"matrix\": [1, 1], \"x\": 1, \"y\": 1 } ] } } For more available configurations, see: https://docs.qmk.fm/#/reference_info_json\nconfig.h config.h is the basic header of the firmware. All configurations here are persist over the whole project. There are lots of configurations available, see this document.\nI list some most commonly used configs in config.h here:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // vendor id #define VENDOR_ID 0x1234 // product id #define PRODUCT_ID 0x1234 // device version(often used for revisions) #define DEVICE_VER 0 // manufacturer #define MANUFACTURER MY_LAB // keyboard name #define PRODUCT fancer // number of rows/cols #define MATRIX_ROWS 2 #define MATRIX_COLS 2 // MCU pins of rows, from top to down(might be overwritten by user, see https://docs.qmk.fm/#/custom_quantum_functions?id=low-level-matrix-overrides) #define MATRIX_ROW_PINS { A1, A3 } // MCU pins of cols, from left to right(might be overwritten as well) #define MATRIX_COL_PINS { A2, A4 } // IO delay in ms between changing matrix pin and reading values #define MATRIX_IO_DELAY 30 // the direction of diode, COL2ROW means the black mark on your diode is facing to the rows #define DIODE_DIRECTION COL2ROW // debounce threshold in ms #define DEBOUNCE 5 // layout of the keyboard #define LAYOUT( \\ k00, k01, \\ k10, k11, \\ ) { \\ { k00, k01, }, \\ { k10, k11, }, \\ } rules.mk rules.mk defines some built options when compiling the firmware. It's actually a makefile which can be recognized with tools like cmake. In rules.mk you can set building configurations, MCU options and enable certain features.\nBuild Options FIRMWARE_FORMAT: defines the compiled firmware's format, like .hex or .bin SRC: added sources files for compilation/linking LAYOUTS: a list of layouts that this keyboard supports. See Matrix Scanning section. LTO_ENABLE: add it to reduce the size of your firmware Feature Options You can also enable/disable many features such as MAGIC Actions, AUDIO, RGBLIGHT, etc. in rules.mk. The full list can be found here: https://docs.qmk.fm/#/config_options?id=feature-options.\nHere is an example of rules.mk of owlab suit80:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # MCU name MCU = atmega32u4 # Bootloader selection BOOTLOADER = atmel-dfu # Build Options # change yes to no to disable # BOOTMAGIC_ENABLE = yes # Enable Bootmagic Lite MOUSEKEY_ENABLE = yes # Mouse keys EXTRAKEY_ENABLE = yes # Audio control and System control CONSOLE_ENABLE = no # Console for debug COMMAND_ENABLE = no # Commands for debug and configuration NKRO_ENABLE = yes # Enable N-Key Rollover BACKLIGHT_ENABLE = no # Enable keyboard backlight functionality RGBLIGHT_ENABLE = no # Enable keyboard RGB underglow AUDIO_ENABLE = no # Audio output ","description":"","tags":["keyboard","qmk"],"title":"Learn QMK - 2","uri":"/posts/keyboard/qmk-2/"},{"categories":null,"content":"STM32 - 3 时钟和USB接口\n时钟 一些缩写 外部高速时钟：HSE 内部高速时钟：HSI 时钟和复位控制：RCC（Reset and Clock Control) STM32的时钟系统简介 这个图是官方手册的时钟树的简化版，为了便于理解，可以从中间的SYSCLK，把整个时钟树分为两部分：左边为时钟源，右边为片上外设。\n系统时钟来源选择 可以看到SYSCLK旁边的那个梯形，就是一个选择器，由他来选择STM32究竟用哪个时钟源。HSE、HSI就不再多说，中间的那个PLL叫锁相环，用来做倍频的，就是把HSI或HSE做一定的倍频之后再作为整个片上外设的时钟输入。\n片上外设时钟 SYSCLK右边，会首先过一个AHB总线进行分频，把输入的时钟频率分成1/n，变成了HCLK。然后HCLK一分为二， 去供给片上外设。其中APB1的最大频率比APB2低一些，即APB2的时钟的最高频率搞一些，功耗也高一些，适合高频的片上外设。\nAPB1和APB2是两条总线，都挂载在AHB总线上 驱动Cortex-M3内核的时钟是HCLK STM32时钟编程接口 API 首先了解两个API：\n1 2 3 4 5 // 配置时钟来源，即上面图中的左半部分 HAL_StatusTypeDef HAL_RCC_OscConfig(RCC_OscInitTypeDef *RCC_OscInitStruct); // 配置SYSCLK、HCLK、PCLK1、PCLK2时钟，即上面图中的右半部分 HAL_StatusTypeDef HAL_RCC_ClockConfig(RCC_ClkInitTypeDef *RCC_ClkInitStruct, uint32_t Flatency); 时钟配置 首先，我们需要去配置时钟来源。从上面的API可以看到，配置时钟源需要一个RCC_OscInitTypeDef，其定义可以去看源码，现在我们就以外部时钟源+9倍频锁相环为例，看看怎么配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 RCC_OscInitTypeDef oscInit; // 配置时钟类型为HSE oscInit.OscillatorType = RCC_OSCILLATORTYPE_HSE; // 配置HSE开启或关闭 oscInit.HSEState = RCC_HSE_ON; // 配置打开锁相环PLL oscInit.PLL.PLLState = RCC_PLL_ON; // 设置PLL的时钟来源 oscInit.PLL.PLLSource = RCC_PLLSOURCE_HSE; // 设置PLL的倍频倍数为9倍频 oscInit.PLL.PLLMUL = RCC_PLL_MUL9; // 设置时钟源 HAL_RCC_OscConfig(\u0026oscInit); 设置完时钟源之后，就可以设置后续的时钟了，代码如下：\n1 2 3 4 5 6 7 8 9 10 RCC_ClkInitTypeDef clkInit; // 被设置的时钟类型，我们选择SYSCLK和HCLK clkInit.ClockType = RCC_CLOCKTYPE_SYSCLK | RCC_CLOCKTYPE_HCLK; // 设置SYSCLK等于PLL时钟源 clkInit.SYSCLKSource = RCC_SYSCLKSOURCE_PLL // 通过设置AHB分频器分频倍率为1，来设置HCLK=SYSCLK clkInit.AHBCLKDivider = RCC_SYSCLK_DIV1; // 最后设置时钟 HAL_RCC_ClockConfig(\u0026clkInit, FLASH_LATENCY_2); 最后一个参数的FLASH_LATENCY配置，实际上和你设置的时钟SYSCLK有关系，具体多少频率的时钟对应多少FLASH_LATENCY需要去库里面的代码里看注释。\n到这里，时钟就配置完了。更多的时钟配置可以去看HAL库的源代码手册。\nUSB USB介绍 首先需要介绍下usb的基本概念和参数，这对于在stm32上使用usb设备有很大的帮助。\nUSB默认是个4pin设备，其中中间的D+和D-分别会接一个15K的下拉电阻：\n所以，默认D+和D-默认都是低电平。然而，在USB插头上面，D+会接一个1.5K的上拉电阻，这样的话一旦插入USB设备，D+就会被拉到高电平，产生一个中断表名设备已连接。\nUSB分为主机和设备，所有的通信都是由主机（比如PC）发起的，输入和输出也都是针对主机而言。\nUSB的传输类型 一共4种：\n控制：一般是用于初次连接的初始化，比如分配地址、获取USB设备信息等 中断：主机会定时询问是否有数据，这就是鼠标、键盘等设备的通信模式 批量：大文件传输 同步：数据同步，比如摄像头 USB供电 USB可以给设备供电，这是通过VBUS来供电的。USB2.0给低功耗设备供电电流最多可以100ma。对于高功耗设备，最高不超过500ma。\nUSB配置描述符 上面说的这些配置，比如是否低功耗设备等，都是在最初连接的时候，设备通过USB配置描述符告诉主机的。下面是一个例子，可以参考下。具体所有的描述符建议去USB的官方查看官方文档：\nUSB HID USB支持很多种设备类型，我们主要会用到HID，即Human Interface Device。\nUSB挂起模式 当USB主机连续3ms没有检测到USB设备有信号，就自动进入USB挂起模式。因此，为了避免进入挂起模式，设备需要周期性地发送Keep Alive或者SOF信号。\nHID键盘的键值 对于USB HID设备来说，其键盘的数据包包含8个字节。基本上可以分为3部分：\nByte0：功能键，如L/R-Ctrl、Shift、Alt、GUI等 Byte1：保留 Byte2-7：普通按键 对于普通按键来说，HID设备已经规定好了各个按键的键值，比如右箭头的键值为79（十六进制0x4F）。如果要向主机发送一个右箭头按键，那么在Byte2写个0x4F就行了。\nSTM32的USB配置 默认情况下，STM32CubeMX生成的USB HID设备是鼠标，需要修改配置描述符来修改为键盘的键值。首先看一下STM32CubeMX配置USB的界面：\n左下角可以看到F411是USB_OTG_FS，意思是带OTF的全速USB设备。中间可以看到相关的配置项，对于键盘设备一般选Device Only即可。下面是休眠和供电的选项，选不选都可以。\n然后，在middleware下选择USB_DEVICE，就可以看到作为USB设备的相关配置：\n中间的Class For FS IP就选择HID设备，下面的Device Descriptor设备描述符，就可以配置这个USB设备连接到电脑上之后的一些信息，比如厂家、设备名等等。然后生成代码即可。\n默认情况下，CubeMX生成的HID设备是鼠标。在生成的代码中，找到usbd_hid.c文件，找到这段代码：\n可以看到有一个nInterfaceProtocol，在默认情况下值为0x02，即mouse。对于键盘我们把它改成0x01即可。此外，在下面还有一个HID_MOUSE_REPORT_DESC_SIZE常量，指的是鼠标的回报描述符的大小，我们也需要把它改为键盘的大小：\n需要注意的是，这个DESC_SIZE有两处，都需要改为键盘。\n改完上面三个地方之后，我们的代码就可以作为键盘的USBHID设备连接到电脑上了。\n数据传输 最后的最后，在代码中只需要调用\n1 USBD_HID_SendReport(\u0026hUsbDeviceHS, data, data_length); 即可向USB主机端发送数据。当然，data和data_length都必须符合USB的描述以及配置。具体请参见USB的文档：https://www.usb.org\n","description":"","tags":["keyboard"],"title":"STM32 - 3","uri":"/posts/keyboard/stm32-3/"},{"categories":null,"content":"STM32 - 2 之前我们已经了解了一些STM32的基础知识，下面就开始正式的编程了。我们主要会使用STM32的HAL库，这些库函数已经在创建工程的时候初始化好了，一般来说，直接使用即可。\nGPIO 片上外设的使能时钟 首先了解一下STM32的片上外设的使能时钟。STM32几乎所有的片上设备都有使能时钟，包括输入输出GPIO。不把使能时钟打开，对应的外设就不能用。STM32这么设计是为了降低功耗。\n那怎么样启动使能时钟呢？以最小系统板上面的PC13为例，这个IO实际上就是GPIOC的13号引脚，那么我们就在main.c里面使用如下代码就可以开启GPIOC的使能时钟：\n1 __HAL_RCC_GPIOC_CLK_ENABLE(); 初始化GPIO 在打开GPIO的使能时钟之后，就要初始化GPIO了。可以使用如下API初始化GPIO：\n1 void HAL_GPIO_Init(GPIOC, \u0026GPIO_InitStruct); 其中，GPIOC就是GPIO的类型，而后面的那个GPIO_InitStruct则是各种初始化参数，如输入输出模式、输入输出引脚等。下面是初始化设置的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 void MX_GPIO_Init(void) { // GPIO初始化设置 GPIO_InitTypeDef GPIO_InitStruct = {0}; // 打开GPIOC的使能时钟 __HAL_RCC_GPIOC_CLK_ENABLE(); // 设置GPIOC的13号引脚的输出Level HAL_GPIO_WritePin(GPIOC, GPIO_PIN_13, GPIO_PIN_RESET); // 然后配置各种GPIO的设置，如引脚为13，模式为PP（推挽输出），没有上拉电阻，输出速率等等。 GPIO_InitStruct.Pin = GPIO_PIN_13; GPIO_InitStruct.Mode = GPIO_MODE_OUTPUT_PP; GPIO_InitStruct.Pull = GPIO_NOPULL; GPIO_InitStruct.Speed = GPIO_SPEED_FREQ_LOW; // 完成GPIO的初始化 HAL_GPIO_Init(GPIOC, \u0026GPIO_InitStruct); } 向GPIO写入值（输出） 在初始化完毕之后，就可以使用HAL_GPIO_WritePin(GPIOC, GPIO_PIN_13, GPIO_PIN_RESET);向对应的GPIO写入值了。参数都很容易理解，最后一个参数，如果是SET，那么就是写1；如果是RESET，就是写0。\nOK，到这里，一个最简单的亮灯程序就写完了。后面，我们就深入了解一下GPIO的各种设置属性。\nGPIO的设置 GPIO的设置主要是使用GPIO_InitTypeDef来声明：GPIO_InitTypeDef GPIO_InitStruct = {0};。GPIO_InitTypeDef主要包含4个设置：\nMode：IO引脚的模式\n我们在上面的示例代码中用到的，就是GPIO_MODE_OUTPUT_PP，即输出推挽模式（Output Push-Pull Mode）。推挽模式就是高低电平均有驱动能力的输出模式。\n此外，输出模式还有GPIO_MODE_OUTPUT_OD，即输出开漏模式。开漏输出高电平相当于高阻态，是没有驱动能力的。\n另外，还可以选择输入模式，即GPIO_MODE_INPUT和GPIO_MODE_AF_INPUT。具体这些模式的区别，可以参考下 https://www.bilibili.com/video/BV1th411z7sn 15分钟左右的讲解，很详细\nPin：就是第几号IO引脚\n如果要设置第13号引脚，就可以使用：GPIO_InitStruct.Pin = GPIO_PIN_13;，如果要设置多个引脚，可以使用|:\n1 2 // 同时设置13和14号引脚 GPIO_InitStruct.Pin = GPIO_PIN_13 | GPIO_PIN_14; Pull：用在输入模式里面，即上拉/下拉电阻，用来设置输入悬空的时候，输入口是高电平（上拉电阻PULLUP）还是低电平（下拉电阻PULLDOWN）还是就把它悬空（NOPULL），详见 https://www.bilibili.com/video/BV1th411z7sn 9分钟30秒左右的讲解\nSpeed：设置IO引脚允许的最大速度\n看f411的源代码可以到这里可以设置4个级别的IO速度，分别是2M、12.5M~50M、25M~100M和50M~200M。速度越快，IO的读取频率就越高，也就越耗电。\n1 2 3 4 #define GPIO_SPEED_FREQ_LOW 0x00000000U /*!\u003c IO works at 2 MHz, please refer to the product datasheet */ #define GPIO_SPEED_FREQ_MEDIUM 0x00000001U /*!\u003c range 12,5 MHz to 50 MHz, please refer to the product datasheet */ #define GPIO_SPEED_FREQ_HIGH 0x00000002U /*!\u003c range 25 MHz to 100 MHz, please refer to the product datasheet */ #define GPIO_SPEED_FREQ_VERY_HIGH 0x00000003U /*!\u003c range 50 MHz to 200 MHz, please refer to the product datasheet */ 最后，需要注意传进去的GPIO_InitTypeDef是一个指针，所以需要使用\u0026GPIO_InitStruct 传地址进去。\nGPIO输入 我们就用最简单的开关来学习GPIO口的输入。\n首先复习一下在配置输入模式的时候，有一个Pull字段。这个字段代表着在输入悬空的时候，默认是上拉电阻还是下拉电阻或者是悬空。这玩意有什么用呢？我们看一下下面的开关电路：\n很好理解，开关按下的时候，GPIO输入就接地，即低电平0。那这个上下拉电阻有什么关系呢？\n看下左边的图，如果开关断开，这个时候PA0输入就悬空了。如果这个时候GPIO口的配置为悬空，那这个引脚就真的悬空了，其输入就不确定。而如果这个时候，GPIO口的输入配置为下拉电阻，此时PA0悬空，输入就被下拉电阻拉到了低电平，那这个按键实际上就没用了。因此，在按键一边接地的时候，如果没有手动配置上拉电阻（右边的图），那就必须把GPIO的输入设置为上拉电阻模式（PULLUP），来保证开关断开时输入为高电平。\n当然，如果像右边一样，外面单独配置了上拉电阻，那么PA0的输入模式配置为上拉电阻模式或者悬空时都可以的。\n❗ 注意：由于STM32系列并不是所有的开关都有下拉电阻模式，因此，在习惯上，只要是开关都采用低电平触发的方式，即一边接地一边接GPIO。\n输入模式 按键消抖 一般来说，按下按键之后，按键会有一个5-10ms的抖动，然后才会稳定在高电平。消除抖动的策略很简单，在检测到按键之后，延迟20ms再进入完全按下时的那个while循环。然后就只要按下就一直在这个while循环里，直到松开按键，跳出while循环，然后再delay20ms。对于按键的要求比较高的场合，可能还需要后续的滤波。\nGPIO输出 上面我们已经了解了GPIO输出模式的推挽模式（PP），即输出的时候，上下两个MOS管均可以导通（同时只有一个能通），然后输出就能够被下推到0V或上拉到3.3V，这样高低电平都有强驱动能力。\n除了推挽模式之外，输出还有开漏模式（OD）。其实开漏模式和推挽模式的区别就在于，开漏模式的上面接高电平的MOS管一直是断开的。这样的话，只有下面的MOS可以导通，那就低电平有强驱动能力。而在输出1的时候，上下MOS均断开，输出就是一个高阻抗状态，就相当于这个输出直接断开了（不是输出高电平）。\n两个输出模式的区别：\n开漏模式的应用场景 驱动IED PP和OD模式都能驱动LED，驱动电路略有不同：\n区别是除了电路不同之外，两个模式驱动LED还有如下区别：\nPP模式是输出1灯亮，而OD是输出0灯亮 PP模式是芯片向外供电，而OD是外面的电源供电，芯片吸电 OD模式相对于PP模式，理论上能够承载的最大电流多一些 OD模式能够降低芯片功耗 逻辑线与 逻辑线与最多是用在I2C通信总线上。\nI2C是一种通信总线，它有两个线：一个是SDA用于传数据，一个是SCL用于传时钟。其接线如图：\nI2C的所有SDA和SCL的IO都是OD模式，这样的话，只要有一个输出是0，那么对应的SDA就会被拉到0V输出低电平。而只有所有的SDA或者SCL全是1，由于是高阻态，对应输出的SDA或SCL就会被上拉电阻拉到VDD，输出高电平。这就是逻辑线与。\n匹配不同电压等级 STM32输出的高电平一般是3.3V，但是如果我们想要去驱动需要5V输入的芯片应该如何做呢？下面就是使用开漏模式驱动5V输入的原理图：\n可以看到，在STM32输出0的时候，那右边芯片的DIN输入就是0V低电平。如果STM32输出1的时候，由于输出口为高阻态，那么DIN就会被外接电源拉到5V，也就是右边芯片需要的高电平。这就实现了不同电压水平的匹配。\n","description":"","tags":["keyboard"],"title":"STM32 - 2","uri":"/posts/keyboard/stm32-2/"},{"categories":null,"content":"STM32 学习STM32的笔记，视频教程：https://www.bilibili.com/video/BV1th411z7sn?p=2\u0026spm_id_from=pageDriver\u0026vd_source=58fb33df0449f8258f0e273447aab712\n片上资源 首先，需要了解一下STM32芯片上面都有哪些可以使用的资源，如下图：\n简单记忆一下各种资源/外设的缩写即可，后面会详细用到\n引脚定义 引脚定义是学习一个单片机的核心，一般来说如果了解了引脚定义，这个单片机怎么用的基本就知道个八九不离十了。例子里面是STM32F103C8T6， 详细定义可以见下面的表。这里首先讲一下每个引脚的含义：\nVBAT： 备用电源\nPC13-TAMPER-RTC：这个引脚有多重功能，分别是PC13（IO口）、TAMPER（侵入检测）、RTC（输出RTC校准时钟、秒脉冲等）\nPC14-OSC32_IN：IO或接32KHZ的RTC晶振的IN\nPC15-OSC32_OUT：同上，只不过这个引脚是RTC晶振的OUT\nOSC_IN和OSC_OUT：系统主晶振，一般是8MHZ（芯片里面会对这个频率进行倍频，比如生成72MHZ的频率作为芯片的主时钟）\nNRST：系统复位，N代表是低电平复位\nVSSA和VDDA：芯片内模拟部分的电源，VSS是负极（接GND），VDD正极（接3.3V）\nPAx、PBx和PCx：都是IO，其中有一些特殊的或者有其他功能的，下面会详细讲。注意在表里面没有加粗的IO，都是不推荐作为IO使用的（因为这些IO有其他功能，除非IO实在是不够了，否则还是用那些加粗的IO）\nPA0-WKUP意味着PA0除了是IO之外，还有wakeup的功能\nPB2：看主功能栏，这个引脚除了IO之外还有BOOT的功能，即可以用来配置启动模式\nPA13/14/15、PB3/4：还可以作为调试端口。可以看到有两种调试模式：JTAG和SWD\nSWD调试需要两根线：SWDIO和SWCLK\nJTAG则需要5根线：JTMS、JTCK、JTDI、JTDO、NJTRST\nSTLINK使用的SWD，因此只需要两根线。不过这5个IO，默认都不会被用来做普通IO。如果你是用SWD，且想使用剩下的三个IO，那么需要在程序里面配置。\nVSS_x和VDD_x：系统的主电源，同样，VSS负极，VDD正极。由于STM32是分区供电，因此可以有多个电源，这些电源正极/负极可以短接在一起，然后一起供电\nBOOT0：和PB2/BOOT1一样，也是用来做启动配置\n启动配置 看完引脚图，大概有个印象即可。下面看一下启动配置。还记得之前表里的两个BOOT吗？BOOT0和PB2/BOOT1。这两个引脚可以用来设置模式：\n引脚为0就是接地GND的意思，1是高电平。默认都是使用flash作为启动区域。另外，下面那句话的意思就是，BOOT0和BOOT1只在刚开始上电之后的一瞬间（第四个上升沿）有用，后面就随便了。\n最小系统电路 最小系统电路就是能让芯片跑起来的最小电路。下面就看一下F103的最小电路：\n供电 首先看供电的部分。可以看到，所有的VDD都接了3.3V，然后VSS都接了GND。在所有的3.3V和GND之间都有一个电容，俗称滤波电容。滤波电容主要是保持供电电压的稳定，一般供电最好都来一个比较好。\n另外，VBAT是备用的电源，如果需要用到就接，不用到就接3.3V或者悬空。如果用到VBAT，那就接一个3V的纽扣电池即可，电池正极VBAT，负极接地。\n可以看到，整个供电还是很简单的。但是由于供电口比较多，所以走线可能有些麻烦。\n晶振 STM32的主晶振一般都是8MHZ。晶振就连接芯片上的OSC_IN和OSC_OUT：\n这两个电容是启震电容，另一端接地即可。\n复位电路 复位电路的原理稍微麻烦点。在上电的一瞬间，电容是相当于短路的，因此就产生了一瞬间的低电平。芯片是低电平复位，所以可以理解为开机的一瞬间复位。之后，电容充电断开，在K1开关断开的情况下，NRST就和3.3V相连，一直是高电平不复位。按下复位开关，NRST接地，就完成了手动复位。\n启动配置电路 就是上面选的BOOT0/1选择启动配置的电路。在我们的411板子上，是这样的：\n可以看到这是一个拨码开关，默认情况下开关处在123的位置，而456这一侧为ON，即默认情况下三个位置都没有接通。从图上可以看到，如果左右没有接通，则三个位置都断开。即BOOT0=0，而PB2/BOOT1悬空。\n如果想把BOOT1设置为1，则需要同时把1-6和2-5连接起来，即上面两个开关拨向ON。把BOOT0置1则只需要拨动3-4开关到ON即可。\nHello World工程 下面我们就来写一个最简单的工程来测试。首先我们的板子上，有一个PC13连接的LED，这个就是我们用来测试板子的灯，电路图在这里：\n可以看到其中一个是常亮的pwr指示灯，另外一个是连接PC13端口的测试灯。\n然后就是创建工程的步骤了，我们使用VSCode + PlatformIO + CubeMX创建我们的工程，具体可以参考这里：https://haobogu.github.io/posts/develop-stm32-using-vscode/。\nOK，创建完工程之后，就可以开始写代码了。打开main.c文件，找到主函数里面的while(1)循环，发现里面什么都没有。因为默认生成的是空工程。这个时候如果编译下载之后，主板上的绿灯就直接灭了，因为我们啥都没写。那么我们第一个程序就从这里开始。\n使用寄存器的方式点亮LED STM32的编程一般有3种方式：\n寄存器编程：直接控制芯片中的寄存器，比较基础，速度快。但是由于STM32系列很复杂，寄存器非常多，因此这种方法后期比较麻烦 库函数编程：ST公司提供了可以直接对芯片做各种操作的库函数，可以理解为寄存器操作的封装，十分方便，可以作为主要的编程方式 HAL编程：可以理解为低代码编程，通过ST公司的对应软件里面的图形化界面完成编程。但是这种方式不利于对芯片底层的理解，不好开发进阶功能 综上，我们主要会选择库函数编程的方式。当然寄存器编程也要了解。因此我们的第一个程序就从寄存器编程开始（这个章节如果看不懂，无所谓，就了解下）。\n寄存器配置 首先是RCC寄存器的时钟，这个可以理解为是GPIO的使能时钟。这里，出现了STM32F4和F1的不同：在F1中GPIO都是APB2的外设，而在F4中，GPIO是AHB1的外设。具体可以看下F411的手册：\n可以看到在F411中，这些GPIO都是和AHB1相连的。这也是和视频中的不一样的地方。不过原理都是相同的，找到对应的使能时钟的RCC寄存器：\n可以看到和视频中类似，有一堆GPIOxEN：\n我们要操作PC13上面的灯，那就GPIOCEN写入1即可：\n1 2 3 4 5 // main.c中的主函数中，其他代码省略 // 首先打开GPIOC的时钟 // 由于写入的是16进制，我们需要每4位转换一次，即 RCC -\u003e AHB1ENR = 0x00000004; 然后找到对应IO的通用配置高寄存器GPIOx_CRH，可以看到对应GPIOC有两个地方，CNF配置为00，即推挽模式，而MODE配置为11（这里F411还是和视频中不一样，我直接写F411的代码）\n1 2 3 // 配置输出寄存器 GPIOC -\u003e MODER = 0x0C000000; GPIOC -\u003e OTYPER = 0x00000000;\t往PC13端口，写入高电平 写出数据的寄存器叫GPIOx_ODR，我们往PC13写出高电平，即：\nGPIOC -\u003e ODR = 0x00002000; OK，这就是寄存器编程的大概流程。我们在这里主要是简单了解一下编程的流程，即使能时钟、配置IO、输入输出。后面我们会使用HAL库来对STM32进行编程。\n","description":"","tags":["keyboard"],"title":"STM32","uri":"/posts/keyboard/stm32/"},{"categories":null,"content":"在Mac上面使用VSCode开发stm32 因为最近有项目需要对stm32进行硬件的开发，而我现在主要是在用M1芯片的Mac，且官方提供的各种配套工具都不太行，所以记录一下在Mac下折腾stm32开发的过程。\n准备 首先，进行硬件开发你得有相应的硬件。我是从淘宝上买了stm32f411ceu6的开发板，以及st-link v2的烧写器，在本文中我们就以它们作为示例。\n其次，需要下载宇宙第一编辑器VSCode，并且安装PlatformIO插件。这个过程就不再展开了，可能安装PlatformIO插件会慢点，等待即可。\n安装STM32CubeMX 再然后就需要安装STM32CubeMX了。这个工具是用来自动生成对应的工程的，要开发stm32系列单片机，几乎也是必须的。\n首先，下载STM32CubeMX安装包：https://www.st.com/zh/development-tools/stm32cubemx.html#overview。官方提供了Mac系统对应的安装包，点击下载即可，大约几百M\n下载下来是一个zip文件，解压之\n解压后的文件夹中，有一个.app文件。一般来说直接执行这个.app文件即可，但是对于M1的Mac来说，由于苹果的安全策略，导致直接安装不可行，需要使用如下命令处理.app文件\n1 sudo xattr -cr xxxxx.app 安装过程需要你本地装有Java，如果你本地没有Java，或者Java版本不对，依然不行。不过好在在安装包中，已经贴心地给你准备好了Jre（没错，压缩包解压之后有一个jre文件夹，就是做这个的）。不过，由于苹果的安全策略，你还是没有办法直接使用jre文件夹中的Java，类似地，你需要对jre文件夹再执行一遍xattr：\n1 sudo xattr -cr jre 现在你可以使用Java执行安装包：\n1 java -jar xxx.app 之后，会弹出安装包的界面（这界面真复古）。一路next，就能可以把STM32CubeMX安装好了\n使用STM32CubeMX初始化工程 所有的依赖都安装好了，下面来初始化工程。打开STM32CubeMX，上面的File菜单中选择New Project（可能会读条，等待即可）。然后在新建工程的窗口中搜索你的MCU，我这里使用stm32f411ceu6：\n选择完MCU之后，右上角Start Project，就进入初始化页面。前面的选项都不用改，直接点到Project Manager部分，输入一个Project名称，然后在ToolChain/IDE这里选择Makefile：\n点击右上角GENERATE CODE即可生成代码\nOption1：使用PlatformIO 打开VSCode的命令界面，输入show platformIO，打开platformIO的主页面，点击页面上的New Project创建工程。在创建工程弹窗中， Project Name填写刚刚在CubeMX中写的Project名称，Board选择stm32f411CE，framework注意需要选择STM32Cube。然后重点来了，取消Use default location勾选，并且在下面的路径选择中选择刚刚CubeMX生成的代码目录的父文件夹。\n点击finish，初次生成可能需要等待，完成之后VSCode会自动跳到对应的文件夹。\n然后，检查一下文件下的根目录下面是否同时存在[你的工程名].ioc和platformio.ini，如果有，那说明代码工程已经被成功创建。\n接下来，打开platformio.ini，把CubeMX生成的Src目录和Header目录设置为platformIO的对应目录，然后设置一下下载器，我使用的是st-link：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # platformio.ini [env:genericSTM32F411CE] platform = ststm32 board = genericSTM32F411CE framework = stm32cube # 设置下载器 debug_tool = stlink upload_protocol = stlink [platformio] include_dir=Core/Inc src_dir=Core/Src Done！所有的配置都完成了。当然你也可以把platformIO自动生成的工程文件夹src, lib , test删掉（我们用的是CubeMX生成的代码工程，不是吗）。\n烧录和调试 连接st-link以及开发板，单击VSCode下面的upload选项即可烧录代码：\n如果想要调试的话，单击F5（VSCode的默认调试快捷键）即可。Easy， huh？\nOption2：使用makefile + OpenOCD 如果不想使用platformIO，其实也是可以的。只不过需要对VSCode多一点点配置。\nC/C++设置 在生成工程之后，首先打开.vscode目录下的c_cpp_properties.json，新建一个配置项，比如Mac，然后把各种依赖添加进includePath，并且设置browse、defines、compilerPath等参数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 { \"configurations\": [ { \"name\": \"Mac\", \"includePath\": [ \"${workspaceFolder}/**\", \"${workspaceFolder}/Inc\", \"${workspaceFolder}/Drivers/STM32H4xx_HAL_Driver/Inc\", \"${workspaceFolder}/Drivers/STM32H4xx_HAL_Driver/Inc/Legacy\", \"${workspaceFolder}/Drivers/CMSIS/Include\", \"${workspaceFolder}/Drivers/CMSIS/Device/ST/STM32F4xx/Include\", \"/opt/homebrew/Cellar/arm-none-eabi-gcc/10.3-2021.07/gcc/arm-none-eabi/include\", \"/opt/homebrew/Cellar/arm-none-eabi-gcc/10.3-2021.07/gcc/arm-none-eabi/include/c++/10.3.1\", \"/opt/homebrew/Cellar/arm-none-eabi-gcc/10.3-2021.07/gcc/lib/gcc/arm-none-eabi/10.3.1/include\", \"/opt/homebrew/Cellar/arm-none-eabi-gcc/10.3-2021.07/gcc/lib/gcc/arm-none-eabi/10.3.1/include-fixed\" ], \"defines\": [ \"USE_HAL_DRIVER\", \"STM32F411xE\" ], \"macFrameworkPath\": [ \"/Library/Developer/CommandLineTools/SDKs/MacOSX12.sdk/System/Library/Frameworks\" ], \"compilerPath\": \"/opt/homebrew/bin/arm-none-eabi-gcc\", \"cStandard\": \"c17\", \"cppStandard\": \"c++20\", \"intelliSenseMode\": \"macos-clang-arm64\", \"configurationProvider\": \"ms-vscode.makefile-tools\", \"browse\": { \"path\": [ \"${workspaceFolder}/**\", \"${workspaceFolder}/Inc\", \"${workspaceFolder}/Drivers/STM32H4xx_HAL_Driver/Inc\", \"${workspaceFolder}/Drivers/STM32H4xx_HAL_Driver/Inc/Legacy\", \"${workspaceFolder}/Drivers/CMSIS/Include\", \"${workspaceFolder}/Drivers/CMSIS/Device/ST/STM32F4xx/Include\", \"/opt/homebrew/Cellar/arm-none-eabi-gcc/10.3-2021.07/gcc/arm-none-eabi/include\", \"/opt/homebrew/Cellar/arm-none-eabi-gcc/10.3-2021.07/gcc/arm-none-eabi/include/c++/10.3.1\", \"/opt/homebrew/Cellar/arm-none-eabi-gcc/10.3-2021.07/gcc/lib/gcc/arm-none-eabi/10.3.1/include\", \"/opt/homebrew/Cellar/arm-none-eabi-gcc/10.3-2021.07/gcc/lib/gcc/arm-none-eabi/10.3.1/include-fixed\" ], \"limitSymbolsToIncludedHeaders\": true, \"databaseFilename\": \"${workspaceFolder}/.vscode/browse.vc.db\" } } ], \"version\": 4 } 需要注意的是，defines需要和你的Makefile里面的C defines对应\n另外， 添加进Path中的arm-none-eabi-gcc需要填写你本地的对应SDK的安装地址。\n然后，我们可以在命令行中执行make来测试是否可以正确编译。注意，如果你添加了额外的.c文件，则需要在Makefile里面指定：\n如果你看到了如下信息，则说明工程配置基本没有问题：\nOpenOCD配置 首先安装OpenOCD：\n1 brew install openocd 安装完毕之后，使用openocd --version检查是否安装成功。\n然后，在项目根目录下创建openocd.cfg，并且参考/opt/homebrew/Cellar/open-ocd/0.11.0/share/openocd/scripts/下的配置，配置你自己的芯片。比如，对于我使用的stm32f411 + stlink，我需要进行如下配置：\n# Use stlink source [find interface/stlink.cfg] transport select hla_swd # Set target source [find target/stm32f4x.cfg] 注意 在Windows下面，OpenOCD配置有个坑就是，我们需要使用sysprogs公司编译的OpenOCD，原因是VSCode中cortex-debug插件默认使用了VirtualGDB，而VirtualGDB就是sysprogs公司出品的。因此我们还需要使用该公司编译的OpenOCD才能正确下载。我们可以去这里下载对应的OpenOCD，解压并且添加到Path环境变量中。\n添加SVD文件 添加.svd文件可以在调试的时候看到各个外设的状态，非常方便。首先在这里下载对应芯片的svd文件：https://github.com/posborne/cmsis-svd/\n如果这里没有，可以去keil的官网（https://www.keil.com/dd2/pack/）下载对应开发包，解压之后去CMSIS/SVD目录下找对应的SVD（网页很卡，小心）。\n把SVD复制到项目根目录备用。\nVSCode设置 接下来就是VSCode的debugger设置。首先，需要配置编译命令。在.vscode文件夹下面添加tasks.json，并且填入如下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"make all\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"make\", \"args\": [ \"all\", \"-j\", \"4\" ] }, { \"label\": \"make clean\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"make\", \"args\": [ \"clean\" ] } ] } VSCode的task相关内容可以去看VSCode官方文档，介绍得非常详细。这里我们就定义了两个task，一个是make all，一个是make clean。\n在配置完Task之后，按下快捷键cmd+shift+b，就可选择快速运行task了：\n然后，我们配置debug。首先安装cortex-debug插件。安装完毕之后，在.vscode目录下创建launch.json，并且进行如下配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Cortex Debug\", \"cwd\": \"${workspaceFolder}\", \"executable\": \"./build/f411.elf\", \"request\": \"launch\", \"type\": \"cortex-debug\", \"runToEntryPoint\": \"main\", \"servertype\": \"openocd\", \"showDevDebugOutput\": \"parsed\", \"configFiles\": [ \"openocd.cfg\" ], \"svdFile\": \"STM32F411xx.svd\", \"device\": \"stlink\", \"preLaunchTask\": \"make all\" } ] } 需要注意的是，在executable下，需要填写你自己的固件路径。然后设置svdFile为你刚刚下载的那个SVD文件。然后点击F5就可以愉快地debug了。在设置了SVD之后，还可以在Debug页面下看到各个外设和寄存器的状态：\nIAP 在产出的.bin文件末尾追加4bytes的CRC校验值，然后在bootloader中，首先从USB读取文件到一块Flash的临时区域，然后根据文件的长度，计算.bin文件的CRC校验和并且和.bin文件末尾的值作比较，如果一样，则正式把结果烧写到Flash的程序运行区域。否则直接退出。这样可以防止在复制固件的过程中出现错误，从而让系统变砖。\n","description":"","tags":["keyboard"],"title":"Develop stm32 using VSCode + PlatformIO on MacOS","uri":"/posts/keyboard/develop-stm32-using-vscode/"},{"categories":null,"content":"QMK What's QMK? QMK is a series of open source firmware for keyboard. Besides firmware, it also provides many useful tools for configuring your keyboard, such as QMK Configurator, QMK toolbox, etc.\nInstallation To install QMK on Mac(Intel), you can just use brew install qmk/qmk/qmk. But for M1 Macs, this installation might fail. Here is the reference for installing QMK through rosetta2: https://www.reddit.com/r/olkb/comments/nh2fk9/guide_installing_qmk_on_m1_macbook/\nAfter installation, you should use qmk setup to set your QMK configuration. Then, you can use QMK to build firmware for your keyboard!\nBuild your first firmware We can start from build a default keymap for your keyboard. To build the firmware with default keymap, you can use\n1 qmk compile -kb \u003cyour_keyboard\u003e -km default This requires your keyboard is supported officially by QMK. If not, you can create a PR to add your keyboard to QMK repo.\nIf you see the message like following, your firmware is sucessfully built:\nLinking: .build/clueboard_66_rev3_default.elf [OK] Creating load file for flashing: .build/clueboard_66_rev3_default.hex [OK] Copying clueboard_66_rev3_default.hex to qmk_firmware folder [OK] Checking file size of clueboard_66_rev3_default.hex [OK] * The firmware size is fine - 26356/28672 (2316 bytes free) Create your first firmware In the last section, we just created a firmware for clueboard_66_rev3 with default keymap. Now we'll create a new keyboard and build the firmware with a customized keymap.\nCreate a new keyboard First of all, you can create your own qmk keyboard using\n1 qmk new-keyboard Then you'll asked to choose your keyboard's name, layout and MCU. Keyboard naming supports at most 4 folder levels, the generated keyboard folder will be placed under keyboards folder:\n1 2 3 4 5 6 7 8 9 # Keyboard naming example, clueboard uses sub-folders for both organizations and revisions - qmk_firmware - keyboards - clueboard ← This is the organization folder, there’s no rules.mk file - 60 ← This is a compilable keyboard, it has a rules.mk file - 66 ← This is also compilable- it uses DEFAULT_FOLDER to specify rev3 as the default revision - rev1 ← compilable: make clueboard/66/rev1 - rev2 ← compilable: make clueboard/66/rev2 - rev3 ← compilable: make clueboard/66/rev3 or make clueboard/66 If a sub-folder has rules.mk, it will be considered as a keyboard. The keyboard folder has the following files\nreadme.md\nreadme.md is needed in QMK configurator for basic information about this keyboard\ninfo.json\ninfo.json is used by QMK API, it contains the information and metadata about this keyboard. Here is the reference about info.json\nconfig.h\nconfig.h sets important hardware informations about the keyboard, such as matrix size, product name, USB VID/PID, etc. This file is required for all keyboards firmware. For most cases, information in config.h should be consist with info.json\nrules.mk\nrules.mk means that this folder is a keyboard target and you can run make command here. In this file, you can write the default configuration of your keyboard and enable QWK features.\n\u003ckeyboard_name\u003e.c\nYou can write your own code here. There are some functions that you can write:\nvoid matrix_init_kb(void) void matrix_scan_kb(void) bool process_record_kb(uint16_t keycode, keyrecord_t *record) void led_set_kb(uint8_t usb_led) \u003ckeyboard_name\u003e.h\nYou'll define the matrix for your keyboard and other variables which will be used in \u003ckeyboard_name\u003e.c here\nCreate a new keymap After creating your keyboard, you can create a new keymap for your keyboard using\n1 qmk new-keymap -kb \u003cyour_keyboard\u003e Then, a keymap.c file will be created at the keyboard folder. You can edit this file to customize your keymap as you want.\nEdit your keymap Open keymap.c using your favorite IDE/text editor, edit the file as you want. There are some keycodes reference here:\nBasic Keycodes Quantum Keycodes Grave/Escape Mouse keys Remember to save your keymap after you edit it.\nBuild your firmware After you edit your keymap, you can use qmk compile command to complie your firmware:\n1 qmk compile -kb \u003cyour_keyboard\u003e -km \u003cyour_keymap\u003e If you see the output like:\nLinking: .build/lockr_fancer_default.elf [OK] Creating binary load file for flashing: .build/lockr_fancer_default.bin [OK] Creating load file for flashing: .build/lockr_fancer_default.hex [OK] Size after: text\tdata\tbss\tdec\thex\tfilename 0\t23028\t0\t23028\t59f4\tlockr_fancer_default.bin Copying lockr_fancer_default.bin to qmk_firmware folder [OK] Congraduations! You've done with your first customized keyboard and keymap using QMK.\nFlash your keyboard The last thing you need to do is to flash the firmware to your keyboard. There are some steps to flash the firmware:\nPut your keyboard into DFU mode\nIt depends on the specific hardware and firmware. You can try the steps to enable DFU mode here.\nFlash your firmware using QMK toolbox\nThis is the simplest way if you've successfully enable DFU mode and QMK toolbox recognizes it. Simply load the .hex or .bin file you just compiled in QMK toolbox and click the Flash button in QMK toolbox. If you see the following output:\n*** DFU device connected: Atmel Corp. ATmega32U4 (03EB:2FF4:0000) *** Attempting to flash, please don't remove device \u003e\u003e\u003e dfu-programmer.exe atmega32u4 erase --force Erasing flash... Success Checking memory from 0x0 to 0x6FFF... Empty. \u003e\u003e\u003e dfu-programmer.exe atmega32u4 flash \"D:\\Git\\qmk_firmware\\gh60_satan_default.hex\" Checking memory from 0x0 to 0x3F7F... Empty. 0% 100% Programming 0x3F80 bytes... [\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e] Success 0% 100% Reading 0x7000 bytes... [\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e] Success Validating... Success 0x3F80 bytes written into 0x7000 bytes memory (56.70%). \u003e\u003e\u003e dfu-programmer.exe atmega32u4 reset *** DFU device disconnected: Atmel Corp: ATmega32U4 (03EB:2FF4:0000) All things are done! Then test your keyboard.\n(optional) Flash your keyboard from CLI\nYou can also flash your keyboard from QMK cli tool, this is also quite simple:\n1 qmk flash Flash STM32 For most stm32 series MCUs, you have to enable DFU mode and then flash it. If you have a brand new stm32, you can enable DFU first using a hardware-way. STM32 series has three booting strategies, see the following table:\nSTM32 has a default bootloader in system memory, with enabling DFU mode. What you need to do is setting BOOT0 pin to 1, and BOOT1 pin to 0, reset the chip. Then open QMK toolbox to flash your firmware or using\nqmk flash command. If you see\nin QMK toolbox or the following output in your terminal:\nOpening DFU capable USB device... Device ID 0483:df11 Device DFU version 011a Claiming USB DFU Interface... Setting Alternate Interface #0 ... Determining device status... DFU state(10) = dfuERROR, status(10) = Device's firmware is corrupt. It cannot return to run-time (non-DFU) operations Clearing status Determining device status... DFU state(2) = dfuIDLE, status(0) = No error condition is present DFU mode device DFU version 011a Device returned transfer size 2048 DfuSe interface name: \"Internal Flash \" Downloading element to address = 0x08000000, size = 55316 Erase [=========================] 100% 55316 bytes Erase done. Download\t[=========================] 100% 55316 bytes Download done. File downloaded successfully Submitting leave request... Transitioning to dfuMANIFEST state You've successfully flash your STM32 MCU!\nMCUs 注：\nQMK默认支持的MCU列表\n","description":"","tags":["keyboard","qmk"],"title":"Learn QMK","uri":"/posts/keyboard/qmk/"},{"categories":null,"content":"Get Started With Tokio What's Tokio? Tokio is an asynchronous runtime for the Rust programming language. It is designed for IO-bound applications, and provides many useful utilities for asynchronous cases.\nTokio application Let's start with a simple hello world tokio application. First of all, add tokio as the dependency of your project:\n1 tokio = { version = \"1\", features = [\"full\"] } Then, write your first tokio hello world program:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 async fn say_world() { print!(\"world\"); } #[tokio::main] async fn main() { // Calling `say_world()` does not execute the body of `say_world()`. let op = say_world(); // This println! comes first print!(\"hello\"); // Calling `.await` on `op` starts executing `say_world`. op.await; } Here are some notes about this hello world program:\nThe say_world function is prefixed with async keyword, which indicates that it's an asynchronous function. In Rust, this asynchronous function will not be executed until .await is called.\nThe main function is also asynchronous, but it must be marked using #[tokio::main].\nAt the beginning of main, say_world is called and the result is binded to op. However, because say_world is an asynchronous function, it won't be executed until op.await is called. Hence, the code above will always print helloworld at the console.\nSpawning In this section, we'll write a server which accepts and processes TCP sockets.\nIn the main function, we bind a tokio::net::TcpListener to a local address, listen to the address and process incoming sockets in an infinite loop:\n1 2 3 4 5 6 7 8 9 10 11 12 use tokio::net::{TcpListener, TcpStream}; #[tokio::main] async fn main() { // Bind the listener to the address let listener = TcpListener::bind(\"127.0.0.1:6379\").await.unwrap(); loop { // The second item contains the IP and port of the new connection. let (socket, _) = listener.accept().await.unwrap(); process(socket).await; } } Note that the process() function should be asynchronous:\n1 2 3 async fn process(socket: TcpStream) { // process } The code above has a problem: we can only process only one request at a time. When it processes the request, it blocks in the inside the loop. In other languages, to process concurrent requests, threads/coroutines should be spawned to process requests in background, the main loop won't block. In Rust, you can do it as well, using tokio::spawn:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 use tokio::net::TcpListener; #[tokio::main] async fn main() { let listener = TcpListener::bind(\"127.0.0.1:6379\").await.unwrap(); loop { let (socket, _) = listener.accept().await.unwrap(); // A new task is spawned for each inbound socket. The socket is // moved to the new task and processed there. tokio::spawn(async move { process(socket).await; }); } } You can see we spawned a closure(which is just like a anonymous function) using tokio::spawn. Note the async and move keyword before the spawned closure. async indicates that this closure is asynchronous and won't block, while move indicates that the used variables socket is moved into the closure and won't be dropped until the closure completes.\nTask A Tokio task is an asynchronous green thread. It can be created using tokio::spawn(async{}).\ntokio::spawn() will return a JoinHandle, the caller can use JoinHandle.await.unwrap() to get the result of the task:\n1 2 3 4 5 6 7 8 9 10 11 12 #[tokio::main] async fn main() { let handle = tokio::spawn(async { // Do some async work \"return value\" }); // Do some other work let out = handle.await.unwrap(); println!(\"GOT {}\", out); } Data passed to the task In the TcpListener example, we also add move keyword before spawning the task. This keyword is required, because socket cannot be accessed from multiple threads.\nTask is Send! Tasks spawned by tokio::spawn must implement Send trait. This is because when we call .await, the task is moved by tokio runtime between threads.\nBut how to make a task Send? The answer is, if all data held by the task is Send, then the task is Send. If there exists which is not Send, the task won't compile, the following is an example:\n1 2 3 4 5 6 7 8 9 use std::sync::{Mutex, MutexGuard}; // It won't compile! async fn increment_and_do_stuff(mutex: \u0026Mutex\u003ci32\u003e) { let mut lock: MutexGuard\u003ci32\u003e = mutex.lock().unwrap(); *lock += 1; do_something_async().await; } // lock goes out of scope here This function will not compile, because MutexGuard is not Send. You can do a little refactoring to address it:\n1 2 3 4 5 6 7 8 9 // This works! async fn increment_and_do_stuff(mutex: \u0026Mutex\u003ci32\u003e) { { let mut lock: MutexGuard\u003ci32\u003e = mutex.lock().unwrap(); *lock += 1; } // lock goes out of scope here do_something_async().await; } Share state between tasks The state can be wrapped in Arc\u003cMutex\u003c_\u003e\u003e, making it accessable across many tasks and potentially many threads. For example, if you want to pass a shared HashMap to spawned tasks, you can just use Arc::new(Mutex::new(HashMap::new())) to initialize the map:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 use tokio::net::TcpListener; use std::collections::HashMap; use std::sync::{Arc, Mutex}; #[tokio::main] async fn main() { let listener = TcpListener::bind(\"127.0.0.1:6379\").await.unwrap(); println!(\"Listening\"); let db = Arc::new(Mutex::new(HashMap::new())); loop { let (socket, _) = listener.accept().await.unwrap(); // Clone the handle to the hash map. let db = db.clone(); println!(\"Accepted\"); tokio::spawn(async move { process(socket, db).await; }); } } Note that it is std::sync::Mutex, not tokio::sync::Mutex used to guard the HashMap.\nChannels Channels pass informations between threads, which is similar to channels in Golang. But in Rust, there are more types of channels, which serve for different purposes:\nmpsc: multi-producer, single-consumer channel. Many values can be sent.\noneshot: single-producer, single consumer channel. A single value can be sent.\nbroadcast: multi-producer, multi-consumer. Many values can be sent. Each receiver sees every value.\nwatch: single-producer, multi-consumer. Many values can be sent, but no history is kept. Receivers only see the most recent value.\nThere are also some other channel crates in Rust ecosystem, such as crossbeam. This document explains the differences. In one word, crossbeam::channel would block the current thread while tokio not, because tokio is designed for asynchronous situations.\nIn this article, I'll introduce mpsc and oneshot as the example.\nDefine a channel First, we define a mpsc channel:\n1 2 3 4 5 6 7 use tokio::sync::mpsc; #[tokio::main] async fn main() { // Create a new channel with a capacity of at most 32. let (tx, mut rx) = mpsc::channel(32); } mpsc::channel(32) returns both sender and receiver of the channel, we bind the sender with name tx and the receiver with rx. 32 is the buffer size of the channel.\nTo send a message to the channel, we can use tx.send():\n1 2 3 tokio::spawn(async move { tx.send(\"sending from first handle\").await; }); Because mpsc is a multi-producer channel, we can clone the sender and send messages from multiple tasks:\n1 2 3 4 5 6 7 8 9 let tx2 = tx.clone(); tokio::spawn(async move { tx.send(\"sending from first handle\").await; }); tokio::spawn(async move { tx2.send(\"sending from second handle\").await; }); .await is called after send(), which indicates the sending thread will wait until the receiver reads the sent message.\nThen, we use rx.recv() to receive messages sent to the channel:\n1 2 3 while let Some(message) = rx.recv().await { println!(\"GOT = {}\", message); } The receiver cannot be cloned, only one receiver can be used to receive messages for mpsc channel. When all senders are dropped, rx.recv() would return None, which indicates all senders are gone and the channel is closed.\nThe complete example code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 use tokio::sync::mpsc; #[tokio::main] async fn main() { let (tx, mut rx) = mpsc::channel(32); let tx2 = tx.clone(); tokio::spawn(async move { tx.send(\"sending from first handle\").await; }); tokio::spawn(async move { tx2.send(\"sending from second handle\").await; }); while let Some(message) = rx.recv().await { println!(\"GOT = {}\", message); } } In the code, we process the received message in the main thread. You can also spawn a new thread to process it.\nIO Tokio provides apis for asynchronous IO, which are similar with IO apis in std.\nBuffered read and write Generally, the message is transmitted like a frame, a typical HTTP frame is like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 enum HttpFrame { RequestHead { method: Method, uri: Uri, version: Version, headers: HeaderMap, }, ResponseHead { status: StatusCode, version: Version, headers: HeaderMap, }, BodyChunk { chunk: Bytes, }, } The frame will be converted to byte arrays to be transmitted, so when we read data, we have to convert byte arrays back to frames. Take TcpStream::read() as an example, when we manually call read(), an arbitrary amount of data might be returned. The returned data could be a frame, or a partial frame, or multiple frames. This is a little bit complex, as a result buffered IO is introduced to address this.\nFirst of all, we wrap TcpStream with a buffer:\n1 2 3 4 5 6 7 use bytes::BytesMut; use tokio::net::TcpStream; pub struct Connection { stream: TcpStream, buffer: BytesMut, } ","description":"","tags":["rust"],"title":"Get Started With Tokio","uri":"/posts/engineering/tokio/"},{"categories":null,"content":"Basics of Vue 什么是Vue？\nVue是一款用于构建用户界面的 JavaScript 框架。它基于标准 HTML、CSS 和 JavaScript 构建，并提供了一套声明式的、组件化的编程模型，帮助你高效地开发用户界面，无论任务是简单还是复杂。\n简介 Vue是现在最流行的前端框架之一，它主要拓展了HTML的语法，使得我们可以声明基于JS状态的HTML。同时，Vue会追踪JS的状态变化，并且响应式地更新HTML。Vue既可以直接增强静态的HTML而无需构建流程，也可以作为Web Component嵌入网页。\n对于具有构建流程的工程来说，Vue支持一种特别的文件，名叫单文件组件（也就是.vue文件，英文缩写为SFC）。在SFC中，你可以把HTML、CSS、JavaScript封装到一个文件中，下面就是一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u003cscript\u003e export default { data() { return { count: 0 } } } \u003c/script\u003e \u003ctemplate\u003e \u003cbutton @click=\"count++\"\u003eCount is: {{ count }}\u003c/button\u003e \u003c/template\u003e \u003cstyle scoped\u003e button { font-weight: bold; } \u003c/style\u003e 可以看到整个文件分成三块，从上到下分别是JS、HTML和CSS。\nVue基础 下面开始介绍Vue的基础知识，我们从创建一个Vue应用开始。\nVue应用 应用实例和根组件 每一个Vue应用实例都需要使用createApp创建，createApp的入参是根组件，根组件下面会有各种各样的子组件来构成页面。\n1 2 3 4 5 6 7 8 9 10 11 12 import { createApp } from 'vue' const app = createApp({ /* 根组件选项 */ }) // 我们也可以从其他文件中导入根组件 import { createApp } from 'vue' // 从一个单文件组件中导入根组件 import App from './App.vue' const app = createApp(App) 挂载应用 每一个应用实例需要被挂载，即调用了.mount()函数之后才能被渲染。.mount()接收一个“容器”参数，可以是一个实际的 DOM 元素或是一个 CSS 选择器字符串：\n1 \u003cdiv id=\"app\"\u003e\u003c/div\u003e 1 app.mount('#app') 这样，每一个页面就可以有多个Vue的应用实例，把应用挂载在对应的元素上即可\n模板语法 Vue 使用一种基于 HTML 的模板语法，使我们能够声明式地将其组件实例的数据绑定到呈现的 DOM 上。所有的 Vue 模板都是语法上合法的 HTML，可以被符合规范的浏览器和 HTML 解析器解析。\n文本插值 最简单的数据绑定形式就是文本插值，在HTML中使用双大括号来指定：\n1 \u003cspan\u003eMessage: {{ msg }}\u003c/span\u003e 在上面的例子中，双大括号标签会被替换为msg对应的property的值，每次msg的property的值改变的时候HTML渲染的结果也会实时更新。\n绑定Attributes HTML中，每个标签有各种Attributes。如果想把各种Attributes绑定到可变的值里面，可以用v-bind：\n1 \u003cdiv v-bind:id=\"dynamicId\"\u003e\u003c/div\u003e 上面的代码就把\u003cdiv\u003e的attribute id绑定在了dynamicId这个值上面。v-bind:可以使用:代替。如果你想绑定特定的class，把id替换成对应的class即可。下面就是一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u003cscript setup\u003e import { ref } from 'vue' const titleClass = ref('title') \u003c/script\u003e \u003ctemplate\u003e \u003ch1 :class=\"titleClass\"\u003eMake me red\u003c/h1\u003e \u003c/template\u003e \u003cstyle\u003e .title { color: red; } \u003c/style\u003e 这样，就把Make me red这个文本的颜色设置成了红色（即\u003cstyle\u003e中css定义的颜色）\n事件监听 我们可以使用v-on来监听DOM事件：\n1 \u003cbutton v-on:click=\"increment\"\u003e{{ count }}\u003c/button\u003e 也可以使用@作为简写：\n1 \u003cbutton @click=\"increment\"\u003e{{ count }}\u003c/button\u003e 上面的代码就监听了\u003cbutton\u003e的click事件，当监听到事件之后会触发increment函数。\n表单输入的绑定 Vue对于表单输入的case有一种特殊的语法，叫做v-model。首先，我们看一下使用上面介绍的v-on和v-bind如何做到表单输入：\n1 \u003cinput :value=\"text\" @input=\"onInput\"\u003e 1 2 3 4 5 function onInput(e) { // a v-on handler receives the native DOM event // as the argument. text.value = e.target.value } 在HTML的中，绑定一个值名为text，然后把onInput函数绑定在input事件上，最后在onInput中修改text的值。\nVue直接提供了v-model用来做这种事情，其用法也很简单：\n1 \u003cinput v-model=\"text\"\u003e 这样就自动把输入的值（即HTML中的input）绑定在了text上并且实现自动更新。v-model不只能绑定文本输入，还能绑定很多类型的输入比如checkbox、单选按钮等等。\n条件渲染 在Vue中，可以使用条件渲染来决定某一个元素是否被渲染。条件渲染的关键词有三个：v-if,v-else,v-else-if。其中，v-else和v-else-if必须跟在一个v-if或一个v-else-if元素后面。v-if后面可以跟一个JavaScript语句，其值为true或者false，下面是一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 \u003cdiv v-if=\"type === 'A'\"\u003e A \u003c/div\u003e \u003cdiv v-else-if=\"type === 'B'\"\u003e B \u003c/div\u003e \u003cdiv v-else-if=\"type === 'C'\"\u003e C \u003c/div\u003e \u003cdiv v-else\u003e Not A/B/C \u003c/div\u003e 在某些情况下，我们想要渲染一系列元素，但是给每个元素添加相同的v-if又太麻烦了，这时候我们应该使用\u003ctemplate\u003e元素来把所有元素包起来：\n1 2 3 4 5 \u003ctemplate v-if=\"ok\"\u003e \u003ch1\u003eTitle\u003c/h1\u003e \u003cp\u003eParagraph 1\u003c/p\u003e \u003cp\u003eParagraph 2\u003c/p\u003e \u003c/template\u003e \u003ctemplate\u003e元素并不会被渲染，它只是一个不可见的包装器元素。\n除了v-if之外，Vue还提供了v-show来决定一个元素是否被显示。v-show和v-if的区别在于，v-show仅切换了该元素上名为display的CSS属性，无论该元素显示与否，它都会被渲染（而v-if直接决定该元素会不会被渲染）。同时v-show也不支持else。\n总的来说，v-if在首次渲染时的切换成本比v-show更高。因此当你需要非常频繁切换时v-show会更好（因为v-if每次都会切换渲染 or 不渲染），而运行时不常改变的时候v-if会更合适。\n列表渲染 我们可以使用v-for来渲染一个元素的列表，其语法如下：\n1 2 3 4 5 \u003cul\u003e \u003cli v-for=\"todo in todos\" :key=\"todo.id\"\u003e {{ todo.text }} \u003c/li\u003e \u003c/ul\u003e 其中，todos是一个列表，v-for会遍历这个列表，然后把当前的值命名为todo，todo这个变量只能在v-for元素内部使用。同时，可以看到我们使用了:key=\"todo.id\"给每个todo变量分配了一个id，并且绑定到了key上面。这个key是Vue中一个特殊的属性，可以让列表自动渲染到对应位置，可以参考这里：https://staging-cn.vuejs.org/api/built-in-special-attributes.html#key。\n计算属性 当我们需要展示的某些值/属性需要计算之后才能得到结果时，可以使用computed()来完成对应的计算。下面是一个更加复杂的todo例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \u003cscript setup\u003e import { ref, computed } from 'vue' let id = 0 const newTodo = ref('') const hideCompleted = ref(false) const todos = ref([ { id: id++, text: 'Learn HTML', done: true }, { id: id++, text: 'Learn JavaScript', done: true }, { id: id++, text: 'Learn Vue', done: false } ]) const filteredTodos = computed(() =\u003e { return hideCompleted.value ? todos.value.filter((t) =\u003e !t.done) : todos.value }) function addTodo() { todos.value.push({ id: id++, text: newTodo.value, done: false }) newTodo.value = '' } function removeTodo(todo) { todos.value = todos.value.filter((t) =\u003e t !== todo) } \u003c/script\u003e \u003ctemplate\u003e \u003cinput v-model=\"newTodo\" @keyup.enter=\"addTodo\"\u003e \u003cbutton @click=\"addTodo\"\u003eAdd Todo\u003c/button\u003e \u003cul\u003e \u003cli v-for=\"todo in filteredTodos\" :key=\"todo.id\"\u003e \u003cinput type=\"checkbox\" v-model=\"todo.done\"\u003e \u003cspan :class=\"{ done: todo.done }\"\u003e{{ todo.text }}\u003c/span\u003e \u003cbutton @click=\"removeTodo(todo)\"\u003eX\u003c/button\u003e \u003c/li\u003e \u003c/ul\u003e \u003cbutton @click=\"hideCompleted = !hideCompleted\"\u003e {{ hideCompleted ? 'Show all' : 'Hide completed' }} \u003c/button\u003e \u003c/template\u003e \u003cstyle\u003e .done { text-decoration: line-through; } \u003c/style\u003e 在这个例子中，我们给每个todo分配一个done字段，代表是否完成；还声明了一个hideCompleted变量，当这个变量为true的时候，隐藏已经完成的todo。这样的话，在每一次点击hideCompleted按钮的时候，computed()都会根据当前的所有todo（即代码中的todos）计算出来filteredTodos，然后显示。\ncomputed()会自动地追踪输入的状态，并且在状态有改变的时候自动地更新输出（即filteredTodos）。\n模板ref 上面我们介绍了在Vue中常用的一些语法，当然在必要的时候我们也可以手动地去操作DOM。在HTML中，我们可以使用ref=\"p\"来声明一个DOM对应的ref。在\u003cscript setup\u003e中，我们也需要首先来声明对应的ref变量：\n1 const p = ref(null) 注意，此时我们使用null来声明这个变量，这是因为在\u003cscript setup\u003e中这个DOM元素还没有被挂载。只有当这个组件被挂载之后，我们才能够在代码里面获取到对应的对象。这里就涉及到了生命周期，在这里我们需要实现一个onMounted钩子，就可以在组件被加载之后自动执行，并且获取到对应的模板ref：\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u003cscript setup\u003e import { ref, onMounted } from 'vue' const p = ref(null) onMounted(() =\u003e { p.value.textContent = 'mounted!' }) \u003c/script\u003e \u003ctemplate\u003e \u003cp ref=\"p\"\u003ehello\u003c/p\u003e \u003c/template\u003e 这样，在组件加载之后就可以在代码中获取到其引用，并且自动地把文本换成\"mounted!\"\n监听器 监听器顾名思义就是监听某个ref，在它有变化的时候完成某些事件。在Vue中，可以使用watch()来监听某个ref：\n1 2 3 4 5 6 7 8 import { ref, watch } from 'vue' const count = ref(0) watch(count, (newCount) =\u003e { // 每当count变化，在控制台中打日志 console.log(`new count is: ${newCount}`) }) 组件Component 在真实的代码中，每个应用都是多层组件嵌套而成的。我们可以从其他.vue文件中导入组件，并且直接使用：\n1 2 3 4 5 6 7 \u003cscript setup\u003e import ChildComp from './ChildComp.vue' \u003c/script\u003e \u003ctemplate\u003e \u003cChildComp /\u003e \u003c/template\u003e 组件的props 组件可以声明props属性，然后在父组件使用的时候，就可以传入对应的props作为输入。\n首先，在定义组件的时候，我们需要使用defineProps声明其接受的props：\n1 2 3 4 5 6 \u003c!-- ChildComp.vue --\u003e \u003cscript setup\u003e const props = defineProps({ msg: String }) \u003c/script\u003e defineProps无需导入，可以直接使用。在定义完成之后，其父组件就可以向使用Attributes一样使用子组件定义的props，也可以使用v-bind来绑定一个变量值：\n1 \u003cChildComp :msg=\"greeting\" /\u003e Emits 除了props之外，子组件可以向父组件发送事件，然后在父组件中使用v-on监听。和props类似，emits需要使用defineEmits定义，且无需导入：\n1 2 3 4 5 \u003cscript setup\u003e const emit = defineEmits(['response']) emit('response', 'hello from child') \u003c/script\u003e 这样，在父组件中就可以使用v-on来监听response事件了：\n1 2 3 4 5 6 7 8 9 10 11 \u003cscript setup\u003e import { ref } from 'vue' import ChildComp from './ChildComp.vue' const childMsg = ref('No child msg yet') \u003c/script\u003e \u003ctemplate\u003e \u003cChildComp @response=\"(msg) =\u003e childMsg = msg\" /\u003e \u003cp\u003e{{ childMsg }}\u003c/p\u003e \u003c/template\u003e 传递slots 除了通过props的形式之外，父组件还可以通过slot的形式来向子组件传递数据：\n1 2 3 \u003cChildComp\u003e 这里是slot \u003c/ChildComp\u003e 这就让父组件可以给子组件传递\u003ctemplate\u003e数据了。在子组件中，也可以在对应的位置声明\u003cslot\u003e，作为默认值：\n1 2 3 \u003ctemplate\u003e \u003cslot\u003eFallback content\u003c/slot\u003e \u003c/template\u003e 然后当父组件什么都不传的时候，就会默认地显示Fallback content。而在父组件传递数据的时候，对应的位置会显示数据：\n1 2 3 4 5 6 7 8 9 10 \u003cscript setup\u003e import { ref } from 'vue' import ChildComp from './ChildComp.vue' const msg = ref('from parent') \u003c/script\u003e \u003ctemplate\u003e \u003cChildComp\u003e{{msg}}\u003c/ChildComp\u003e \u003c/template\u003e 此时，子组件这个地方会显示父组件里的msg内容，而不是Fallback content。\nMore 更多的Vue组件基础，可以参考：https://staging-cn.vuejs.org/guide/essentials/component-basics.html#components-basics\n","description":"","tags":["frontend"],"title":"Basics of Vue","uri":"/posts/engineering/vue/"},{"categories":null,"content":"Tree sitter - advanced parsing What makes tree-sitter powerful is not only its parsing performance. Incrementally parsing, multi-language document support and pattern matching are also great features of tree-sitter. In this article, I'll introduce you those advanced features of tree-sitter.\nIncrementally parsing One of tree-sitter's main design goal is to support incrementally parsing. Update an existing parse tree in tree-sitter is easy and fast - all you need to do is pass the edit range to the tree and re-parse it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Take JSON example in last article as our start let mut parser: Parser = Parser::new(); parser.set_language(tree_sitter_json::language()).unwrap(); let source_code = \"[1, null]\"; let mut old_tree: Tree = parser.parse(source_code, None).unwrap(); // If the JSON code is changed from \"[1, null]\" to \"[1, 2, null]\" let new_source_code = \"[1, 2, null]\"; // First, we need to pass the edit to old_tree // Note the edit range must be defined let edit = InputEdit { start_byte: 3, old_end_byte: 3, new_end_byte: 4, start_position: Point::new(0, 3), old_end_position: Point::new(0,3), new_end_position: Point::new(0,4), }; old_tree.edit(\u0026edit); // Then, re-parse the tree using new code // Pass the old tree so that tree-sitter can do incrementally parsing based on the edit and old_tree let new_tree = parser.parse(new_source_code, Some(\u0026old_tree)).unwrap(); If you have stored some tree nodes before updating the tree, those tree nodes should also be updated. Otherwise, you have to re-fetch those nodes again from the new parse tree. You can use node.edit() to update tree nodes:\n1 node.edit(\u0026edit); Note that this function is only needed in the case that you have the node object before editing the tree, and the node will be used after editing the tree.\nMulti-language Document A single source code file may contains different languages, such as jsx/tsx file, which contains JavaScript and React/Html code. Tree-sitter supports this kind of document by support parsing a certain range of a file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 let source_code = \"1234[1, null]123\"; let r = Range { start_byte: 4, end_byte: 13, start_point: Point::new(0, 4), end_point: Point::new(0, 13), }; // Set parsed range match parser.set_included_ranges(\u0026[r]) { Ok(_) =\u003e {}, Err(e) =\u003e println!(\"{}\", e.to_string()), } // Parse \"[1, null]\" in \"1234[1, null]123\" only let tree = parser.parse(source_code, None).unwrap(); Concurrency Tree-sitter's parse tree supports concurrent accessing, but you have to use tree.clone() to get a copy of the tree. This operation is cheap because it just increase an atomic reference count by 1 internally.\nWalking Trees If you want to traverse the whole parse tree, a tree cursor can be used to walk the syntax tree with maximum efficiency. As the first step, you can initialize a tree cursor from any tree-sitter node:\n1 let cursor = node.walk(); Then you can move the cursor using goto_xxx functions. Those functions returns true if the cursor has been successfully moved to the target node.\n1 2 3 let success = cursor.goto_first_child(); let success = cursor.goto_parent(); let success = cursor.goto_next_sibling(); You can also get the current node, current node's field name and current node's field id using cursor:\n1 2 3 4 // The cursor is always binded to a valid node, so `unwrap()` is not needed when using `cursor.node()` let current_node = cursor.node(); let field_name = cursor.field_name().unwrap(); let field_id = cursor.field_id().unwrap(); Pattern Matching Tree-sitter also provides an approach to search for patterns in a parse tree, which is quite useful for many code analysis tasks.\nQuery Syntax Searching in the parse tree needs a query. In tree-sitter, a query consists of one or more patterns, and each pattern is a S-expression.\nA tree-sitter query in S-expression has two elements in a pair of parentheses. The first is the node's type and the second is a list of other S-expressions which represent the node's children. Here is a query in our JSON example:\n(array (number) (null)) The children can be omitted, the following query matches any array with at least one number child:\n(array (number)) Fields In you want to match a pattern more specifically, you can specify the name of a child node. Here is an example:\n(array left: (number)) This query matches an array which has a number child node named \"left\".\nAlso, you can use ! before a child to constrain that the node does NOT have this child:\n(array left: (!number)) This query matches an array which does NOT have a number child named \"left\".\nAnonymous Nodes The above queries matches only named nodes. To matching anonymous nodes, you can write their name between double quotes:\n(binary_expression operator: \"!=\" right: (null)) In this case, the query matches a binary_expression where the operator is \"!=\" and the child \"right\" is a null node.\nCapturing node Sometimes, you may want to use a specific matched node in a query. You can associate a name using @ character just after that node:\n(array left: (number) @my-number) This query would associate a name \"my-number\" to the number child.\nQuantification Operators Just like regular expression, you can use + and * to represent the repeated sequence of nodes, where + matches one or more repetitions and * matches zero or more repetitions. Here are several examples:\n(class_declaration (decorator)* @the-decorator name: (identifier) @the-name) In this example, the query matches a class_declaration which has zeros or more decorator. All matched decorator and their names are captured.\nMoreover, you can use ? to represent an optional node. The usage is just like + and *:\n(call_expression function: (identifier) @the-function arguments: (arguments (string)? @the-string-arg)) Grouping Sibling Nodes A pair of parentheses () is used to group sibling nodes. Those groups can be marked as repeated or optional using quantification operators *, + and ? introduced in the last section.\n( (number) (\",\" (number))* ) This query matches a comma-separated number sequence.\nAlternations A pair of square brackets [] is used to mark alternative nodes or patterns. This is also similar with regular expression:\n[ \"break\" \"delete\" \"else\" \"for\" \"function\" \"if\" \"return\" \"try\" \"while\" ] @keyword This example matches any of those words and captures it as @keyword.\nWildcard Node In tree-sitter, a wildcard node is represented using _. It matches any nodes(named nodes or anonymous nodes), while (_) matches only named nodes:\n(call (_) @call.inner) This example matches any named nodes in call and captures them as call.inner.\nAnchors . is the anchor operator which is used to constrain matched child patterns. The constrains of an anchor operator only affect named nodes. There are three cases to use anchor operator .:\n. is placed before the first child In this case, the child pattern matches only the first matched child node. For example, if we want to match an array and its first identifier child, this query is good: (array . (identifier) @the-first-identifier) Without using ., this pattern would matches every child identifier and bind the name to all matched children. . is placed after the last child In this case, the child pattern matches the last matched child node, here is a similar example: (array (identifier) @the-last-identifier .) This query matches an array and its last identifier child, binds the name to the last identifier. . is placed between two child patterns If the anchor operator is placed between two nodes, the pattern would matches those two nodes when they are immediate siblings: (dotted_name (identifier) @prev-id . (identifier) @next-id) In this example, the query matches a dotted_name with two consecutive identifiers. That is, if the sequence is a.b.c.d, only a.b, b.c and c.d would be matched. Without the anchor operator ., a.c, a.d and b.d would also be matched. Predicates Predicate S-expression in tree-sitter can be used to represent any metadata or conditions associated with a pattern. It records information and passes the information to the higher level code to perform filtering.\nSome higher-level bindings such as Rust binding has implemented several predicates, like #eq? and #match?.\nA predicate S-expression starts with a predicate name prefixed by #. After the name, you can put @-prefixed names or strings as you want.\nFor example, the following pattern matches identifier whose names is written in SCREAMING_SNAKE_CASE:\n( (identifier) @constant (#match? @constant \"^[A-Z][A-Z_]+\") ) In this example, #match? is the predicate name, @constant and \"^[A-Z][A-Z_]+\" is the following parameters. Tree-sitter's Rust binding has implemented #match? predicate, which accepts two parameters and returns whether the two parameters are matched.\nHere is another example:\n( (pair key: (property_identifier) @key-name value: (identifier) @value-name) (#eq? @key-name @value-name) ) This example matches a key-value pair that the key and value identifiers have the same name.\nThe Query API We've introduced the query syntax in tree-sitter, in this section, we'll introduce how to use query API in Rust.\nCreate a query You can create a new query using Query::new(language, source_str):\n1 2 3 use tree_sitter::Query; let query = Query::new(tree_sitter_json::language(), \"(array (number))\").unwrap(); If there's an error in the query, Query::new() would return a QueryError, which indicates the position and the type of the error in the query string.\nExecute the query To execute the query, you need to create a QueryCursor. QueryCursor carries state and information of an execution of a query, so it cannot be shared between threads, while the Query is thread-safe. Note that even though you cannot use QueryCursor between many threads, you can reuse it a lot of times for many query executions in one thread. Here is an example about how to use QueryCursor to execute a given Query on a syntax node:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // Initialize a query which matches an array with a number child as matched-array let query = Query::new(tree_sitter_json::language(), \"(array (number))@matched-array\").unwrap(); // Initialize a query cursor let mut cursor = QueryCursor::new(); // Match the source code let source_code = \"[1, 2, 3]\"; let parse_tree: Tree = parser.parse(source_code, None).unwrap(); let mut root_node = parse_tree.root_node(); let matches = cursor.matches(\u0026query, root_node, source_code.as_bytes()); // Print out all matches matches.for_each(|m| { println!(\"Match: {:?}\", m); }); // You can also use QueryCursor::captures if you don't care about which pattern is matched let captures = cursor.captures(\u0026query, root_node, source_code.as_bytes()); captures.for_each(|c| { println!(\"Capture: {:?}\", c); }); The output is:\nMatch: QueryMatch { id: 0, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 0 }] } Capture: (QueryMatch { id: 0, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 0 }] }, 0) Note that you have to name the matched node(@matched-array in this example), otherwise you'll get nothing matched. You could also name a lot of node which need to be matched, and then you'll get many matches with many captures. If we add @number in the query above: let query = Query::new(tree_sitter_json::language(), \"(array (number)@number )@matched-array\").unwrap();, we'll get\nMatch: QueryMatch { id: 0, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 1 }, QueryCapture { node: {Node number (0, 1) - (0, 2)}, index: 0 }] } Match: QueryMatch { id: 1, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 1 }, QueryCapture { node: {Node number (0, 4) - (0, 5)}, index: 0 }] } Match: QueryMatch { id: 2, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 1 }, QueryCapture { node: {Node number (0, 7) - (0, 8)}, index: 0 }] } Capture: (QueryMatch { id: 0, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 1 }, QueryCapture { node: {Node number (0, 1) - (0, 2)}, index: 0 }] }, 0) Capture: (QueryMatch { id: 1, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 1 }, QueryCapture { node: {Node number (0, 4) - (0, 5)}, index: 0 }] }, 0) Capture: (QueryMatch { id: 2, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 1 }, QueryCapture { node: {Node number (0, 7) - (0, 8)}, index: 0 }] }, 0) Capture: (QueryMatch { id: 0, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 1 }, QueryCapture { node: {Node number (0, 1) - (0, 2)}, index: 0 }] }, 1) Capture: (QueryMatch { id: 1, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 1 }, QueryCapture { node: {Node number (0, 4) - (0, 5)}, index: 0 }] }, 1) Capture: (QueryMatch { id: 2, pattern_index: 0, captures: [QueryCapture { node: {Node array (0, 0) - (0, 9)}, index: 1 }, QueryCapture { node: {Node number (0, 7) - (0, 8)}, index: 0 }] }, 1) In the matches case, there are THREE matched sequences, each matched sequence contains TWO matched node: @number and @matched-array. This is because we didn't specify the anchor, so even though the @matched-array in the three matches are same, the matched @numbers are different. That's why we have three different match results which all have same matched @matched-array -- the matching call returns all posibilities of matches.\nAs for QueryCaptures, we have SIX of them because we have THREE matched results and each matched result has TWO matched node. So some of the captures have same QueryMatchs -- they are actually the same QueryMatch instance, but each match result has more than one matched node. And the index of QueryCaptures indicates which one is the current capture.\nStatic Node Types For programming languages with static typing, tree-sitter provides type information about specific syntax node. The information is saved in a generated file node-types.json. In this file, there is an array of objects which stores all node types. Each array element describes a particular type of a syntax node:\nBasic info Every array element has two entries:\n\"type\": the node's type defined in the grammar. We can get it using node.kind(), we've introduced it here \"named\": a boolean variable which represents whether a node is named Internal nodes \"type\" and \"named\" fields define a syntax node. For syntax nodes which have children, we have more options to describe them -- \"fields\" or \"children\".\n\"fields\": an object that represents possible fields of a node. The key is the name of the field, and the value is the child type, which is describe blow \"children\": if a node doesn't have \"fields\", \"children\" is what we used to describe all possible named children A child type object describes a particular type of the child node, it's defined by the following fields:\n\"required\": a boolean variable which represents whether there is always at least one node of this type \"multiple\": a boolean variable which represents if multiple chilren can be this type \"types\": an array of possible node types of this child, each element of this array has two entries: \"type\" and \"named\", whose meaning is the same as what we described above Here are two examples of a node with \"fields\" and a node with \"children\":\nExample with fields:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"type\": \"method_definition\", \"named\": true, \"fields\": { \"body\": { \"multiple\": false, \"required\": true, \"types\": [{ \"type\": \"statement_block\", \"named\": true }] }, \"decorator\": { \"multiple\": true, \"required\": false, \"types\": [{ \"type\": \"decorator\", \"named\": true }] }, \"name\": { \"multiple\": false, \"required\": true, \"types\": [ { \"type\": \"computed_property_name\", \"named\": true }, { \"type\": \"property_identifier\", \"named\": true } ] }, \"parameters\": { \"multiple\": false, \"required\": true, \"types\": [{ \"type\": \"formal_parameters\", \"named\": true }] } } } In this example, we define a method_definition node which has four types of fields: body, decorator, name and parameters.\nExample with children:\n1 2 3 4 5 6 7 8 9 10 11 12 13 { \"type\": \"array\", \"named\": true, \"fields\": {}, \"children\": { \"multiple\": true, \"required\": false, \"types\": [ { \"type\": \"_expression\", \"named\": true }, { \"type\": \"spread_element\", \"named\": true } ] } } Supertype nodes In tree-sitter's grammar for some languages, there exists some hidden rules which is used to generate \"supertypes list\" in node-types.json. We won't discuss how to create a grammar or define a parser, so we just introduce how we use \"supertypes\" in the node-types.json.\nIn node-types.json, some nodes have \"subtypes\" field, which specifies the types of nodes that current supertype node can wrap:\n1 2 3 4 5 6 7 8 9 10 11 { \"type\": \"_declaration\", \"named\": true, \"subtypes\": [ { \"type\": \"class_declaration\", \"named\": true }, { \"type\": \"function_declaration\", \"named\": true }, { \"type\": \"generator_function_declaration\", \"named\": true }, { \"type\": \"lexical_declaration\", \"named\": true }, { \"type\": \"variable_declaration\", \"named\": true } ] } As you can see in the example, type \"_declaration\" is defined as the parent of \"class_declaration\", \"funtion_declaration\", \"generator_function_declaration\", \"lexical_declaration\" and \"variable_declaration\".\nThe super type can be used in other types of nodes, which shortens the definition of other nodes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"type\": \"export_statement\", \"named\": true, \"fields\": { \"declaration\": { \"multiple\": false, \"required\": false, \"types\": [{ \"type\": \"_declaration\", \"named\": true }] }, \"source\": { \"multiple\": false, \"required\": false, \"types\": [{ \"type\": \"string\", \"named\": true }] } } } ","description":"","tags":["rust","parsing"],"title":"Tree sitter - advanced parsing","uri":"/posts/code-intelligence/tree-sitter-advanced/"},{"categories":null,"content":"Tree sitter What's tree-sitter? Generally speaking, tree-sitter is a parser generator tool, which creates parsers for programming languages and parses source code to parse tree. Tree-sitter also provides incrementally parsing feature, making updating of parse tree efficient.\nTree-sitter aims to be\nGeneral Fast Robust Dependency-free At the moment(2022/01/23), tree-sitter has been used in many famous projects, such as Neovim, GitHub Code Search, etc.\nLanguage bindings Tree-sitter runtime is written in pure C, so it's easy to embed it to other languages. Tree-sitter provides many language bindings, you can find a list here: https://tree-sitter.github.io/tree-sitter/#language-bindings.\nIn this article, We'll use Rust bindings of tree-sitter. If you don't use Rust, don't worry! Other language bindings of tree-sitter should be similar to use.\nUsing Tree-Sitter Parsers in Rust The Rust bindings of tree-sitter can be found here. You can see all API docs at https://docs.rs/tree-sitter/latest/tree_sitter/.\nThe official document also has introductiosn of tree-sitter's raw C API. If you want to use tree-sitter in other languages, C API is a good start to learn.\nGetting Started To use tree-sitter's Rust API, you should add tree-sitter dependency to your Cargo.toml first:\n1 tree-sitter = \"0.20.3\" You can also build tree-sitter from source, see: https://tree-sitter.github.io/tree-sitter/using-parsers#building-the-library\nBasic Conceptions There are four basic conceptions in tree-sitter:\nlanguage: defines how to parse a particular language parser: generates syntax tree from source code syntax tree: is a tree representation of an entire source code file syntax node: is a single node in syntax tree In Rust's API, they are called:\nlanguage -\u003e Language parser -\u003e Parser syntax tree -\u003e Tree syntax node -\u003e Node An Example Here is an example in Rust that uses tree-sitter JSON parser. Because tree-sitter maintains parsers of most popular programming languages, we don't have to generate parsers for those languages again. Here is a list of available parsers for now.\nTo use tree-sitter's official JSON parser, just add tree-sitter-json crate to your Cargo.toml:\n1 tree-sitter-json = \"0.19.0\" The example code of parsing JSON:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 use tree_sitter::{Node, Parser, Tree}; fn main() { // Create a parser let mut parser: Parser = Parser::new(); // Set the parser's language (JSON in this case) parser.set_language(tree_sitter_json::language()).unwrap(); // Build a syntax tree based on source code stored in a string. let source_code = \"[1, null]\"; let parse_tree: Tree = parser.parse(source_code, None).unwrap(); // Get the root node of the syntax tree. let root_node: Node = parse_tree.root_node(); // Get some child nodes. let array_node: Node = root_node.named_child(0).unwrap(); let number_node: Node = array_node.named_child(0).unwrap(); // Check that the nodes have the expected types. assert_eq!(root_node.kind(), \"document\"); assert_eq!(array_node.kind(), \"array\"); assert_eq!(number_node.kind(), \"number\"); // Check that the nodes have the expected child counts. assert_eq!(root_node.child_count(), 1); assert_eq!(array_node.child_count(), 5); assert_eq!(array_node.named_child_count(), 2); assert_eq!(number_node.child_count(), 0); // Print the syntax tree as an S-expression. let s_expression = root_node.to_sexp(); println!(\"Syntax tree: {}\", s_expression); } Execute cargo run, you'll get the following output:\nSyntax tree: (document (array (number) (null))) The output is the S-expression of the JSON string input.\nThis program uses tree-sitter's Rust API and tree-sitter-json crate. If you don't want to add tree-sitter-json to your dependency list, there is a second approach: add cc to your `[build-dependency] and embed the parser at build stage by add the following code to your build script:\n1 2 3 // Cargo.toml [build-dependencies] cc=\"*\" 1 2 3 4 5 6 7 8 9 10 11 12 13 // build.rs use std::path::PathBuf; // Suppose that you have `tree-sitter-javascript` in your root directory fn main() { let dir: PathBuf = [\"tree-sitter-javascript\", \"src\"].iter().collect(); cc::Build::new() .include(\u0026dir) .file(dir.join(\"parser.c\")) .file(dir.join(\"scanner.c\")) .compile(\"tree-sitter-javascript\"); } Then, you can declare a language and set the language to your parser in your source code:\n1 2 3 4 extern \"C\" { fn tree_sitter_javascript() -\u003e Language; } let language = unsafe { tree_sitter_javascript() }; parser.set_language(language).unwrap(); The second approach to use tree-sitter is what's in tree-sitter's official document, but anyway, I prefer the first approach.\nBasic Parsing In the section above, we have an example to parse JSON string. In this section, I'll introduce more about basic parsing using tree-sitter.\nProviding the code In the example above, we simply provide source code to the parser. However, in many modern editors and IDEs, the code is stored as other data structures such as rope and piece table(VSCode uses it as TextBuffer!).\nTo support those custom data structures, tree-sitter also provides parse_with() method. With it, you can write your own function to read a chunk of text at a given byte_offset and position:\n1 2 3 4 5 6 7 8 9 10 11 12 use tree_sitter:Point; // call_back converts your own data structure at the byte_offset and position to the input of parser // call_back returns a slice of UTF8-encoded text starting at the given byte_offset and position fn call_back(byte_offset: usize, position: Point) -\u003e String { } fn main() { ... // Parse using user provided data let tree = parser.parse_with(\u0026mut call_back, Node).unwrap() } Syntax node Each node in the parse tree has a type, which is a string that defined in grammar:\n1 let node_type: \u0026str = node.kind(); You can also get all type definitions by\n1 2 3 4 use tree_sitter_json::NODE_TYPES; fn main () { println!(\"{}\", NODE_TYPES); } You can get the position of a syntax node in the form of range in bytes or row/column pair:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Position let start: Point = node.start_position(); let end: Point = node.end_position(); // Byte range use std::ops::Range; let byte_range: Range = node.byte_range(); // Get both use tree_sitter::Range; let range: Range = node.range(); let start_byte: usize = range.start_byte; let end_byte: usize = range.end_byte; let start_point: Point = range.start_point; let end_point: Point = range.end_point; Retrieving nodes Every parse tree has a root node:\n1 let root: Node = tree.root_node(); You can also access children, siblings and parent of a Node:\n1 2 3 4 5 6 7 8 9 10 11 // Get children for i in 0..node.child_count() { let child = node.child(i).unwrap(); } // Get parent let parent = node.parent().unwrap(); // Get siblings let next_sib = node.next_sibling().unwrap(); let prev_sib = node.prev_sibling().unwrap(); All those functions returns Option\u003cNode\u003e. The returned value id None means the required node is not exist. For example, the parent of a root_node is always None:\n1 2 // None let null_node = root_node.parent(); Named nodes The parse tree generated by tree-sitter is actually concrete syntax tree(CST), which contains all tokens in the source text. But in many cases, we don't need so much information. It's easier to do code analyze on an abstract syntax tree. An abstract syntax tree(AST) is also a parse tree, but has less information compared with a CST. Tree-sitter supports both use cases by using named nodes and anonymous nodes.\nNamed and anonymous nodes are defined in grammar. Here is an example:\nif_statement: ($) =\u003e seq(\"if\", \"(\", $._expression, \")\", $._statement); It's the definition of if_statement syntax node. This node has 5 children: the condition expression($._expression), the body expression($._statement), \"if\", \"(\" and \")\". In this case, the condition expression and the body expression are named nodes, because they have explicit names defined in the grammar. And \"if\", \"(\" and \")\" are anonymous nodes, because they are represented as raw string in the grammar.\nIn the tree-sitter's parse tree, you can check whether a node is a named node using node.is_named().\nWhen you traverse the tree, you can also use named variants of the describled methods above to ignore all anonymous nodes:\n1 2 3 let children_num = node.named_child_count(); let children = node.named_child(idx).unwrap(); let prev_sib = node.prev_named_sibling().unwrap(); Field To make the parse tree easier to analyze, tree-sitter supports field names of a syntax node in grammar. For example, you can use the field name to access children of a syntax node:\n1 let child = node.child_by_field_name(\"field_name\"); Field also has an unique id which you can use to access the child:\n1 let child = node.child_by_field_id(0); You can convert between the string field name and the field id using Language:\n1 2 3 4 5 6 7 // Get language let json_language = tree_sitter_json::language(); // Convert between field name and id let field_count = json_language.field_count(); let id = json_language.field_id_for_name(\"field_name\").unwrap(); let field = json_language.field_name_for_id(0).unwrap(); Similarly, node's type has an id as well and you can use Language to convert between them:\n1 2 3 4 5 6 7 8 // Convert between node type and id let node_type_count = json_language.node_kind_count(); let id = json_language.id_for_node_kind(\"array\", true).unwrap(); let node_type = json_language.node_kind_for_id(0).unwrap(); // Check node type's property let is_visible = json_language.node_kind_is_visible(); let is_named = json_language.node_kind_is_named(); That's all about tree-sitter basics In this article, we've covered most basic tree-sitter APIs. In the next articles, I'll introduce some advanced topic of tree-sitter, like incrementally parsing, pattern matching and grammar writting.\n","description":"","tags":["rust","parsing"],"title":"Tree sitter - basics","uri":"/posts/code-intelligence/tree-sitter/"},{"categories":null,"content":"Rust Build Scripts https://doc.rust-lang.org/cargo/reference/build-scripts.html\nWhat's Build Scripts in Rust Build scripts in Rust is used to integrate Rust code to external tools or dependecies such as C libraries, or execute tasks before compiling the Rust project. You can enable build scripts by placing a file named build.rs at the root of a Rust package. Cargo will compile and execute the build script before building the package.\nHere are some usages of build scripts according to the Cargo Book:\nBuilding a bundled C library. Finding a C library on the host system. Generating a Rust module from a specification. Performing any platform-specific configuration needed for the crate. The following sections will introduce the basics of build scripts in Rust.\nLife Cycle of Build Scripts Build scripts are compiled to an executable and executed before the package is built. It should return a zero exit code if there's no error. The rest of the package will be compiled after the build script finished. If any error occurs, a non-zero exit code should be returned by the build script to interrupt the build process.\nBy default, Cargo will rerun the build script if any file changes in the project. But you can also specify the watched files using rerune-if-changed command:\n1 println!(\"cargo:rerun-if-changed=wrapper.h\"); Inputs and Outputs of the Build Script All inputs of the build script are passed using environment variables. Here is the list of all available inputs for build scripts.\nAll output files of the build script should be saved in the directory specified in OUT_DIR environment variable. The script may commnunicate with Cargo by printing commands like cargo:xxx to stdout, all other lines will be ignored.\nHere is a list of all commands that Cargo accepts according to the Cargo Book:\ncargo:rerun-if-changed=PATH — Tells Cargo when to re-run the script. cargo:rerun-if-env-changed=VAR — Tells Cargo when to re-run the script. cargo:rustc-link-arg=FLAG – Passes custom flags to a linker for benchmarks, binaries, cdylib crates, examples, and tests. cargo:rustc-link-arg-bin=BIN=FLAG – Passes custom flags to a linker for the binary BIN. cargo:rustc-link-arg-bins=FLAG – Passes custom flags to a linker for binaries. cargo:rustc-link-lib=[KIND=]NAME — Adds a library to link. cargo:rustc-link-search=[KIND=]PATH — Adds to the library search path. cargo:rustc-flags=FLAGS — Passes certain flags to the compiler. cargo:rustc-cfg=KEY[=\"VALUE\"] — Enables compile-time cfg settings. cargo:rustc-env=VAR=VALUE — Sets an environment variable. cargo:rustc-cdylib-link-arg=FLAG — Passes custom flags to a linker for cdylib crates. cargo:warning=MESSAGE — Displays a warning on the terminal. cargo:KEY=VALUE — Metadata, used by links scripts. Build Dependencies You can specify dependencies for build scripts using build-dependencies section in Cargo.toml:\n1 2 [build-dependencies] cc = \"1.0.46\" The build script cannot use dependencies or dev-dependencies in Cargo.toml.\nThe package.links Manifest Key The package.links manifest key is used to specify the native libraries that the package depends on. When using this key, the package must have a build script with the cargo:rustc-link-lib=[KIND=]=NAME command. With links key, Cargo knows about the native libraries that the package depends on. Also, metadata is passed as the output of build script in the KEY=VALUE format using links key(see cargo:KEY=VALUE command). This metadata is automatically converted to an environment variable for the dependent's build script.\n1 2 3 [package] # links to libonnxruntime links = \"onnxruntime\" By default, there is at most one package per links value to prevent conflict symbols.\n*-sys Packages In Rust's convention, any package that link to system libraries should have a -sys suffix. In our case, we generated onnxruntime-sys at 【TODO】. It's also common to have another package which wraps the -sys package, for example, onnxruntime, to provide safe and high-level APIs.\nOverriding the Build Script You can also override params in build script at Cargo configuration file. Here is an example:\n1 2 3 4 5 6 7 8 9 [target.x86_64-unknown-linux-gnu.foo] rustc-link-lib = [\"foo\"] rustc-link-search = [\"/path/to/foo\"] rustc-flags = \"-L /some/path\" rustc-cfg = ['key=\"value\"'] rustc-env = {key = \"value\"} rustc-cdylib-link-arg = [\"…\"] metadata_key1 = \"value\" metadata_key2 = \"value\" In this case, if a package links to foo, then it's build script will not be executed. Instead, the medata specified in Cargo configuration file will be used.\nLearn Build Script from Examples The Cargo book provides a complete example of how to write build scripts.\n","description":"","tags":["rust"],"title":"Rust Build Scripts","uri":"/posts/engineering/rust-build-scripts/"},{"categories":null,"content":"Rust bindgen What's bindgen? bindgen is a tool which generates Rust FFI to C/C++ libraries automatically. It's quite useful when we want to use a C/C++ library in Rust. For example, PyTorch provides C library for users which don't want to use Python. With bindgen, we can quickly create Rust binding of PyTorch C library from C header(see tch-rs)\nUse bindgen The recommended way to use bindgen is using it in build.rs. build.rs is a Rust file placed in the root of a package which is used to integrate third-party libraries or user customized tools to Rust compiling process. Cargo will compile and execute build.rs first and then build the package.\nBecause many C/C++ headers have platform-specfic features, with using bindgen inside build.rs, we can generate bindings for the current target on-the-fly. Other users can also use your package by generating bindings for their platform.\nIn the following sections, we will take onnxruntime C library as an example, generate Rust bindings using bindgen.\nAdd bindgen as a build dependency First, we have to add bindgen as the build dependency. A build dependency is the dependency which is only used in building(see this section in The Cargo Book).\n1 2 [build-dependencies] bindgen = \"0.59.1\" Create a wrapper.h Header Because some libraries have more than one headers, we can include those headers in wrapper.h and take wrapper.h as an entrypoint for bindgen.\nIn our example, onnxruntime's header file can be found at https://github.com/microsoft/onnxruntime/blob/master/include/onnxruntime/core/session/onnxruntime_c_api.h. So we can just download it and then use it in build.rs.\nCreate build.rs file Create build.rs at the project root, Cargo will automatically compile and execute it before compiling the rest of the project.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 extern crate bindgen; use std::env; use std::path::PathBuf; fn main() { // Tell cargo to invalidate the built crate whenever the wrapper changes println!(\"cargo:rerun-if-changed=onnxruntime_c_api.h\"); // The bindgen::Builder is the main entry point // to bindgen, and lets you build up options for // the resulting bindings. let bindings = bindgen::Builder::default() // The input header we would like to generate // bindings for. .header(\"onnxruntime_c_api.h\") // Tell cargo to invalidate the built crate whenever any of the // included header files changed. .parse_callbacks(Box::new(bindgen::CargoCallbacks)) // Finish the builder and generate the bindings. .generate() // Unwrap the Result and panic on failure. .expect(\"Unable to generate bindings\"); // Write the bindings to the src/bindings/[os]/[arch]/bindings.rs file. let out_path = PathBuf::from(env::var(\"CARGO_MANIFEST_DIR\").unwrap()) .join(\"src\") .join(\"bindings\") .join(env::var(\"CARGO_CFG_TARGET_OS\").unwrap()) .join(env::var(\"CARGO_CFG_TARGET_ARCH\").unwrap()); // If the directory doesn't exist, create it fs::create_dir_all(\u0026out_path).expect(\"Unable to create dir\"); // Write bindings to file bindings .write_to_file(out_path.join(\"bindings.rs\")) .expect(\"Couldn't write bindings!\"); } Run cargo build, and then the bindings to onnxruntime are generated, you can find it at src/bindings/[os]/[arch]/bindings.rs.\nUse generated bindings include! macro can be used to dump the generated bindings into crate's main entry point.We can add different cfg headers for bindings of different platforms.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #![allow(non_upper_case_globals)] #![allow(non_camel_case_types)] #![allow(non_snake_case)] #[cfg(all(target_os = \"windows\", target_arch = \"x86_64\"))] include!(concat!( env!(\"CARGO_MANIFEST_DIR\"), \"/src/bindings/windows/x86_64/bindings.rs\" )); #[cfg(all(target_os = \"macos\", target_arch = \"x86_64\"))] include!(concat!( env!(\"CARGO_MANIFEST_DIR\"), \"/src/bindings/macos/x86_64/bindings.rs\" )); Because onnxruntime's symbols are defined in C, they may not follow Rust's style convention. We can suppress warnings by a bunch of #![allow(...)] pragmas.\nTest bindings The generated code contains some tests. We can also add our test at src/lib.rs:\n1 2 3 4 5 6 7 8 #[cfg(test)] mod tests { use super::*; #[test] fn it_works() { assert_eq!(8, ORT_API_VERSION); } } Run cargo test, cargo will execute all tests defined in bindings.rs and lib.rs.\ntest bindgen_test_layout_wait__bindgen_ty_2 ... ok test tests::it_works ... ok test bindgen_test_layout_wait__bindgen_ty_1 ... ok test result: ok. 81 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.01s So far, we have successfully generated bindings for onnxruntime and tested them. With bindgen, we can also customize the generated bindings. For more options of bindgen, check https://rust-lang.github.io/rust-bindgen/customizing-generated-bindings.html.\nMore about build.rs We just generated bindings for onnxruntime. But that's not enough if we want to use it in production, especially when you want to deploy the application using the bindings to different platforms. This is where build.rs comes into play.\nAs we mentioned above, build.rs is compiled and executed before the package is compiled. So we can add somethings like platform-specific configuration, library downloading, etc. to build.rs. The full reference of build.rs is here.\n","description":"","tags":["rust"],"title":"Rust bindgen","uri":"/posts/engineering/rust-bindgen/"},{"categories":null,"content":"背景 在Transformer之前，NLP领域的seq2seq模型主要基于RNN结构，如LSTM，GRU等。这种结构有几个难以克服的缺点:\n难以并行化 速度慢 长距离依赖记忆不够长 Google提出的基于self-attention的Transformer和Transformer-XL结构可以很好地解决RNN的缺点，所以自2017年以来，Transformer已经成为了NLP领域对语言建模的默认选项。 Bert、GPT、XLNET等模型的基础单位都是Transformer。\nOverview 论文里面的Transformer沿用了传统的encoder-decoder结构。其主要构造单元为：\nMulti-Head Attention layer Position-wise fully connected layer Positional encoding LayerNorm 需要注意的是，对于encoder和decoder，每个结构可以重叠N次，在论文里面N=6，即encoder叠了N层。这时，decoder的每一层拿到的K-V的输入都是encoder的最后一层的结果\nAttention Multi-Head Attention是Transformer中最重要的一部分。\nScaled Dot-Product Attention Multi-head attention的基础是Scaled Dot-Product Attention, 这里，输入有三个，Q代表query，K、V代表一个Key-Value对。用公式表示为：\n$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})\\times V $$\n所谓self-attention实际上就是Q、K、V三个是一样的。这里的Q、K、V都是多个单词embedding的矩阵。如果句子长度为128个token，embedding的长度$d_{model}=512$，那么左边的softmax输出的实际上就是128个权重向量，和value embedding相乘得到加了self-attention的结果。\n可以简单理解为，self-attention是通过Q和K算出注意力需要放到哪些维度上，即权重向量，然后乘V得到加上attention之后的输出。\nMulti-Head Attention 了解了Scaled Dot-Product Attention之后，Multi-Head Attention就简单了：在输入的时候，把V、K、Q都重复h次，这个h就是head的数目。当然这里不是简单的复制，而是对V、K、Q做线性变换h次。注意，在线性变换的时候，需要把输出的维度变为$\\frac{d_{model}}{h}$，这样的话在最终输出concat之后的结果的维度才正确。同时，这样也不会增加计算消耗。\n在模型中，除了self-attention之外，还有一个encoder-decoder attention，区别就是在encoder-decoder attention中，Q和K变成了encoder的输出。V还是上一步decoder的输出。这里和传统RNN结构一样。\n实际上，Transformer的并行性主要体现在训练上面，在做推理的时候，需要和传统RNN一样，一个词一个词地预测，并且在预测下一个词的时候需要当前的输出作为输入。\nPosition-wise Feed-Forward Layer 这一层简称FFN，是对每一个位置的结果单独训练一个网络，位置和位置之间不共享参数，因此叫\"Position-wise\"。FFN的结构也很简单：两个线性变换，其中一个是ReLU： $$ FFN(x) = max(0, xW_1+b_1)W_2+b_2 $$\n中间层的宽度是超参数$d_{ff}$，在论文中取值为2048\nNorm Layer Transformer中，Attention层和FFN层后面都加了一个Normalization：\n$$ LayerNorm(x+Sublayer(x)) $$\n其中，LayerNorm的方法见Hinton的论文。\nPositional Encoding 由于self-attention中并没有保存位置相关的信息，因此需要加Positional Encoding。\n在Transformer论文中，作者提供了两种Positional Encoding：第一种是基于三角函数的，第二种是learned positional embedding。经过实验两者的结果差不多，所以作者使用的基于三角函数的Positional Encoding。这里，i是维度，pos是token的位置。所以，对于每个维度，位置编码的波长都是不同的。而对于相同的维度来说， $PE_{pos+k}$ 总是可以表示为$PE_{pos}$的线性函数。\nTransformer的问题 虽然在理论上，Transformer接受任意长的序列进行并行训练。但是，在实际训练中，由于GPU、TPU内存的限制，一般的做法是指定一个固定的上下文长度，然后把输入序列按照这个长度进行切分(segmentation)，然后把每个segment扔到Transformer里面进行训练。这种训练方式带来了两个缺点：第一个是无法捕捉跨segment的长距离依赖，第二个是固定长度切分会把同一个语义块（如一个句子）切分到不同的segment里面，这样的话在预测时，由于缺乏上文信息对前几个单词的预测效果很差。在训练时，也会降低收敛速度。\n在预测时，Transformer在每一步只预测下一个单词，然后右移一位再预测。同样是只能拿到segment内的信息，且右移之后所有的预测步骤都要走一遍，速度慢。\n为了解决这些问题，Google提出了Transformer-XL\nTransformer-XL Transformer-XL主要有两个贡献：1. 解决fixed-length context问题。2.引入了一种新的位置编码方式\nSegment-Level Recurrence with State Reuse Transformer-XL解决fixed-length context的方式是引入recurrent：对于每个segment，重用其之前的segment的状态开始训练而不是从头开始训练。\n这里，$h_{\\tau}^n$表示第$\\tau$个segment $s_\\tau$生成的第n个隐层状态序列。所以这里首先把前面的segment和当前的segment做一个concat，然后再用线性映射的方式生成新的Q、K、V扔进Transformer中。\n在预测时也是类似，可以用到之前所有的segment的信息。由于所有的segment的隐状态都可以被缓存，所以相比Transformer，Transformer-XL的速度可以提升1800+倍。\nRelative Positional Encoding 在Transformer-XL中，由于要对不同的窗口进行处理，因此在原版Transformer中的基于三角函数的绝对位置编码就会出现问题：原来的编码只和维度i和token的位置pos有关系，那么在不同的segment中会出现一样的位置编码（即有相同的pos和i）。\n原来的绝对位置编码公式中，$U_{1:L}$ 是绝对位置编码，$E_{s_\\tau}$是embedding，$h_\\tau$是隐状态\n所以，Transformer-XL引入了相对位置矩阵：\n$$ R_i \\in \\mathbb{R}^{L_{max}\\times d} $$\n这里，i是两个位置的相对距离，$L_{max}$是整个输入序列的最长长度，在实际计算中，i可以是从0到M+L-1的任何数字，M是记忆的长度，L是segment长度。\n在实际应用中，R是可以被提前计算出来的，使用vanilla transformer中的三角函数即可。\n不同于vanilla Transformer中单纯地把位置编码和embedding加起来，Transformer-XL是在计算attention score的时候动态插入相对位置编码\n这个公式猛一看不好理解，回想一下原始的attention score的计算方式：\n$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})\\times V $$\n考虑进来原来的绝对位置编码U和线性变换矩阵W，有\n$$ QK^T=(E+U)W_q\\times ((K+U)W_k)^T $$\n那对于第i个query和第j个key来说，有\n对比两个方案，发现区别有3点：\n使用提前计算好的相对位置编码$R_{i-j}$代替绝对位置编码$U_j$\n这样就在计算attention score的时候动态引入了相对位置编码\n使用可学习的参数$u,v$代替$W_qU_i$\n在考虑相对位置时，比较标准是位置i，所以这里的$U_i$是固定的($R_0$)，因此对于任意的i，这里都可以采用同样的向量表示。因此这两个参数就被归一化成了可学习的参数$u,v$\n使用$W_{k,E},W_{k,R}$代替$W_k$ 之前由于是$(E+U)W_k$，对E和U做的是同一个变换。这里把它分开，变成两个可学习的线性变换。其中$W_{k,E}$对应的是对key的embedding的变换，而$W_{k,R}$是对key的相对位置编码的变换。同理，使用两个参数$u,v$代替$W_qU_i$也是相同的理念\n所以对于新的attention的公式，(a)对应的是content-based addressing，即query和key的内容(embedding)之间的关系；(b)对应的是content-dependent positional bias，即query内容相关的位置关系；(c)对应的是global content bias，即全局的内容影响（key的影响）；(d)对应的是global positional bias，全局的位置关系，即当前key的位置。\n这种位置编码以前的论文也出现过，但是只有(a)和(b)，舍弃了(c)和(d)，即只看重和当前query相关的信息和位置编码。在Transformer-XL中，把和全局信息相关的东西也考虑进来了。\n在实际实现的时候，通过平移可以减小bd的计算量，见Appendix B\n相关代码为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def rel_multihead_attn(w, r, r_w_bias, r_r_bias, attn_mask, mems, d_model, n_head, d_head, dropout, dropatt, is_training, kernel_initializer, scope='rel_attn'): # w : token emb # r : 反向的绝对位置emb # r_w_bias ：公式中的u # r_r_bias : 公式中的v ... ... ... # 提取W_q + u和W_q + v rw_head_q = w_head_q + r_w_bias rr_head_q = w_head_q + r_r_bias # 计算(a)和(c) AC = tf.einsum('ibnd,jbnd-\u003eijbn', rw_head_q, w_head_k) # 计算(b)和(d)，这里的计算用了一个trick，使得BD的O(N^3)的计算量降到了O(N)，见论文appendix B BD = tf.einsum('ibnd,jnd-\u003eijbn', rr_head_q, r_head_k) BD = rel_shift(BD) # 对QK^T做scale attn_score = (AC + BD) * scale # Mask attn_mask_t = attn_mask[:, :, None, None] attn_score = attn_score * (1 - attn_mask_t) - 1e30 * attn_mask_t # Softmax + dropout attn_prob = tf.nn.softmax(attn_score, 1) attn_prob = tf.layers.dropout(attn_prob, dropatt, training=is_training) # attention向量乘以V得到最终的结果 attn_vec = tf.einsum('ijbn,jbnd-\u003eibnd', attn_prob, w_head_v) ... ... 在Segment-Level Recurrence和Relative Positional Encoding的基础上，一个N层的Transformer-XL的整体公式表示为：\n其中，n=1,...,N，$h_\\tau^0=E_{s_\\tau}$，即第一层的h是embedding。\nReference Transformer: https://arxiv.org/abs/1706.03762\nTransformer-XL: https://arxiv.org/abs/1901.02860\nThe Annotated Transformer: https://nlp.seas.harvard.edu/2018/04/03/attention.html\nThe Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/\n","description":"","tags":["“deep learning\"","nlp","transformer","paper note"],"title":"From Transformer to Transformer-XL","uri":"/posts/deep-learning/tttttransformer/"},{"categories":null,"content":"Introduction Software engineering(SE) and programming languages(PL) should make the transition just like NLP: from brittle rule-based expert systems to statistical methods and machine learning techniques.\nThe paper is orginized as follows:\nDifference between natural language and source code Taxonomy of probabilistic models of source code SE and PL applications Challenges and future directions The Naturalness Hypothesis The Naturalness Hypothesis\nSoftware is a form of human communications; software corpora have similar statistical properties to natural language corpora; these properties can be exploited to build better software engineering tools\nInsights:\nBecause coding is an act of communication, so the large code corpora is expected to have rich patterns, just like natural language corpora. Example: models developed for natural language are effective for source code Probabilistic model can be learned to describe how developers naturally writes and use code. Naturalness means learnable and predictionable Text, Code and Machine Learning Source code plays a role of connector between computers and the human mind.\nHindle et al. shows that code is more predictable than texts using n-gram language model.\nEnumerate differences between code and texts is helpful for us to gain some insights on modifying NLP techniques to deal with code:\nCode is executable and has formal syntax and semantics. We will discuss these aspects comparing code with text.\nExecutable Code is more brittle, small changes may lead to huge differences. Texts are usually more robust to readers. It requires combination of probabilistic and formal methods.\nProgramming languages can be translated between each other exactly, because all mainstream programming languages are Turing-complete.\nFormality Programming languages are formal languages. Code's formality facilites reuse, Gabel et al. found that code is more pattern dense than text.\nAlso, code has less ambiguity than text, but the problem of ambiguity still exists.\nCross-Channel Interaction Natural semantic units of code are identifiers, statements, blocks and functions. All of them have less semantic information than textual semantic units like words and sentences.\nAlmost 70% words are identifiers and neologisms(新词) are more common in code.\nHow existing work handles neologism: introduce cache mechanisms or decompose identifiers at a subtoken level Which semantic unit of code is more useful? It's an open question.\nProbabilistic Models of Code Code-generating Models General code-generating models can be formulated as: $$ P_D(c|C(c)) $$ When context $C(c)$ is $\\emptyset$, the probability distribution $P_D$ is a language model for code; when $C(c)$ is code, $P_D$ is a transducer model; when $C(c)$ is natural language, $P_D$ is a code-generative multimodal model of code.\nIn addition, by the way they generate code's structure, code-generating models can be divided into three types:\nToken-level models: generate token sequences\nCannot always generate executable code Approaches: N-gram, with smoothing and cache mechanism. RNN Syntactic models: generate tree structures\nGenerate syntactically correct code Different from application in NLP, PCFG is not suitable for code generation. Because nearby tokens may be far away in the AST, PCFG cannot even capture close dependencies(as well as long-range dependencies, but this is not the point, n-gram do not capture this neither) Expensive computation cost than token-level model Semantic models: generate graph structures\nDIfficult, less resource on this topic Types of Code Generating Models 1. Language Models LM models programming language itself without using any external content.\nCode LMs are evaluated using perplexity or cross-entropy just like normal LM in NLP\n2. Code Transducer Models Code transducer models translate code from one format to another format. For example, pesudo code generator or code migration.\nMost of transducer models are phrase based model, which is not good at describing long-range dependency.\nTransducer models can be evaluated using BLEU, etc.\n3. Multimodal Models Multimodal model exploits non-code modalities, like comments, search queries, etc.\nThe key idea is to convert natural language to an intermediate representation, then parse it to code, which is similar with semantic parsing in NLP\nRepresentational Models of Code Different from generative models, representational models learns embeddings from code.\nDistributed representations(Code embedding) Tasks\nPredict method name Predict relevant API sequences(Seq2seq) Represent solution-problem mapping Source code input\u0026output pairs to review student assignment Neural code-generative models Generate other modalities like comments Structured prediction Predict interdependent variables, like POS prediction in NLP. Structured prediction and distributed representations are not exclusive.\nExample:\nRaychev et al. represent code as a variable dependency network, each variable is a node, model pairwise interactions as a CRF -\u003e predict types and names of variables\nGu et al. predict the sequence of API calls\nPattern Mining Models Pattern mining models aims to discover a finite set of human-interpretable patterns from source code.\nExample:\n​\tFowkes et al. learn the latent variables of a graphical model to infer common API usage patterns\nApplications Recommender Systems Modeling programmers' intent is a challenge.\nThe most prominent recommender system in code intelligence field is code completion.\nExisting Research:\nn-gram and cached n-gram:\nn-gram LM is used to model source code, cache is used to catch local information\nAST-level LM\nMaddison and Tarlow\nInferring Coding Conventions Coding convention: syntactic constraints on code beyond grammar, like formatting, CamelCase naming, etc.\nLearning of coding conventions from codebases helps teams to determine their coding conventions\nMachine learning models for learning surface structure are well-suited for this task, while there exists a challenge: the sparsity of the code constructs.\nCode Defects Finding code defects is quite difficult beacuse of the rarity of defects and the extreme diversity of source code.\nProliminary work shows that the lower probability of specific code may indicate code defects. Not all anomalous behavior is a bug(it may be just rare behavior), the vast diversity of code constructs entails sparsity, from which all anomaly detection methods suffer.\nCode Translation, Copying, and Clones Translation The success of SMT has inspired researchers to use ML to translate code from one source language to another. Translation on similar languages are easier. But for languages which have different memory allocations, translation is difficult.\nClones Clones may indicate refactoring opportunities.\nEmbedding can be used to detect clones due to the similar snippets share the similar distributed vector representations. Use embedding allows us to learn a continuous similarity metric(like cosine distance) between code locations, rather than using edit distance.\nCode to Text and Text to Code Code to Text C2T has applications to code documentation and readability.\nExample: code to pseudocode, code to comments\nText to Code This area is more related to semantic parsing in NLP.\nSemantic parsing is the task of converting a natural language utterance into a representation of its meaning, often database or logical queries. Those representations can be used in subsequentily question answering.\nBased on current research, the target code can be Excel macros, Java expressions, shell commands, etc.\nFor more information, please refer to Neubig's survey of code generation methods.\nDocumentation, Traceability and Information Retrieval General C2T and T2C can be used here, but more specialized solutions are more effective on problems like documentation and code search. Information retrieval methods are widely used.\nCode search problems are evaluated using rank-based measures.\nProgram Synthesis Program synthesis is concerned with generating full or partial programs from a specification. When the specification is natural language, this is a semantic parsing task(same as T2C). Besides natural language, a lot of other specifications can be used in program synthesis.\nProgram Analysis Program analysis aims to soundly extract semantic properties (like correctness) from programs. There are three streams which exploit probabilistic models to deal with code sound analyses problem.\nA family of modes relaxes the soundness requirement, yielding probabilistic results instead\nThe second stream uses machine learning to create models that produce plausible hypotheses of formal verification statements that can be proved\nExample: Brockschmidt et al. proposed a model which generates separation logic expressions from the heap graph of a program, suitable for formally verifying program's correctness.\nThe third stream learns heuristics replacing hard-coded heuristics to speed-up the search for a formal automated proof\nChallenges and Future Directions Deep learning is popular, but it is not necessary. In many cases, simple models can outperform advanced off-the-self deep learning methods.\nBridging Representations and Communities In software engineering research, a well-defined and widely useful set of representations are used, such as AST, CFG, etc. In contrast, machine learning usually uses continuous representations.\nIntroducing better representations that bridge the gap between machine learning and source code will allow the probabilistic models of code to reason about the rich structure and semantics of code.\nData Sparsity, Compositionality and Strong Generalization Sparsity: it's hard to find multiple source code elements which perform exactly the same tasks.\nCompositionality: meaning of some element can be understood by composing the meaning of its constituent parts.\nExample\nMethods -\u003e Classes\nWord -\u003e Variable names in camel case naming\nThis area is still challenging for general machine learning fields, because capturing relations between objects especially across abstraction levels is difficult.\nIf suﬃcient progress is to be made, representing source code artifacts in machine learning will improve signiﬁcantly, positively aﬀecting other downstream tasks\nStrong generalization: big machine model learned from big code is usually to large for developer's machine. Hence, generalization is a deployability problem.\nDirection: transfer learning \u0026 one-shot learning Learning from multiple views of source code may help to deal with generalization problem and data sparsity Measures High-quality benchmarks, shared tasks and evaluation metrics play an important role win great success in NLP and CV in recent years.\nSome meansurements can be imprecise\nSome metrics that are widely used in NLP are not suitable for source code, such as BLEU.\nSmall changes in source code may lead to big difference; syntactically diverse answers may be semantically equivalent.\nNew Domains Debugging Trace buggy code during execution\nTraceability Study links among software engineering artifacts\nExample: fix commit -\u003e bug reports\nCode Completion and Synthesis We don't have a well-accepted benchmark in this area.\nWhen reasoning about incomplete code's semantics, modeling how code could be completed is helpful\nEducation Assistive Tools Conclusions Almost every area of program analysis and software engineering would benefit from the development of probabilistic models of source code.\nProbabilistic models of source code raise the exciting opportunity of learning from existing code, probabilistically reasoning about new source code artifacts and transferring knowledge between developers and projects\nMost of the research contained in this review was conducted within the past few years\n","description":"","tags":["paper note","research"],"title":"A Survey of Machine Learning for Big Code and Naturalness","uri":"/posts/code-intelligence/a-survey-of-machine-learning-for-big-code-and-naturalness/"},{"categories":null,"content":"Original post: http://www.sheshbabu.com/posts/rust-module-system/\nRust的模块系统比较特殊，初学者不太容易上手理解，上面的文章详细介绍了Rust模块系统，下面是阅读笔记。\n工程结构 首先，我们看一个典型的Rust工程的文件结构，和其中的调用\nExample 1 首先看第一个例子：从main.rs中调用config.rs。在Rust中，每一个文件或文件夹都被看做一个module，如config.rs，实际上就是一个名为config的module。我们可以从其他的文件中导入该module。在最初没有任何导入语句的时候，Rust编译器只能看到如下的结构：\n默认情况下，Rust的编译器只会把main.rs看做是crate模块，而不会根据文件目录自动生成模块树，所有的模块结构都需要我们手动导入构建。\n那么，接下来我们就手动构建我们的模块树。在Rust中，关键字mod被用来声明一个模块或者子模块。比如，对于config.rs，我们需要使用mod config;来声明该模块。\n容易出错的是，我们不是在config.rs中使用mod config;声明，而是需要在main.rs中使用mod config;去声明config模块：\n1 2 // main.rs mod config; // declare config mod 在main.rs中声明了config模块之后，Rust的编译器就会去寻找config.rs文件或者config/mod.rs文件来构建模块结构。如果config.rs中有一个名为print_config()的函数，那么就可以通过config::print_config()来调用了。下面是完整的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // main.rs mod config; fn main() { config::print_config(); println!(\"main\"); } // config.rs pub fn print_config() { // 注意需要把函数声明为public println!(\"config\"); } 到此为止我们学会了同级的模块导入。但是在实际工程中，我们往往希望通过文件夹来把一个系列的文件组织到一起。下面看第二个例子。\nExample 2 假设我们需要从main.rs中调用routes/health_route.rs中的函数print_health_route。由于health_route.rs在文件夹routes下面，因此这个文件对main.rs来说是不可见的。如何解决呢？这个时候，我们就需要在routes文件夹下面添加一个名为mod.rs的文件。\n1 2 3 4 5 6 7 8 9 10 11 my_project ├── Cargo.toml └─┬ src ├── main.rs ├── config.rs ├─┬ routes + │ ├── mod.rs │ ├── health_route.rs │ └── user_route.rs └─┬ models └── user_model.rs mod.rs在Rust的模块系统中非常重要。还记的上面的config.rs吗？如果config模块的代码越来越多，需要分成若干个文件进行组织的时候，就需要创建config文件夹，然后在config/mod.rs中声明文件夹下的所有子模块了。可以这样认为：文件夹模块的config/mod.rs其实就相当于单文件模块的config.rs。\n在本例子中，也是一样的。想要在main.rs中调用print_health_route()，我们需要做如下三件事：\n创建routes/mod.rs，并且在main.rs中，使用mod routes;声明routes模块 在routes/mod.rs中声明health_route子模块，并且把它声明为public 在routes/health_route.rs中，把函数print_health_route()声明为public 完成的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // main.rs mod config; mod routes; fn main() { routes::health_route::print_health_route(); config::print_config(); println!(\"main\"); } // routes/mod.rs pub mod health_route; // routes/health_route.rs pub fn print_health_route() { println!(\"health_route\"); } 此时的模块树如下所示：\nExample 3 第三个例子中，我们会尝试如下的调用链路：main.rs =\u003e routes/user_route.rs =\u003e models/user_model.rs。\n首先和Example 2中的一样，我们把models/user_model.rs子模块构建好：\n然后，我们看调用链路的后半部分：从routes/user_route.rs中调用models/user_model/rs中的函数。对于同级的函数调用，我们可以从模块树的最上面逐级往下调用，即crate::models::user_model\n1 2 3 4 5 // routes/user_route.rs pub fn print_user_route() { crate::models::user_model::print_user_model(); println!(\"user_route\"); } 但是，在文件路径非常长的时候，每次从根模块往下找路径会非常冗长，因此Rust提供了super关键词来定位到父模块。\n如果我想想要从user_route.rs中调用health_route，那么可以使用super::health_route::print_health_route()代替crate::routes::health_route::print_health_route：\n1 2 3 4 5 6 7 pub fn print_user_route() { // crate::routes::health_route::print_health_route(); // can also be called using super::health_route::print_health_route(); println!(\"user_route\"); } Use关键字 在调用其他模块时，大部分时候不会每次都写完整的调用链路，而是会使用use关键字先导入某个模块。就上面的路子，可以改成如下的代码：\n1 2 3 4 5 6 7 8 // 导入 use crate::routes::health_route::print_health_route; pub fn print_user_route() { // 使用 print_health_route(); println!(\"user_route\"); } 三方包 在使用三方包时，需要在Cargo.toml中声明[dependencies]。声明之后的三方包模块会在工程所有的模块中自动生效。\n举例：如果我们在[dependencies]中使用了rand包，在任意一个文件中，我们可以直接使用该模块：\n1 2 3 4 5 6 pub fn print_health_route() { // 直接使用即可 let random_number: u8 = rand::random(); println!(\"{}\", random_number); println!(\"health_route\"); } 当然，我们也可以先use，然后再使用：\n1 2 3 4 5 6 7 use rand::random; pub fn print_health_route() { let random_number: u8 = random(); println!(\"{}\", random_number); println!(\"health_route\"); } 总结 Rust中的模块系统需要显式地去声明结构 在声明一个模块时，需要在它的父级声明，而不是在这个文件本身（上面的main.rs相当于是config.rs的父级，同样，mod.rs也是相当于是同级的health_score.rs的父级） mod关键字用于声明子模块 如果想要一个函数、结构体等被其他模块使用，必须使用pub关键字声明其为public 使用use关键字可以导入模块以减少重复代码 三方包不必显式地声明 ","description":"","tags":["Rust"],"title":"Rust's Module System","uri":"/posts/engineering/rusts-module-system/"},{"categories":null,"content":"Learning Rust - 4 Rust的代码组织 首先了解一些概念：\nPackage：可以理解为一个代码库，包含一个或多个crate，以及如何生成这些crate的信息（cargo.toml） Crate：编译单元，每个crate可编译成一个可执行文件或一个库（lib）。Crate中包含隐式的顶层Module Module：代码组织的单位，代码模块 Package和Crate 使用cargo new my-project生成的就是一个Package。在默认生成的工程中，有一个cargo.toml，这个就是Package的配置。此外，默认还有一个src/main.rs，这个就是当前Package的默认Crate。如果一个Package中包含src/main.rs和src/lib.rs，那么这个包里面就有两个Crate，一个是可执行文件的，一个是库的，他们的名称都和Package相同。\n如果你还想添加更多的可执行文件的Crate，那么就在src/bin目录下添加文件即可，每个文件都是一个单独的Binary Crate。\nModule Module是Rust中代码组织的逻辑单元。在一个模块中，可以声明很多东西：\n常量 类型别名 函数 结构 枚举 Traits impl 块 其他模块 在模块中，还能设置其成员的可见性，如果有pub修饰，那么就是模块外部可见的，否则不可见。下面是一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 mod authentication { pub struct User { username: String, password_hash: u64, } impl User { pub fn new(username: \u0026str, password: \u0026str) -\u003e User { User { username: username.to_string(), password_hash: hash_password(password), } } } fn hash_password(input: \u0026str) -\u003e u64 { /*...*/ } } fn main() { let user = authentication::User::new(\"jeremy\", \"super-secret\"); println!(\"The username is: {}\", user.username); println!(\"The password is: {}\", user.password_hash); } 模块authentication中有一个结构体User和一个函数has_password，然后User还定义了一个new函数。注意下面调用时，调用new函数使用的是authentication::User::new()，::符号就是用来获取module或struct的成员的。\n::和.的区别 当左边是一个module或者一个struct类型的时候，使用::，当左边是一个值的时候，使用.。\n添加三方Crate 如果想使用三方Crate，直接在cargo.toml里面添加dependency即可：\n1 2 [dependencies] regex = \"1.4.2\" 上面就是添加了一个名为regex的三方Crate，版本为1.4.2。\n在代码中，就可以使用use关键字来使用Crate中的结构、Module等：\n1 2 3 4 5 6 use regex::Regex; fn main() { let re = Regex::new(r\"^\\d{4}-\\d{2}-\\d{2}$\").unwrap(); println!(\"Did our date match? {}\", re.is_match(\"2014-01-01\")); } Test 单元测试 在Rust中，可以使用#[test]标注单元测试：\n1 2 3 4 5 6 7 8 9 10 fn add(a: i32, b: i32) -\u003e i32 { a + b } #[test] fn add_works() { assert_eq!(add(1, 2), 3); assert_eq!(add(10, 12), 22); assert_eq!(add(5, -2), 3); } 然后，使用cargo test可以运行所有的单元测试。\n如果我希望我的单元测试是失败的，或者测试某种产生panic的情况，可以使用#[should_panic]标注：\n1 2 3 4 5 #[test] #[should_panic] fn add_fails() { assert_eq!(add(2, 2), 7); } 如果希望跳过某些单元测试，则需要使用#[ignore]：\n1 2 3 4 5 #[test] #[ignore = \"not yet reviewed by the Q.A. team\"] fn add_negatives() { assert_eq!(add(-2, -2), -4) } 测试模块 一般情况下，需要把单元测试都写入测试模块，如果一个Module是测试模块，那么可以使用#[cfg(test)]标注：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 fn add(a: i32, b: i32) -\u003e i32 { a + b } #[cfg(test)] mod add_function_tests { use super::*; #[test] fn add_works() { assert_eq!(add(1, 2), 3); assert_eq!(add(10, 12), 22); assert_eq!(add(5, -2), 3); } #[test] #[should_panic] fn add_fails() { assert_eq!(add(2, 2), 7); } #[test] #[ignore] fn add_negatives() { assert_eq!(add(-2, -2), -4) } } 测试模块会在cargo test时编译并且运行\n文档测试 Rust中有一种注释叫做文档注释，以///标记。文档注释的所有内容会被写入Markdown中，并且文档注释中的Markdown代码块可以被编译和测试，这就是文档测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /// Generally, the first line is a brief summary describing the function. /// /// The next lines present detailed documentation. /// Code blocks start with triple backticks. The code has an implicit `fn main()` inside and `extern crate \u003ccratename\u003e`, /// which means you can just start writing code. /// /// ``` /// let result = basic_math::add(2, 3); /// assert_eq!(result, 5); /// ``` /// /// ```rust,should_panic /// panic!(\"panic\"); /// ``` pub fn add(a: i32, b: i32) -\u003e i32 { a + b } 使用cargo test同样会跑文档注释中的代码块\n集成测试 Rust还支持把Crate作为一个整体来测试，这些测试在单独的目录和文件中：tests/xxx.rs。\n","description":"","tags":["rust"],"title":"Learning Rust - 4","uri":"/posts/engineering/learning-rust-4/"},{"categories":null,"content":"Learning Rust - 3 泛型 泛型是非常常见的语言特性。使用泛型类型时，可以指定所需操作，而不必考虑定义类型持有的内部类型。在Rust中，使用尖括号\u003c\u003e来声明一个结构体的泛型类型：\n1 2 3 4 5 6 7 8 9 10 11 struct Point\u003cT\u003e { x: T, y: T, } fn main() { let boolean = Point { x: true, y: false }; let integer = Point { x: 1, y: 9 }; let float = Point { x: 1.7, y: 4.3 }; let string_slice = Point { x: \"high\", y: \"low\" }; } 上面代码中，T就是泛型的类型，在实例化的时候，T可以被任何确定的类型代替。在上面的定义中，实际上就是声明了x和y属于同一种数据类型，但是具体类型未指定。如果想把x和y声明为不同的类型，可以使用：\n1 2 3 4 struct Point\u003cT, U\u003e { x: T, y: U, } 特性 特性（trait）指一组可以被各种数据类型实现的通用接口，它定义了一组通用的行为。比如我们之前遇到的Copy特性，就是代表着一个类型是默认被复制而不是被移动的。如果你想要自己定义的类型默认被复制，那么只需要实现Copy特性即可。这个定义和golang中的接口非常相似，区别在于golang中的接口是隐式实现的，而在Rust中我们必须显式声明某个类型实现了某个特性。\n我们也可以使用trait关键字自己去定义特性：\n1 2 3 trait Area { fn area(\u0026self) -\u003e f64; } 上面的代码就定义了”有面积“的这么一个特性，中间定义的函数就代表着，如果一个类型有该特性，那么需要实现计算其面积的area方法。比如我们定义一个矩形的结构：\n1 2 3 4 struct Rectangle { width: f64, height: f64, } 我们就可以为Rectangle结构实现Area特性，来计算其面积：\n1 2 3 4 5 impl Area for Rectangle { fn area(\u0026self) -\u003e f64 { self.width * self.height } } 为实现某种类型的特性，我们使用关键字 impl Trait for Type，其中 Trait 是要实现的特征的名称，Type 是实现器结构或枚举的名称。\n实现了某种特性之后，我们就可以按照常规方法调用的方式来使用特性里面定义的方法：\n1 2 3 4 5 6 let rectangle = Rectangle { width: 10.0, height: 20.0, }; println!(\"Rectangle area: {}\", rectangle.area()); 派生 还有一种情况，比如我定义了下面的类型：\n1 2 3 4 struct Point { x: i32, y: i32, } Point中的所有成员都已经实现了Copy特性，但是Point本身没有实现Copy特性，所以默认情况下，Point还是会被移动。那么这种情况下如何快速实现Point的Copy特性呢？Rust提供了derive注解，可以为类型快速地自动生成新特性：\n1 2 3 4 5 #[derive(Copy)] struct Point { x: i32, y: i32, } 如果一个结构的每一个字段都已经实现了某个特性，那么使用#[derive(trait)]就能够自动地为某个结构实现该特性。\n使用特性 有了特性这么一个工具，我们就可以要求某些函数的参数实现特定的特性。换句话说，只要实现某特性的结构都可以作为该函数的参数。比如我们有如下特性，表明可以被转换为json字符串：\n1 2 3 trait AsJson { fn as_json(\u0026self) -\u003e String; } 然后我们就可以定义一个函数，这个函数可以把任意可以转换为json的类型打印出来：\n1 2 3 fn print_data_as_json(value: \u0026impl AsJson) { println!(\"{}\", value.as_json()); } 可以看到，我们声明某个特性为函数参数类型的时候，前面需要加impl，意思就是这里实际上是这个特性的具体实现者。\n当然，通过泛型我们也可以做到这一点：\n1 fn print_data_as_json\u003cT: AsJson\u003e(value: \u0026T) { ... } 这里就规定了一个泛型类型，这个泛型类型需要实现AsJson特性才能被接收。上面的两种实现是等价的。\n实例：实现一个迭代器 Iterator是一个Rust自带的特性，定义如下：\n1 2 3 4 trait Iterator { type Item; fn next(\u0026mut self) -\u003e Option\u003cSelf::Item\u003e; } 下面是一些解释：\nIterator 具有方法 next，调用时它将返回 Option\u003cItem\u003e。 只要有元素，next 方法就会返回 Some(Item)。 用尽所有元素后，它将返回 None 以指示迭代已完成。\n此定义使用一些新语法：type Item 和 Self::Item，它们使用此特征定义关联的类型。 此定义意味着 Iterator 特征的每一次实现还需要定义关联的 Item 类型，该类型用作 next 方法的返回类型。 换句话说，Item 类型将是从 for 循环块内的迭代器返回的类型。\n假设我们需要实现一个Counter，用作迭代器且进行计数，首先下面是struct的声明：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #[derive(Debug)] // 继承Rust自带的Debug特性 struct Counter { length: usize, count: usize, } impl Counter { // Counter有一个new函数，用于初始化一个新的Counter fn new(length: usize) -\u003e Counter { Counter { count: 0, length, } } } 然后，我们就实现Counter结构的Iterator特征。此处需要使用impl Trait for Struct格式：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 impl Iterator for Counter { type Item = usize; // 我们将通过 usize 进行计数，因此，我们声明相关 Item 类型应为该类型。 // next方法是实现迭代器的必须方法 fn next(\u0026mut self) -\u003e Option\u003cSelf::Item\u003e { self.count += 1; if self.count \u003c= self.length { Some(self.count) } else { None } } } 到这里我们就为Counter实现了Iterator特征。还记得我们可以对一个Iterator使用for item in Iterator的语法吗？这里我们对Counter也可以这样用了：\n1 2 3 4 5 fn main() { for number in Counter::new(10) { println!(\"{}\", number); } } ","description":"","tags":["rust"],"title":"Learning Rust - 3","uri":"/posts/engineering/learning-rust-3/"},{"categories":null,"content":"Learning Rust - 2 所有权和借用 所有权（ownership）系统是Rust最为与众不同的特性，它可以让Rust更好地管理内存、保障内存安全而不需要垃圾回收（GC）\n作用域 在Rust中，作用域使用大括号{}表示。一个作用域内定义的变量只在该作用域内有效：\n1 2 3 4 5 { let mascot = String::from(\"ferris\"); println!(\"{}\", mascot); // 有效 } println!(\"{}\", mascot); // 编译错误！mascot只在上面{}内的作用域有效 每当超出变量的范围时，变量及其对应的内存会被释放。\n所有权转移 在定义一个变量的时候，实际上是把一个数据绑定在一个变量名上。如let a = String::from(\"str\");，实际上就是把一个整数绑定在变量名a上面。因此，在Rust中，变量也被称为“绑定”。绑定有时候会更为贴切一点，因为Rust中的变量，默认是不可变的（还记得mut关键字吗）。\n对于一个绑定来说，其数据就是由被绑定名“拥有”的，这就是Rust中所有权的简单理解。如上面let a = String::from(\"str\");;的例子，就可以简单理解为变量名a拥有数据str。\n那么，看下面的语句：\n1 let b = a; 这个语句就是把a所拥有的数据1转移给了变量b，这就叫所有权的转移。之后，a就不再拥有数据1了，下面的代码将会报错：\n1 2 3 let a = String::from(\"str\"); // 把 str 绑定给 a let b = a; // 把 a 拥有的数据转移给了 b println(\"{}\", a); // 编译错误！a的值已经被移走了 类似的事情也发生在调用函数时：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 fn process(input: String) {} fn caller() { let s = String::from(\"Hello, world!\"); process(s); // Ownership of the string in `s` moved into `process` process(s); // Error! ownership already moved. } // 报错信息 error[E0382]: use of moved value: `s` --\u003e src/main.rs:6:13 | 4 | let s = String::from(\"Hello, world!\"); | - move occurs because `s` has type `String`, which does not implement the `Copy` trait 5 | process(s); // Transfers ownership of `s` to `process` | - value moved here 6 | process(s); // Error! ownership already transferred. | ^ value used here after move 第二个process(s)将会报错。\n在 Rust 中，所有权转移（即移动）是默认行为。此模式对编写 Rust 代码的方式有着深远的影响。 它是 Rust 提出的内存安全承诺的核心。\n复制 那么，如果我想复制一个值而不是移动它呢？\n你可能已经注意到上面的报错信息，有提到一个叫Copy的特性（trait）。特性可以大概理解为golang里面的接口interface，只要实现了该特性那么就能在相应的地方使用。在这里，如果一个对象实现了Copy特性，那么它就会被复制而不是被移动。例子就是u32类型，u32类型默认已经实现了Copy的特性，因此，下面的代码不会报错：\n1 2 3 4 5 6 7 fn process(input: u32) {} fn caller() { let s = 1u32; process(s); // Ownership of the number in `n` copied into `process` process(s); // `n` can be used again because it wasn't moved, it was copied. } 显式克隆 还有一种解决方案是，显式地复制一份数据，这样复制出来的数据被移动，旧的值还在。这个时候调用.clone()函数即可：\n1 2 3 4 5 6 7 fn process(s: String) {} fn main() { let s = String::from(\"Hello, world!\"); process(s.clone()); // Passing another value, cloned from `s`. process(s); // s was never moved and so it can still be used. } 但是，每次显式克隆都需要完整复制一份数据，速度会很慢，而且浪费内存\n有没有更优雅的办法呢？ 答案肯定是有的，这就涉及到Rust中另外一个重要的概念：借用（borrow）\n借用 借用的意思也很直接：所有权还是你的，我就借过来用一下。在Rust中，借用是通过引用提供的：\n1 2 3 let greeting = String::from(\"hello\"); let greeting_reference = \u0026greeting; // We borrow `greeting` but the string data is still owned by `greeting` println!(\"Greeting: {}\", greeting); // We can still use `greeting` 和C++的引用一样，前面加个\u0026即表示是该变量的引用。\n在上面的代码中，我们使用 \u0026 借用了 greeting。 greeting_reference 的类型为 \u0026String。 由于我们只借用了 greeting，并没有移动所有权，因此，在我们创建 greeting_reference 之后仍然可以使用 greeting。\n既然是借用，那么我们控制使用者如何使用：\n如果我们不想使用者修改值，那么就传一个不可变引用，即\u0026T 如果允许使用者修改，那么传一个可变引用，即\u0026mut T 对于一个变量来说，只能有一种类型的引用（可变或不可变）。对于不可变引用来说，多少都行；但是变量的只能有一个可变引用。\n如何理解呢？假设我把变量借用出去，如果我不允许使用者修改，那么我借给多少人都无所谓，反正不会变；如果我允许使用者修改，那么在同一时刻我只能借给一个人，否则一个变量可能会被多个使用者在不同地方同时修改，会出问题。\n这也是其他语言经常会产生bug的地方：某个变量在意想不到的地方被修改了，但是我不知道，还按照没有被修改的情况使用。Rust在很大程度上可以避免这个问题：不想让你改，那么就使用不可变引用。\n生命周期 使用引用还会出现另外一个问题：引用的项如果被释放了，那么我这个引用就指向了一块无效的内存，这就是悬垂指针。\n1 2 3 4 5 6 7 8 fn main() { let x; { let y = 42; x = \u0026y; // We store a reference to `y` in `x` but `y` is about to be dropped. } println!(\"x: {}\", x); // `x` refers to `y` but `y has been dropped! } C/C++经常会遇到这种问题，但是Rust通过生命周期保证了所有引用都始终引用有效的项。上面的代码编译会报错，因为y在其作用域结束时已经被释放。用'a和'b分别表示x和y的生命周期，可以看到x活得更久，生命周期更长：\n1 2 3 4 5 6 7 8 fn main() { let x; // -------------------+--'a：x的生命周期 { // | let y = 42; // -+-- 'b：y的生命周期 | x = \u0026y; // | | } // -+ | println!(\"x: {}\", x); // | } 函数中的生命周期 大部分时候生命周期是隐含并可以推断的，但是有时候编译器推断不了，比如下面的这个函数：\n1 2 3 4 5 6 7 8 // 无法编译通过 fn longest_word(x: \u0026String, y: \u0026String) -\u003e \u0026String { if x.len() \u003e y.len() { x } else { y } } 编译器不知道返回值的生命周期，我们也不能确定，因为根据输入的值，x、y均有可能。对于这种函数，我们需要添加生命周期注解，来显式地声明返回值和函数参数的生命周期之间的关系：\n1 2 3 fn longest_word\u003c'a\u003e(x: \u0026'a String, y: \u0026'a String) -\u003e \u0026'a String { ... } 其中，'a就是生命周期注解，其名称无所谓，'b，'xyz都行，只需要保证函数参数和返回值的生命周期注解一样即可。函数后面\u003c'a\u003e的意思是该函数存在生命周期注解，名称为'a。后面带'a的参数说明这些参数的生命周期和返回值的生命周期相关。\n上面代码的意思就是告诉编译器，longest_word这个函数的返回值的生命周期，和x、y这两个参数的生命周期有关。这样，Rust编译器在编译的时候，就会自动取有注解的参数的生命周期中较短的那个作为返回值的生命周期。这样，就可以检查出下面的错误：\n1 2 3 4 5 6 7 8 9 10 // 编译不通过 fn main() { let magic1 = String::from(\"a!\"); let result; { let magic2 = String::from(\"shazam!\"); result = longest_word(\u0026magic1, \u0026magic2); // result可能是 magic2 } // magic2被释放 println!(\"The longest magic word is {}\", result); // 避免了此处可能存在的悬垂指针 } 结构或枚举中的生命周期 如果一个struct或一个枚举包含引用类型的字段，那么它也需要像函数一样显式地标记生命周期：\n1 2 struct Highlight(\u0026str); // 错误 struct Highlight\u003c'a\u003e(\u0026'a str); // 正确 这里，'a可以认为是一个提醒，提醒 Highlight 结构的生存期不能超过它借用的 \u0026str 的生存期。下面的代码可以解释一下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #[derive(Debug)] struct Highlight\u003c'document\u003e(\u0026'document str); fn erase(_: String) { } fn main() { let text = String::from(\"The quick brown fox jumps over the lazy dog.\"); let fox = Highlight(\u0026text[4..19]); let dog = Highlight(\u0026text[35..43]); let moved_text = text; // 编译不通过，不能把 text 移走，因为 Highlight类型的对象（fox,dog）还在其生命周期内 println!(\"{:?}\", fox); println!(\"{:?}\", dog); } ","description":"","tags":["rust"],"title":"Learning Rust - 2","uri":"/posts/engineering/learning-rust-2/"},{"categories":null,"content":"Learning Rust - 1 学习微软Rust教程的笔记。\nRust的特点 Rust 是现有系统软件语言（如 C 和 C++）的一种安全替代语言。 与 C 和 C++ 一样，Rust 没有大型运行时或垃圾回收器，这几乎与所有其他现代语言形成了鲜明对比。 但是，与 C 和 C++ 不同的是，Rust 保证了内存安全。 Rust 可以避免很多与在 C 和 C++ 中遇到的内存使用错误相关的 bug。\nRust 有以下优点，非常适合各种应用程序：\n类型安全：编译器可确保不会将任何操作应用于错误类型的变量。 内存安全：Rust 指针（称为“引用”）始终引用有效的内存。 无数据争用：Rust 的 borrow 检查器通过确保程序的多个部分不能同时更改同一值来保证线程安全。 零成本抽象：Rust 允许使用高级别概念，例如迭代、接口和函数编程，将性能成本控制在最低，甚至不会产生成本。 这些抽象的性能与手工编写的底层代码一样出色。 最小运行时：Rust 具有非常小的可选运行时。 为了有效地管理内存，此语言也不具有垃圾回收器。 在这一点上，Rust 非常类似于 C 和 C++ 之类的语言。 面向裸机：Rust 可以用于嵌入式和“裸机”编程，因此适合用于编写操作系统内核或设备驱动程序。 了解Cargo Cargo是Rust语言的生成工具和依赖管理器，Cargo为管理Rust程序带来了很多便利。\nCargo 可以为你做许多事情，包括：\n使用 cargo new 命令创建新的项目模板。 使用 cargo build 编译项目。 使用 cargo run 命令编译并运行项目。 使用 cargo test 命令测试项目。 使用 cargo check 命令检查项目类型。 使用 cargo doc 命令编译项目的文档。 使用 cargo publish 命令将库发布到 crates.io。 尝试Cargo 首先使用Cargo创建一个新的Rust工程：\n1 cargo init \u003cproject-name\u003e 创建完工程之后，使用VSCode打开刚才创建的文件夹，可以看到cargo已经自动为你生成了cargo.toml和src/main.rs两个文件。\nCargo.toml 是 Rust 代码库的配置文件，用于管理依赖版本等。如果你有其他语言的经验，可以类比package.json或者go.mod等 src 子目录中的 main.rs 文件为当前工程的主入口文件，里面的fn main()即为主入口函数 cargo init生成的工程是一个可运行的Rust的HelloWorld工程。下面还有一些命令可以尝试：\n运行当前工程：\n1 cargo run 编译当前工程：\n1 cargo build 编译当前工程（发布使用）：\n1 cargo build --release 编译完成之后，你可以在target/debug和target/release目录下看到编译出的可执行文件。\n语言基础 变量 声明变量 使用关键字let\n1 2 3 4 5 6 7 8 9 10 11 // Declare a variable let a_number; // Declare a second variable and bind the value let a_word = \"Ten\"; // Bind a value to the first variable a_number = 10; println!(\"The number is {}.\", a_number); println!(\"The word is {}.\", a_word); 变量的可变性 和一般的语言不一样，默认情况下，Rust的变量是不可变的：\n1 2 let a_number = 10; a_number = 15; // 此行报错！ 如果想要变量可变，那么需要使用关键字mut:\n1 2 let mut a_number = 10; a_number = 15; // 可以 为什么要默认变量是不可变的呢？《The Rust Programming Language》有如下解释：\n如果一部分代码假设一个值永远也不会改变，而另一部分代码改变了这个值，第一部分代码就有可能以不可预料的方式运行。不得不承认这种 bug 的起因难以跟踪，尤其是第二部分代码只是 有时 会改变值。\nRust 编译器保证，如果声明一个值不会变，它就真的不会变。这意味着当阅读和编写代码时，不需要追踪一个值如何和在哪可能会被改变，从而使得代码易于推导。\n不过可变性也是非常有用的。变量只是默认不可变；正如在第二章所做的那样，你可以在变量名之前加 mut 来使其可变。除了允许改变值之外，mut 向读者表明了其他代码将会改变这个变量值的意图。\n-- 变量与可变性 - Rust 程序设计语言\n不可变变量和常量的区别： 变量与可变性 - Rust 程序设计语言\n总结一下：\n不允许对常量使用 mut。常量不光默认不能变，它总是不能变。 声明常量使用 const 关键字而不是 let，并且 必须 注明值的类型 常量只能被设置为常量表达式，而不能是函数调用的结果，或任何其他只能在运行时计算出的值 常量可以在任何作用域中声明，包括全局作用域 变量隐藏 可以重复使用let声明同名的变量，这样的话变量名会被绑定在新的值上面，旧的变量就被“隐藏”了。需要注意的是，旧变量仍然存在。\n1 let a_number = 10;let a_number = 15; // 隐藏上面的变量，但是上面的变量不会被删除，仍存在于内存中println!(\"{}\", a_number); // 15 数据类型 Rust是静态类型语言。在声明变量时，编译器会自动推断变量类型，但是也可以使用:（类似typescript）手动指定变量类型：\n1 let number1 = 15; // 默认是i32类型let number2: i64 = 15; // 手动声明i64整型 数字类型 分为整数、浮点数。具体表见：数据类型 - Rust 程序设计语言\n默认的整型和浮点数类型为：i32和f64\n布尔类型 true or false\n字符 字符类型为char，使用单引号括住：\n1 let s = 's';let emoji = '😃'; 字符串 Rust中，有好几种字符串类型：\u0026str（字符串引用）, String（堆上字符串）等。具体使用还是有区别的。可以参考：字符串 - Rust 程序设计语言。现在，可以先简单地认为 String 是可随程序运行而更改的文本数据。 \u0026str 引用是文本数据的不可变视图，不会随着程序运行而改变。\n元组tuple 元组是固定长度的分组，使用(\u003cvalue1\u003e, \u003cvalue2\u003e, ...)表示。每个value的类型可以不一样。获取元组中的元素，可使用tuple.index\n1 // Declare a tuple of three elementslet tuple_e: (char, i32, bool) = ('E', 5i32, true);// Use tuple indexing and show the values of the elements in the tupleprintln!(\"Is '{}' the {}th letter of the alphabet? {}\", tuple_e.0, tuple_e.1, tuple_e.2); 控制流 if else 语法很简单：\n1 if condition { } else if another_condition { } else { } 用的时候，ifelse块还可以充当表达式：\n1 let formal = true;let greeting = if formal { // if used here as an expression \"Good day to you.\" // 注意，此处没有分号结尾，即 \"Good day to you.\" 为这个if块的返回值} else { \"Hey!\" // 返回 “Hey!\"};println!(\"{}\", greeting) // prints \"Good day to you.\" 复杂数据结构 结构体 使用关键字struct定义，结构类型名称采用大写形式。\nRust 支持三种结构类型：经典结构、元组结构和单元结构。 这些结构类型支持使用各种方式对数据进行分组和处理。\n“经典 C 结构”最为常用。 结构中的每个字段都具有名称和数据类型。 定义经典结构后，可以使用语法 \u003cstruct\u003e.\u003cfield\u003e 访问结构中的字段。 元组结构类似于经典结构，但字段没有名称。 要访问元组结构中的字段，请使用索引元组时所用的语法：\u003ctuple\u003e.\u003cindex\u003e。 与元组一样，元组结构中的索引值从 0 开始。 “单元结构”最常用作标记。 我们将在了解 Rust 的特征功能时，将深入了解单元结构之所以实用的原因。 以下代码显示三种结构类型变体的示例定义：\n1 2 3 4 5 6 7 8 // Classic struct with named fields struct Student { name: String, level: u8, pass: bool } // Tuple struct with data types only struct Grades(char, char, char, char, f32); // Unit struct struct Unit; 结构体实例化：\n1 2 3 4 5 6 7 // Instantiate classic struct, specify fields in random order, or in specified order let user_1 = Student { name: String::from(\"Constance Sharma\"), remote: true, level: 2 }; let user_2 = Student { name: String::from(\"Dyson Tan\"), level: 5, remote: false }; // Instantiate tuple structs, pass values in same order as types defined let mark_1 = Grades('A', 'A', 'B', 'A', 3.75); let mark_2 = Grades('B', 'A', 'A', 'C', 3.25); 当然，我们也可以为结构体定义成员函数，使用impl关键字即可\n1 2 3 4 5 6 7 8 9 10 11 12 // Classic struct with named fields struct Student { name: String, level: u8, pass: bool } impl Student { fn get_name(\u0026self) -\u003e \u0026String { return \u0026self.name; } } fn main() { let s = Student{name: \"n\".to_string(), level:1, pass:true}; println!(\"{}\", s.get_name()); } 枚举 关键字enum。需要注意的是，Rust的枚举中，每个值可以有不同的类型。这样的话，在使用某个枚举类型时，必须接受其下面所有值的类型：\n1 2 3 4 5 6 7 8 enum WebEvent { // An enum variant can be like a unit struct without fields or data types WELoad, // An enum variant can be like a tuple struct with data types but no named fields WEKeys(String, char), // An enum variant can be like a classic struct with named fields and their data types WEClick { x: i64, y: i64 } } 一般来说，我们不直接在枚举里面定义一个复杂的结构，而是在外面定义好相应的结构体之后，在枚举里面使用：\n1 2 3 4 5 6 7 8 9 // Define a tuple struct struct KeyPress(String, char); // Define a classic struct struct MouseClick { x: i64, y: i64 } // Redefine the enum variants to use the data from the new structs // Update the page Load variant to have the boolean type enum WebEvent { WELoad(bool), WEClick(MouseClick), WEKeys(KeyPress) } 在使用枚举时，采用运算符::来指定具体的枚举值：\n1 2 3 4 5 6 7 // bool let we_load = WebEvent::WELoad(true); // Instantiate a MouseClick struct and bind the coordinate values let click = MouseClick { x: 100, y: 250 }; // Set the WEClick variant to use the data in the click struct let we_click = WebEvent::WEClick(click); Rust函数 Rust的函数使用关键字fn声明：\n1 2 3 fn main() { println!(\"Hello, world!\"); } 函数的返回值由-\u003e确定，参数填在()里面，使用:指定类型：\n1 2 3 fn is_divisible_by(dividend: u32, divisor: u32) -\u003e bool { ... } 在函数体中，大多数的语句是分号;结尾的。如果不是分号结尾的语句，则有可能是函数的返回值！\n1 2 3 4 5 6 7 8 fn getFive() -\u003e i32 { 5 } // 等同于 fn getFive() -\u003e i32 { return 5; } ###集合类型\nRust中自带了一些常见的集合类型：数组、向量、HashMap等\n数组 Rust中的数组是具有相同数据类型和固定长度的对象集合。定义和索引：\n1 2 3 4 5 6 7 // Declare array, initialize all values, compiler infers length = 7 let days = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]; // Declare array, first value = \"0\", length = 5 let bytes = [0; 5]; // Set first day of week let first = days[0]; 向量 Rust中的向量是长度可变的相同数据类型的对象集合。向量声明：\n1 2 3 4 // Declare vector, first value = \"0\", length = 5 let zeroes = vec![0; 5]; // Create empty vector, declare vector mutable so it can grow and shrink let mut fruit = Vec::new(); 注意，代码中的vec!是一个宏，而Vec::new()为调用Vec中的new()方法\n索引、添加和删除值：\n1 2 3 4 5 6 fruit.push(\"Apple\"); fruit.push(\"Banana\"); fruit.push(\"Cherry\"); let cherry = fruit.pop(); let apple = fruit[0]; let banana = fruit[-1]; HashMap Rust中的HashMap定义在标准库中，因此在使用前需要使用\n1 use std::collections::HashMap; 引入。use关键字和其他语言中的import类似，用于导入。\n初始化，添加、获取、删除元素：\n1 2 3 4 5 6 7 8 9 let mut reviews: HashMap\u003cString, String\u003e = HashMap::new(); reviews.insert(\"Ancient Roman History\".to_string(), \"Very accurate.\".to_string()); reviews.insert(\"Programming in Rust\".to_string(), \"Great examples.\".to_string()); let key: \u0026str = \"Programming in Rust\"; let v = reviews.get(key); reviews.remove(key); 循环 Rust中，提供了三种循环：loop, while, for\nloop Rust中的loop为无限循环，只能使用break跳出。使用break时，还能顺带返回一个值：\n1 2 3 4 5 6 7 8 9 10 11 let mut counter = 1; // stop_loop is set when loop stops let stop_loop = loop { counter *= 2; if counter \u003e 100 { // Stop loop, return counter value break counter; } }; // Loop should break when counter = 128 println!(\"Break the loop at counter = {}.\", stop_loop); 如果loop中有多个break，那么每处返回的类型需要一致。\nwhile 和其他语言的while没什么区别：\n1 2 3 while condition { ... } for 对于数组之类的数据结构，可以用for \u003cvalue\u003e in \u003clist\u003e\n1 2 3 4 5 6 7 8 let big_birds = [\"ostrich\", \"peacock\", \"stork\"]; for bird in big_birds { ... } // 也可使用iter() for bird in big_birds.iter() { ... } 另外一种常见的用法是for idx in a...b，其中，a...b表示从a开始，步长为1迭代到b（不包含b)：\n1 2 3 for number in 0..5 { // 0, 1, 2, 3, 4 } 错误处理 panic panic 是 Rust 中最简单的错误处理机制。发生panic时，Rust会输出一条错误消息、清理资源，然后退出程序。可以调用panic!宏来使当前进程panic。一般来说，只有在程序遇到无论如何都恢复不了的错误时使用：\n1 2 3 fn main() { panic!(\"Farewell!\"); } Option 在Rust中，使用Option\u003cT\u003e处理可能为空的值。其他语言中，会有null, nil, None之类的值表示空值，在Rust中，除了与其他语言（比如C）交互时，其他情况下基本都不会使用null。\nOption\u003cT\u003e是一个带泛型的枚举：\n1 2 3 4 enum Option\u003cT\u003e { None, // The value doesn't exist Some(T), // The value exists } 那什么时候会用到Option呢？下面就是一个例子：\n在前面的单元中，我们提到尝试访问矢量的不存在的索引会导致程序 panic，但你可以通过使用 Vec::get 方法（该方法返回 Option 类型，而不是 panic）来避免这种情况。 如果该值存在于指定的索引处，系统会将其包装在 Option::Some(value) 变体中。 如果索引超出界限，则它会改为返回 Option::None 值。\n1 2 3 4 5 6 7 8 9 let fruits = vec![\"banana\", \"apple\", \"coconut\", \"orange\", \"strawberry\"]; // pick the first item: let first = fruits.get(0); println!(\"{:?}\", first); // Some(\"banana\") // pick the 99th item, which is non-existent: let non_existent = fruits.get(99); println!(\"{:?}\", non_existent); // None Rust提供了多种方法来处理Option值：\nmatch：类似其他语言的switch，针对Option中的每种情况分别处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 match Option\u003cT\u003e { Some(value) =\u003e { ... } None =\u003e {... } } // 下面是一个实例 let fruits = vec![\"banana\", \"apple\", \"coconut\", \"orange\", \"strawberry\"]; for \u0026index in [0, 2, 99].iter() { // fruits.get(index)返回一个Option\u003cString\u003e match fruits.get(index) { Some(\u0026\"coconut\") =\u003e println!(\"Coconuts are awesome!!!\"), Some(fruit_name) =\u003e println!(\"It's a delicious {}!\", fruit_name), None =\u003e println!(\"There is no fruit! :(\"), } } if let：如果只关心Option中的某一个特定值\n1 2 3 4 5 6 let a_number: Option\u003cu8\u003e = Some(7); // 如果我只关心这个数字为7的情况，此时适合使用if let if let Some(7) = a_number { println!(\"That's my lucky number!\"); } unwrap()/expect()：直接获取Option中的Some值，但是Option为None，会直接panic。区别在于expect()可以自定义panic的报错信息\n1 2 3 4 5 6 let a: i32 = Some(1).unwrap(); let empty_gift: Option\u003c\u0026str\u003e = None; empty_gift.unwrap(); // panic! empty_gift.expect(\"the gift is none!\"); // panic with given message! // thread 'main' panicked at 'the gift is none!' unwrap_or(\u003cdefault_value\u003e)：如果Option为None，则使用默认值\nResult Rust的Option提供了对空值的处理，而对于可能出现的程序的错误，Rust提供了Result\u003cT, E\u003e枚举来处理：\n1 2 3 4 enum Result\u003cT, E\u003e { Ok(T): // A value T was obtained. Err(E): // An error of type E was encountered instead. } Result枚举也非常好理解：要么程序运行OK，返回一个T类型的值；要么程序运行Err，返回一个E类型的错误。和Option类似，Result也提供了unwrap()和expect()方法直接获取OK()包的值。如果返回的是Err，则会panic。\n能够用于Option的match和if let，也可以用于Result\n","description":"","tags":["rust"],"title":"Learning Rust - 1","uri":"/posts/engineering/learning-rust-1/"},{"categories":null,"content":"XLNet Introduction 现在主流的预训练模型可以被分为两类：autoregressive(AR)和autoencoding(AE)\nAR：recurrent model\n单向模型\nAE：reconstruct the original data\ngap between pretraining and finetuning\n假设：被mask的词无关\nXLNet尝试使用一个新的预训练任务来解决AR和AE预训练模型现存的一些问题。\nContribution：\n预训练任务Permutation：不使用前向/后向的顺序，而是使用打乱后的单词顺序 使用Transformer-XL里面的segment recurrence mechainism和relative position encoding 调整Transformer-XL使得其结构适用于permutation-based LM Objective: Permutation Language Modeling 如果序列$\\textbf{x} = [x_1, ...,x_T]$, 那么一共会有T!种排列方式，记为 $Z_T$ 。那么， $z_t$ 和 $\\textbf{z}_{\u003ct}$ 就是permutation$\\textbf{z}\\in Z_t$中的第t个元素，和前t-1个元素（在原始序列的的位置）\n那么，建模的目标就可以设置为：\n其中，$\\theta$是所有的permutation所共享的\nRemark 训练的目标是把输入序列的顺序打乱了的，因此我们需要记住原始序列的顺序：positional encoding只使用原始序列中的位置\n另外，在finetuning的时候，只使用原始序列\n?是否仍然存在pretraining-finetuning gap?\n对Transformer的修改 由于XLNet是permutation language modeling，因此不能使用传统的softmax作为输出层：\n其中$h_\\theta$是X_z\u003ct的隐藏状态。这样的话，会导致对于相同前缀的permutation，其下一个token的概率永远是一样的：\n[1,2,3 | 4,5] vs [1,2,3 | 5,4]\n因此需要加入位置信息$z_t$，上式变为：\n这里就是所谓的Two-Stream Self-Attention的来源：同时考虑$g_\\theta$和$h_\\theta$\nTwo-Stream Self-Attention 这里带来了一个问题：预测$x_{z_t}$，就只能考虑位置信息$z_t$；预测$x_{z_j}, j \u003e t$，那么就可以考虑整个t位置上面的context信息$x_{z_t}$\n所以，XLNet提出同时维护两套隐状态：$h_\\theta(x_{z_{\u003c=t}})$和$g_\\theta(x_{z_{\u003ct}}, z_t)$，后面使用$h_{z_t}$和$g_{z_t}$表示，即content stream和query stream。\n对于第一层，g0被设为一个可训练的向量，h0就直接使用第一个词的embedding。在训练的时候，两个stream共同使用一套参数$\\theta$\n整体模型的结构：\nPartial Prediction 由于实际训练时，permutation LM收敛很慢，因此在每一个permutation序列中只预测一部分词\nIncorporating Ideas from Transformer-XL 从Transformer-XL中借鉴了两个非常重要的机制：\nrelative positional encoding\n用来记录原始序列中的顺序\nsegment recurrence mechanism\nModeling Multiple Segments 对于多segment的训练，XLNet会随机采样两个segment，然后把它们接起来 - [CLS, A, SEP, B, SEP]，作为PLM中的一个序列来训练\nRelative Segment Encoding 只考虑两个位置是否在同一个segment中，记为s_ij\n对于两个位置i,j，Attention weight $a_{ij}=(q_i+b)^Ts_{ij}$ ，其中q_i是Transformer的query，b是bias\nExperiment Setup tokenizer: sentencepiece\nPretraining sequence length: 512\n512TPU v3, 500K steps\noptimizer: Adam weight decay optimizer\nBatch size: 8192\npretraininig cost 5.5 days\nPerformance 好于BERT/RoBERTa\n","description":"","tags":["xlnet","paper note"],"title":"XLNet","uri":"/posts/deep-learning/xlnet/"},{"categories":null,"content":"Introduction Main contributions:\nIntroduce GPT-C, a multi-layer generative transformer model, which is a variant of the GPT-2 Propose and deploy a novel end-to-end code sequence completion system, and an efficient client-side caching system Introduce MultiGPT-C – a multilingual version of GPT-C Dataset 1.2 billion lines of source code in Python, C#, JS and TS;\n4.7 million files\nApproach Model definition\nwhere $W_e$ is the input token embedding matrix, $C$ is the context vector of tokens, $W_p$ is the position embedding matrix.\nThe input token embedding matrix is reused in output layout, which removes large fully connected layout and reduces the number of parameters by 25%\nPreprocessing AST/CST/CFG is not used in this paper.\nThe input code is normalized using a custom tokenizer to overcome the style issues. Also, the token type is extracted in both training and inference procedure to help the normalization, extract subtoken vocabulary and encode the sequences.\nVocabulary Two tokenization methods:\nBPE casing convention, including camelCase, PascalCase and snake_case Several special tokens are added to vocabulary: \u003cBOF\u003e, \u003cEOF\u003e, \u003cEOL\u003e, \u003cINDENT\u003e, \u003cDEDENT\u003e\nSensitive Data Processing To avoid the leaking of sensitive data, IntelliCode Compose uses \u003cNUM_LIT\u003e, \u003cSTR_LIT\u003e and \u003cCOMMENT\u003e to replace numeric literals, string literals and comments.\nNote that not all literals are replaced, there are some literals in each language are kept. For example, \u003cSTR_LIT:__main__\u003e means this token is a string literal \"__main__\".\nModel Training In training, IntelliCode Compose uses a decreasing learning rate with several warm-up perioids. The training is scaled up using synchronous data-parallel distributed training algorithm with local gradient accumulation.\nThe authors use 5 Lambda V100 boxes, each having sixteen V100 GPUs with 32 GB HBM2 memory, eight 100 GB InfiniBand, and one 100 GB Ethernet connection, managed with Kubernetes\nThe best model has 24 layers, 16 heads and the bpe vocabulary size is 50000.\nThe hyper parameters are set as the following table:\nIn training, base learning rate is set to 6.25*10^-5, batch size 128, learning rate decay of 0.98 per epoch.\nSequence Decoding Beam search, ending with \u003cEOL\u003e or other language specific tokens\nBatched inference is performed, the batch size is set to beam size k, which reduces L*k inference calls to L, where L is the sequence length.\nClient-Side Post-Processing Caching The suggestion results are cached as a trie with score. The key is the preceding context. If the user continues typing, the trie is pruned and then the suggestion is calculated by traversing the trie using a greedy search.\nThe trie traversing will be terminated early if none of the child nodes has a score that is larger than the score of its parent multiplied by a ratio 𝑅:\nWhere L is the position in sequence. $\\alpha$ And $k$ are two parameters, $\\alpha$ controls the overall average length, and $k$ controls the speed than R decreases. In the practice, the authors select 𝛼 = 0.8 and 𝜅 = 10 to gain a balance between suggestion length and relevance.\nSuggestion Processing The special tokens are removed or replaced case by case.\nMulti-Lingual Model The authors tests four approaches:\nLanguage-agnostic baseline\nLanguage-type embedding\nLanguage control code\nDouble heads multilingual model\nAdd a programming language classification task during model pretraining\nEvaluation ","description":"","tags":["paper-note","code-completion"],"title":"IntelliCode Compose","uri":"/posts/code-intelligence/intellicode-compose/"},{"categories":null,"content":"Dockerfile A brief introduction to docker 最简单的理解，其实就是在你机器上面跑的一个轻量化的虚拟机。和虚拟机不同的是，docker可以更高效地利用系统资源、有着更快速的启动时间。同时，还可以给开发者提供一致的运行环境，实现一次创建和配置，在任意地方正常运行，有利于持续交付和部署。\n基础概念 Docker包括三个基础概念：镜像、容器和仓库\n镜像\n我们都知道，操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统。比如官方镜像 ubuntu:18.04 就包含了完整的一套 Ubuntu 18.04 最小系统的 root 文件系统。\n容器\n镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。\n仓库\n镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。\n一个 Docker Registry 中可以包含多个 仓库（Repository）；每个仓库可以包含多个 标签（Tag）；每个标签对应一个镜像。\n使用镜像 在Docker Hub中，有很多官方维护的高质量镜像，如果我们想要获取某个镜像，可以直接使用docker pull，如我想要拉取ubuntu的官方镜像，那么直接使用：\n1 docker pull ubuntu:18.04 Dockerfile What's dockerfile 如果我们想要定制一个镜像给其他人去使用，那么就需要把所有的修改、配置等都写入一个脚本中，然后用这个脚本来构建我们的定制镜像。这个脚本就是Dockerfile。\nDocker的镜像有一个分层的机制，就是说，在Dockerfile里面的每一条命令是一层。在构建镜像的时候，是一层一层地构建的，前一层是后一层的基础，每一层构建完毕之后就不会再改变，后一层的任何修改只会作用于当前这一层。\n用一个例子来辅助理解：如果我在上一层添加了一个文件，然后在下一层删除，那么最终在容器运行的时候，我们是看不到这个被删除的文件的，但是实际上这个文件是一直存在在镜像里面的某一层的，只是在下一层中，这个文件被标记为已删除。\n因此，在构建镜像的时候，每一层尽量只包含该层需要添加的东西，任何其他额外的东西应该在该层构建结束之前清理掉。\n构建镜像 在Dockerfile所在的目录执行：\n1 2 3 4 docker build [选项] [上下文路径] # 例子： docker build -t nginx:v3 . 构建上下文Context 可以看到docker build命令最后有一个.，这个表示当前构建的上下文是当前目录。为什么需要指定上下文目录呢？首先需要了解docker build的工作原理\nDocker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。\n所以，当需要用到本地的一些文件的时候，就需要把这些文件添加到上下文目录中，在执行docker build的时候，上下文目录下的文件都会被打包，然后发给docker引擎。\n因此，在Dockerfile里面，如果有以下一条命令：\n1 COPY ./package.json /app/ 其COPY的文件实际上是上下文目录下面的package.json，而非执行docker build的目录，或者Dockerfile所在目录。\n常用Dockerfile命令 FROM 所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。FROM命令就是用来指定基础镜像的。因此，一个Dockerfile中，FROM是必备命令，并且必须是第一条命令。\n在DockerHub上有很多高质量官方镜像，有可以直接使用的服务类镜像，如nginx、redis等，也有方便开发构建运行各种语言的镜像，如openjdk、python等，当然也有基础的操作系统镜像，如ubuntu、centos等。我们可以在其中选择一个最符合我们最终目标的基础镜像开始定制。\nRUN RUN指令是用来执行命令行命令的，有两种形式：\nshell格式：RUN \u003c命令\u003e exec格式：RUN [\"可执行文件\", \"参数1\", \"参数2\"] 需要注意的是，在Dockerfile中最好不要每一条命令都使用一个RUN，而是把一个目的的命令结合到一起，使用分行符\\和连接符\u0026\u0026连到一起：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 错误用法 FROM debian:stretch RUN apt-get update RUN apt-get install -y gcc libc6-dev make wget RUN wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" RUN mkdir -p /usr/src/redis RUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 RUN make -C /usr/src/redis RUN make -C /usr/src/redis install # 正确用法 FROM debian:stretch RUN set -x; buildDeps='gcc libc6-dev make wget' \\ \u0026\u0026 apt-get update \\ \u0026\u0026 apt-get install -y $buildDeps \\ \u0026\u0026 wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ \u0026\u0026 mkdir -p /usr/src/redis \\ \u0026\u0026 tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ \u0026\u0026 make -C /usr/src/redis \\ \u0026\u0026 make -C /usr/src/redis install \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* \\ \u0026\u0026 rm redis.tar.gz \\ \u0026\u0026 rm -r /usr/src/redis \\ \u0026\u0026 apt-get purge -y --auto-remove $buildDeps COPY COPY是把上下文目录下的文件复制到新的一层镜像内的\u003c目标路径\u003e位置，格式如下：\n1 2 COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003c源路径\u003e... \u003c目标路径\u003e COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003c源路径1\u003e\",\"\u003c源路径2\u003e\",... \"\u003c目标路径\u003e\"] 注意：如果源路径是文件夹，那么复制的时候不是直接复制文件夹，而是复制文件夹中的内容到目标路径\nADD ADD相当于是更高级的COPY，在COPY的基础上增加了一些功能。如，\u003c源路径\u003e是一个url的时候，ADD会试图下载这个链接里的文件放到\u003c目标路径\u003e去。但是，这样做需要额外的一个RUN命令去调整权限/解压缩等，所以不推荐这样使用。\n然而！当\u003c源路径\u003e是一个tar包，且压缩格式为gzip，bzip2或xz的情况下，ADD会自动解压缩到\u003c目标路径\u003e下，可能会很有用。\n所以，docker官方推荐，所有的文件复制操作尽量就只用COPY。当且只当用到自动解压缩的时候，才用ADD。\nENV ENV是设置环境变量，比较简单。用法：\n1 2 ENV \u003ckey\u003e \u003cvalue\u003e ENV \u003ckey1\u003e=\u003cvalue1\u003e \u003ckey2\u003e=\u003cvalue2\u003e... 设置完之后，\u003ckey\u003e就可以像正常shell脚本里面的环境变量一样地使用了\nARG ARG是构建参数，不同于ENV的是，ARG所设置的是构建环境的环境变量。所以在未来容器运行的时候，ARG设置的参数是不会存在的。用法：\n1 ARG \u003c参数名\u003e[=\u003c默认值\u003e] 注意\n不要用ARG保存用户密码之类的值，因为在docker history里面还是可以看到所有值的。\nARG指令有生效的范围。如果在FROM之前指定，那么只能作用于FROM指令。在多阶段构建时，需要注意这个问题\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ARG DOCKER_USERNAME=library # 上面设置的ARG只在FROM中生效 FROM ${DOCKER_USERNAME}/alpine # echo无法输出！ RUN set -x ; echo ${DOCKER_USERNAME} # 生效哦 FROM ${DOCKER_USERNAME}/alpine # 在FROM之后使用变量，必须在每个阶段分别指定 ARG DOCKER_USERNAME=library # echo可以输出 RUN set -x ; echo ${DOCKER_USERNAME} WORKDIR WORKDIR用来指定工作目录，用法很简单：\n1 WORKDIR \u003c工作目录路径\u003e 初学者常见错误：\n1 2 3 RUN cd /app # /app/world.txt不存在！ RUN echo \"hello\" \u003e world.txt 正确用法：\n1 2 3 4 5 # WORKDIR指定工作目录 WORKDIR /app # 写入到/app/world.txt RUN echo \"hello\" \u003e world.txt WORKDIR也可以使用相对路径：\n1 2 3 4 5 WORKDIR /a WORKDIR b WORKDIR c RUN pwd # 输出：/a/b/c ","description":"","tags":["docker","dockerfile"],"title":"Get Started with Docker \u0026 Dockerfile","uri":"/posts/engineering/dockerfile/"},{"categories":null,"content":"Paper link: https://www.kdd.org/kdd2019/accepted-papers/view/pythia-ai-assisted-code-completion-system\nAbstract This paper proposes an end-to-end code completion approach call Pythia, which has been deployed as part of Intellicode extension in Visual Studio Code. Pythia is trained on 2700+ Python open source software GitHub repositories and achieves SOTA performance. Pythia uses LSTM to learn the long distance dependencies in code context sequences, which is extracted using in-order depth-first AST traversal. The parameter tuning and deployment related issues are also discussed in the later parts of the paper.\nDataset The authors collected 2700 top-starred Github Python projects in various domains. The dataset contains over 15.8 million method calls. The following is the most method call occurrences:\nThe dataset is divided to development set and test set with a 70-30 ratio on the repo level. Then the development set is split into training and validation sets in the proportion 80-20.\nThe online model is trained using the entire dataset.\nCode Representations The Pythia exploits the partial AST which is derived from code snippets containing method calls and member access expression. The ASTs are serialized to token sequence using in-order depth-first traversal. When extracting training sequence, for each method call, T preceding tokens are used, where T is a tunable parameter.\nFurther, the AST token sequence must be converted to numeric vector to be consumed by LSTM. Word2Vec is used to train the embedding of tokens.\nEmbedding training All tokens are mapped to integers from 1 to V, while the infrequent tokens are removed in order to reduce the vocabulary size. During training, OOVs are mapped to an integer greater than V. . is used as the EOS character.\nMethod names are one-hot encoded as the labels. The task is to predict method names using given code snippets. All tokens are mapped to low-dimentsional vectors with semantic relationships preserved.\nTricks The type of variables is inferred using static analysis methods Import alias is ignored Variable names are normalized to var:\u003cvariable type\u003e format Nerual Code Completion Model LSTM is used to predict completion.\nThe input embedding matrix is reused as the output classification matrix to reduce the number of parameters. Hence, the fully connected layer in before the output is no longer needed.\nModel Training Parallel backpropagation through time algorithm with adam optimizer and mini-batch is used to train the LSTM. In the training, the sequences are split into three buckets by sequence lengths. In each bucket, the lengths of sequences are padded to the maximum sequence length in this bucket.\nAs for the learning rate, the authors scale the learning rate up proportionally to the number of works during the first 4 epochs:\nModel Evaluation Model Deployment To deploy the model to lightweight client devices, the neural network quantization method is used to reduce the number of stored weights. The original Pythia model is trained in IEEE 754 numeric format, which is based on 32-bit float. The numeric format of published model is quantized into 8-bit unsigned integer. For Pythia, the model size is reduced from 152MB to 38MB with quantization. The top-5 accuracy is reduced from 92% to 89%.\n","description":"","tags":["paper-note","code-completion"],"title":"Pythia: AI Assisted Code Completion System","uri":"/posts/deep-learning/pythia-ai-assisted-code-completion-system/"},{"categories":null,"content":"A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning Introduction Current code completion algorithms sufferes from 3 major drawbacks:\nHierarchical structural information of the program is not fully utilized\nIn programs, the semantic relationships can be very long\nUnderuse of the information from related tasks\nIn AST, node's type and value are two closely related attributes - but existing methods predict them separately\nContribution Multi-task learning model for code completion Exploit the hierarchical structure information of program Introduce Transformer-XL to capture long-distance dependencies Motivating example Predicting break Predicting node's type \u0026 value NOTE: borrows the idea from code2vec?\nBackground Language modeling Probability of S:\nN-gram:\nRNN: capture longer dependencies than N-gram\nLSTM/GRU: ease the vanishing gradient problem - uses about 200 context words on average\nTransformer: fixed-length context\n👍Transformer-XL: captures longer context\nMulti-task learning MTL can improve generalization and reduce the risk of over-fitting\nMethod: parameter sharing of hidden layers\nsoft parameter sharing: use the distance of parameters\nhard parameter sharing: hidden layers are shared among tasks, output layers are task-specific\nProposed model Task: predict the probability of next node in AST\nPartial AST encoder: Transformer-XL\nPath2root encoder:\nMTL: hard parameter sharing\noutput layers are specfic\nProgram representation program -\u003e AST -\u003e sequence of nodes\n​\t- in-order depth-first traversal\nNode -\u003e type[value] -\u003e $x = [T;V]$\nPartial AST encoder Transformer-XL is used to encode partial AST. It's a standard Transformer-XL.\nPath2Root encoder The path from predicting node to the root node is extracted. In the above example, the extracted path is {BinOp, Return, body, FunctionDef} in order to predict NameLoad[b]\nIn this part, a bidirectional-LSTM is used as the encoder. The two hidden vectors are concatenated.\nTask-specific output layers Two tasks: predict next node's type and value\nOutput layers: the output of partial AST encoder and path2root encoder are concatenated. Softmax is used to generate the prediction\nTraining loss Because there are multiple tasks, the final loss is calculated using a weighted sum over the task-specific losses\nExperiments Dataset \u0026 Metrics dataset: java/python/js\nMetrics: accuracy\nExperimental setup Vocab size: 50000\nembedding size: type[300], value[1200], total[1500]\n6-layer Transformer-XL, 6 heads, d_h = 64\nSegment length = 50\nPerformance Ablation study Weights ","description":"","tags":["code completion","paper note"],"title":"A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning","uri":"/posts/code-intelligence/a-self-attentional-neural-architecture-for-code-completion-with-multi-task-learning/"},{"categories":null,"content":"命名 变量/常量命名：驼峰\n接口名：XXXer\nGetter/Setter：对于obj.field，getter用obj.Field()，setter用obj.SetField()\n控制结构 if 可以在if中定义变量：\n1 2 3 4 5 6 7 8 9 10 if ok:=isOk();ok { // OK } var v string var err error if v, err = doSomething(); err != nil { // 处理错误 } // 继续 for/switch for有三种情况\n1 2 3 4 5 6 7 8 // 如同C的for循环 for init; condition; post { } // 如同C的while循环 for condition { } // 如同C的for(;;)循环 for { } switch可以替代多个if-else\n1 2 3 4 5 6 7 8 9 10 11 func unhex(c byte) byte { switch { case '0' \u003c= c \u0026\u0026 c \u003c= '9': return c - '0' case 'a' \u003c= c \u0026\u0026 c \u003c= 'f': return c - 'a' + 10 case 'A' \u003c= c \u0026\u0026 c \u003c= 'F': return c - 'A' + 10 } return 0 } 一个case可以有多个条件\n1 2 3 4 switch c { case ' ', '?', '\u0026', '=', '#', '+', '%': return true } 类型判断的典型用法\n1 2 3 4 5 6 7 8 9 10 11 12 13 t = functionOfSomeType() switch t := t.(type) { default: fmt.Printf(\"unexpected type %T\", t) // %T 输出 t 是什么类型 case bool: fmt.Printf(\"boolean %t\\n\", t) // t 是 bool 类型 case int: fmt.Printf(\"integer %d\\n\", t) // t 是 int 类型 case *bool: fmt.Printf(\"pointer to boolean %t\\n\", *t) // t 是 *bool 类型 case *int: fmt.Printf(\"pointer to integer %d\\n\", *t) // t 是 *int 类型 } 函数 返回的结果变量可以先命名\n1 2 3 4 5 6 func nextInt(b []byte, pos int) (value int, nextPos int) { // 直接用 nextPos = 1 // 不带参数的返回，默认返回当前值，即0和1 return } defer defer function(param)可以把function推迟到当前函数结束的时候执行。需要注意的是，如果param也是一个函数，那么param这个函数会先执行\n1 2 3 4 5 func function(p int) { // notDeferred(p)会先被执行 // function推出前，deferred()才会被执行 defer deferred(notDeferred(p)) } 切片Slice 如果把Slice当做参数传入某函数，那么函数对slice的修改，对上层是可见的。即，传进去的类似一个引用，而不是形参。\n1 2 // 典型例子，Read结果会被存在buf中 func (file *File) Read(buf []byte) (n int, err error) 但是：切片自身（其运行时数据结构包含指针、长度和容量）是通过值传递的。参考mySlice = append(mySlice, item)\nMap 如果通过不存在的键来取值，会返回对应类型的零值。\n如果需要区分是不存在，还是存的值本来就是零值，可以使用逗号OK法：\n1 2 3 if re, ok := myMap[key]; ok { // 存在值 } Go中没有集合，一般是使用map[string]bool作为集合类型\n指针 1 2 3 4 5 6 7 type ByteSlice []byte // 必须返回[]byte重新对slice赋值，即s = s.Append(data) func (slice ByteSlice) Append(data []byte) []byte { } // 区别在于，这个Append可以不用赋值，直接可以修改ByteSlice：s.Append(data) func (p *ByteSlice) Append(data []byte) { } 在用的时候，其实都是s.Append()，这是因为使用指针作为接收者的声明形式会被自动识别，在调用的时候会被自动改成(\u0026s).Append()\n接口 Go里面的接口是松散的，只要实现了一个接口里面定义的方法，就相当于实现了这个接口。不需要显式地声明XXX实现了某interface\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type MyInterface { Do() } type MyType struct {} // 实现了Do()，这里就隐式实现了MyInterface接口 func (p *MyType) Do() { } // Do2()可能是其他接口里面的函数 func (p *MyType) Do2() { } 并发 在go里面，多个独立的线程从来不会主动共享：\nDo not communicate by sharing memory; instead, share memory by communicating.\n不要通过共享内存来通信，而应通过通信来共享内存。\nGoroutines goroutine是并发的、轻量级的，使用也很简单：go\n1 go list.Sort() // 并发运行 list.Sort，无需等它结束。 Channels Channel就是上面提到的，goroutine间通信的方式。Channel需要使用make来分配内存：\n1 2 ci := make(chan int) // 整数类型的无缓冲信道 cs := make(chan *os.File, 100) // 指向文件指针的带缓冲信道 这里，缓冲是用来阻塞发送者的，如果没有缓冲，那么在接收者接收到值之前，发送者就一直阻塞；如果有缓冲，那么发送者仅在值被复制到缓冲区前阻塞。\n接收者在接收到相应的值之前会一直阻塞。以上面的Sort为例：\n1 2 3 4 5 6 7 8 9 c := make(chan bool) // 分配一个信道 // 在Go程中启动排序。当它完成后，在信道上发送信号。 go func() { list.Sort() c \u003c- true // 发送信号，什么值无所谓。 fmt.Print(\"sorted\") // 接收者接收到信号之前，不会print }() doSomethingForAWhile() success \u003c-c // 等待排序结束，接收到true ","description":"","tags":["go"],"title":"Effective Go","uri":"/posts/engineering/effective-go/"},{"categories":null,"content":"Git commands This post records some powerful git commands which I was not familiar with.\nMerge a specific commit to another branch 1 2 3 4 5 6 7 8 # Merge a commit to current branch git cherry-pick COMMIT_ID # Merge commits in a range(without start commit) git cherry-pick START_COMMIT_ID..END_COMMIT_ID # Merge commits in a range(with start commit) git cherry-pick START_COMMIT_ID^..END_COMMIT_ID In the commands above, the first 6 chars of START_COMMIT_ID and END_COMMIT_ID are enough.\nFor example, commandgit cherry-pick 496986^...acc9bb is same as git cherry-pick 49698603a9c7d1dfdfef0505b52ab2cd4f7ed1a7^...acc9bb290a39275b59cb18e5eb661563fbf08c43.\n--patch If you want to commit some specific changes, you can use --patch option:\n1 git add --patch \u003cfilename\u003e --patch(or -p for short) is a powerful option, which allows you to pick your interested changes in a file or a commit.\nIf you use --patch or -p, the system will ask you whether to stage changes at every chunk:\nApply this hunk to index and worktree [y,n,q,a,d,j,J,g,/,e,?]? The explanation of those options:\ny - apply this hunk to index and worktree n - do not apply this hunk to index and worktree q - quit; do not apply this hunk or any of the remaining ones a - apply this hunk and all later hunks in the file d - do not apply this hunk or any of the later hunks in the file g - select a hunk to go to / - search for a hunk matching the given regex j - leave this hunk undecided, see next undecided hunk J - leave this hunk undecided, see next hunk e - manually edit the current hunk ? - print help The most commonly used options are y(accept this chunk) and n(ignore this chunk).\n","description":"","tags":["git"],"title":"Powerful Git Commands","uri":"/posts/others/git-commands/"},{"categories":null,"content":"Overview Language Server Protocol (LSP) defines how the development tools and language servers communicate. The development tool performs as a client while all computation is done in backend language server. In LSP, the communication uses JSON-RPC format.\nHow LSP Works? The following image shows how LSP works:\nAs you can see, the communication between the development tool and the language server can be classified into three types: Notification, Request and Response.\nA request requires a response, while the notification does not.\nA response is sent only after the server or the development tool receives a request.\nIf there is no result for a request, a response is still needed, with the result property set to null\nBase Protocol The base protocol contains a header and a content part. The header and content are seperated by \\r\\n\nHere is an example of LSP message:\n1 2 3 4 5 6 7 8 9 10 Content-Length: ...\\r\\n \\r\\n { \"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"textDocument/didOpen\", \"params\": { ... } } Header Part The header consists of header fields, which is a k-v pair seperated by :. Each field ends with \\r\\n.\nFor now, there are only two supported header fields, Content-length: number and Content-type: string, in which the first one is mandatory.\nFor the details of headers, check https://microsoft.github.io/language-server-protocol/specifications/specification-3-14\nContent Part Content part uses JSON-RPC to describe requests, responses and notifications. The charset used is specified in Content-type field in header. By default it's utf-8.\nThe following is the format of messages in TypeScript\nAbstract message First, notification, request and response extend an abstract message format:\n1 2 3 interface Message { jsonrpc: string; } where the LSP uses \"2.0\" as the value of jsonrpc, which is the used JSON-RPC version.\nRequest message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 interface RequestMessage extends Message { /** * The request id. */ id: number | string; /** * The method to be invoked. */ method: string; /** * The method's params. */ params?: Array\u003cany\u003e | object; } Response message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 interface ResponseMessage extends Message { /** * The request id. */ id: number | string | null; /** * The result of a request. This member is REQUIRED on success. * This member MUST NOT exist if there was an error invoking the method. */ result?: string | number | boolean | object | null; /** * The error object in case a request fails. */ error?: ResponseError\u003cany\u003e; } There is an optional error field, the response error is defined as the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // ResponseError definition interface ResponseError\u003cD\u003e { /** * A number indicating the error type that occurred. */ code: number; /** * A string providing a short description of the error. */ message: string; /** * A Primitive or Structured value that contains additional * information about the error. Can be omitted. */ data?: D; } // Error codes export namespace ErrorCodes { // Defined by JSON RPC export const ParseError: number = -32700; export const InvalidRequest: number = -32600; export const MethodNotFound: number = -32601; export const InvalidParams: number = -32602; export const InternalError: number = -32603; export const serverErrorStart: number = -32099; export const serverErrorEnd: number = -32000; export const ServerNotInitialized: number = -32002; export const UnknownErrorCode: number = -32001; // Defined by the protocol. export const RequestCancelled: number = -32800; export const ContentModified: number = -32801; } Notification message 1 2 3 4 5 6 7 8 9 10 11 interface NotificationMessage extends Message { /** * The method to be invoked. */ method: string; /** * The notification's params. */ params?: Array\u003cany\u003e | object; } Beware that a processed notification message must not send a response back.\n$ Notifications and Requests Notifications and requests which start with $/ are protocol implementation dependent, which means this type of notifications and requests might not be implementated in some LSPs. A server or client would ignore the $ notification or send an error message with error code MethodNotFound as the response of $ request.\nActual Protocol There are some examples for the actual protocol used in LSP\nServer Lifetime The lifetime of a server is managed by the client, that is, the client decides when to start and shutdown the server.\nInitialize request This should be the first request sent from client, and before the client receives InitializeResult response from server, no more request and notification should be sent.\nThe initialize request may only be sent once.\nRequest:\nmethod: 'initialize' params: InitializeParams Response:\nresult: InitializeResult error.code error.data Initialized notification The initialized notification is sent from client to server after the client receives the result of initialize request. This notification is used by server to dynamically register capabilities.\nThe initialized notification may only be sent once as well.\nNotification:\nmethod: 'initialized' params: initializedParams Shutdown request The server uses this request to request the server to shutdown(but to not exit). After this request, the client must not send any notifications or request other than exit notification.\nRequest:\nmethod: 'shutdown' params: void Response:\nresult: null error: code and message set in case an exception happens during shutdown request Exit notification A notification to ask the server to exit its process.\nNotification:\nmethod: 'exit' params: void Language Features Completion Request This request is sent from the client to the server to compute completion items at a given cursor position.\nRequest:\nmethod: 'textDocument/completion' params: CompletionParams 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 export interface CompletionParams extends TextDocumentPositionParams { /** * The completion context. This is only available if the client specifies * to send this using `ClientCapabilities.textDocument.completion.contextSupport === true` */ context?: CompletionContext; } export interface CompletionContext { /** * How the completion was triggered. */ triggerKind: CompletionTriggerKind; /** * The trigger character (a single character) that has trigger code complete. * Is undefined if `triggerKind !== CompletionTriggerKind.TriggerCharacter` */ triggerCharacter?: string; } /** * How a completion was triggered */ export namespace CompletionTriggerKind { /** * Completion was triggered by typing an identifier (24x7 code * complete), manual invocation (e.g Ctrl+Space) or via API. */ export const Invoked: 1 = 1; /** * Completion was triggered by a trigger character specified by * the `triggerCharacters` properties of the `CompletionRegistrationOptions`. */ export const TriggerCharacter: 2 = 2; /** * Completion was re-triggered as the current completion list is incomplete. */ export const TriggerForIncompleteCompletions: 3 = 3; } export type CompletionTriggerKind = 1 | 2 | 3; Response:\nresult:CompletionItem[] | CompletionList | null. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 interface CompletionItem { /** * The label of this completion item. By default * also the text that is inserted when selecting * this completion. */ label: string; /** * The kind of this completion item. Based of the kind * an icon is chosen by the editor. The standardized set * of available values is defined in `CompletionItemKind`. */ kind?: number; /** * A human-readable string with additional information * about this item, like type or symbol information. */ detail?: string; /** * A human-readable string that represents a doc-comment. */ documentation?: string | MarkupContent; /** * Indicates if this item is deprecated. */ deprecated?: boolean; /** * Select this item when showing. * * *Note* that only one completion item can be selected and that the * tool / client decides which item that is. The rule is that the *first* * item of those that match best is selected. */ preselect?: boolean; /** * A string that should be used when comparing this item * with other items. When `falsy` the label is used. */ sortText?: string; /** * A string that should be used when filtering a set of * completion items. When `falsy` the label is used. */ filterText?: string; /** * A string that should be inserted into a document when selecting * this completion. When `falsy` the label is used. * * The `insertText` is subject to interpretation by the client side. * Some tools might not take the string literally. For example * VS Code when code complete is requested in this example `con\u003ccursor position\u003e` * and a completion item with an `insertText` of `console` is provided it * will only insert `sole`. Therefore it is recommended to use `textEdit` instead * since it avoids additional client side interpretation. */ insertText?: string; /** * The format of the insert text. The format applies to both the `insertText` property * and the `newText` property of a provided `textEdit`. If ommitted defaults to * `InsertTextFormat.PlainText`. */ insertTextFormat?: InsertTextFormat; /** * An edit which is applied to a document when selecting this completion. When an edit is provided the value of * `insertText` is ignored. * * *Note:* The range of the edit must be a single line range and it must contain the position at which completion * has been requested. */ textEdit?: TextEdit; /** * An optional array of additional text edits that are applied when * selecting this completion. Edits must not overlap (including the same insert position) * with the main edit nor with themselves. * * Additional text edits should be used to change text unrelated to the current cursor position * (for example adding an import statement at the top of the file if the completion item will * insert an unqualified type). */ additionalTextEdits?: TextEdit[]; /** * An optional set of characters that when pressed while this completion is active will accept it first and * then type that character. *Note* that all commit characters should have `length=1` and that superfluous * characters will be ignored. */ commitCharacters?: string[]; /** * An optional command that is executed *after* inserting this completion. *Note* that * additional modifications to the current document should be described with the * additionalTextEdits-property. */ command?: Command; /** * A data entry field that is preserved on a completion item between * a completion and a completion resolve request. */ data?: any } /** * The kind of a completion entry. */ namespace CompletionItemKind { export const Text = 1; export const Method = 2; export const Function = 3; export const Constructor = 4; export const Field = 5; export const Variable = 6; export const Class = 7; export const Interface = 8; export const Module = 9; export const Property = 10; export const Unit = 11; export const Value = 12; export const Enum = 13; export const Keyword = 14; export const Snippet = 15; export const Color = 16; export const File = 17; export const Reference = 18; export const Folder = 19; export const EnumMember = 20; export const Constant = 21; export const Struct = 22; export const Event = 23; export const Operator = 24; export const TypeParameter = 25; } ","description":"","tags":["lsp","language server protocol","code completion","type script"],"title":"Language Server Protocol","uri":"/posts/code-intelligence/language-server-protocol/"},{"categories":null,"content":"Reference: https://liiked.github.io/VS-Code-Extension-Doc-ZH\nI'm working on improve the performance of code completion tools. Recently, I turned to node.js completion. Hence, I have to learning somethings about extension development in visual studio code, in order to develop a demo to validate and test the model.\nGet Started First, install Yeoman and VS Code Extension Generator\n1 npm install -g yo generator-code Then, run yo code and fill out the necessary information, Yeoman will generate a source folder for VSC extension automatically.\nThen, open VSC and enter the plugin folder, press F5, a test VSC window will be opened. Open VSC command window(shift + command(ctrl) + p) and enter Hello World, the VSC will show a notification which says \"Hello World\" in the right-bottom of the test window:\nCongrats! You've successfully run your first extension!\nBut how does it work? We check the created source folder of the VSC extension first:\n. ├── .vscode │ ├── launch.json // Config for launching and debugging the extension │ └── tasks.json // Config for build task that compiles TypeScript ├── .gitignore // Ignore build output and node_modules ├── README.md // Readable description of your extension's functionality ├── src │ └── extension.ts // Extension source code ├── package.json // Extension manifest ├── tsconfig.json // TypeScript configuration Each VSC extension must have a package.json, in which we stores information about the extension, such as name, publisher, activationEvents, etc. Some important keywords are listed here:\nname and publisher: VSC uses publisher.name as the identifier of the extension main: entrance file of the extension activationEvents: the event used to activate the extension contributes: configurations, including user defined configurations In the generated source folder, the entry file is extension.js, now let's have a look:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // The module 'vscode' contains the VS Code extensibility API import * as vscode from 'vscode'; // this method is called when your extension is activated // your extension is activated the very first time the command is executed export function activate(context: vscode.ExtensionContext) { // Use the console to output diagnostic information (console.log) and errors (console.error) // This line of code will only be executed **once** when your extension is activated console.log('Congratulations, your extension \"node-completion\" is now active!'); // The command has been defined in the package.json file // Now provide the implementation of the command with registerCommand // The commandId parameter must match the command field in package.json let disposable = vscode.commands.registerCommand('extension.helloWorld', () =\u003e { // The code you place here will be executed every time your command is executed // Display a message box to the user vscode.window.showInformationMessage('Hello VS World!'); }); context.subscriptions.push(disposable); } // this method is called when your extension is deactivated export function deactivate() {} The entry file exports two functions: activate and deactivate. activate will be executed when the activationEvents are triggered, while deactivate is used to clear in workspace before the extension is closed.\nOur first extension contains three parts actually:\nonCommand activationEvents\nThis is defined in package.json's activationEvents field. After defining this, the user can enter Hello World to activate the extension.\n1 2 3 \"activationEvents\": [ \"onCommand:extension.helloWorld\" ] contributes.commands\nThis is also defined in package.json. This field is used to bind the command ID extension.helloWorld with keyword Hello World. With this we can use Hello World command in VSC's command window.\n1 2 3 4 5 6 7 8 \"contributes\": { \"commands\": [ { \"command\": \"extension.helloWorld\", \"title\": \"Hello World\" } ] } vscode.commands.registerCommand\nThis is the VSC API which binds a function with the command ID extension.helloWorld.\nIn the example above, the command extension.helloWorld is registered with a function of lambda expression\n","description":"","tags":["vscode","extension","typescript","node.js"],"title":"VS Code Extension Development(1) - Get Started","uri":"/posts/engineering/vs-code-extension/"},{"categories":null,"content":"Content Source Organization The top level of content folder is ./content/. Under this folder, the contents can be organized and nested at any level. The following is an example:\nThe page origanization conforms to the content source organization. However, each type of pages has an index page, which is the front page when you goes to this category. For instance, the index page of tags looks like: The correspondence between source file paths and site urls is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Content file path . url . ⊢--^-⊣ . path slug . ⊢--^-⊣⊢---^---⊣ . filepath . ⊢------^------⊣ content/posts/_index.md # Site urls url (\"/posts/\") ⊢-^-⊣ baseurl section (\"posts\") ⊢--------^---------⊣⊢-^-⊣ permalink ⊢----------^-------------⊣ https://example.com/posts/index.html Override Destination Paths in Front Matter In front matter, the slug, url, and layour can be overridden. For example, if a post's path is content/posts/old-post.md, by default, its url would be https://example.com/posts/old-post/. We can set the slug in front matter to change the url:\n1 2 title: New Post slug: \"new-post\" Then the url is changed to https://example.com/posts/new-post/\nThe url can be also overridden by url option in front matter:\n1 2 3 4 title: New Post url: https://example.com/blog/new-url/ # OR url: /blog/new-url/ In the second case, the url will be changed to baseUrl + /blog/new-url/.\nFor more information about url configuration, see URL Management\nSection A section is a collection of pages. Sections are organized under the content/ folder. By default, all first-level folder under content/ are sections. In deeper level, we can create a section by creating a folder with a _index.md file, which indicates that this folder is a section.\nA section cannot be defined or overridden by a front matter parameter – it is strictly derived from the content organization structure.\nHugo provides page variables:\n.CurrentSection .FirstSection .Parent .Section: First path element in the directory .Sections: Sections below this content Content Section vs Content Type Content section is a organization-level concept, while content type is the template of a page, such as draft, post, etc.\nConfig Markdown Renderer The default markdown renderer in Hugo is Blackfriday. There are lots of configurations in config.toml about Blackfriday:\nConfig name Default value Explanation taskLists true Markdown TO-DO List smartypants true Smart punctuation subsitutions, such as (c)-\u003e © fractions true Render fractions in a collapsed format extensions [] Enable Blackfriday's Markdown extensions HrefTargetBlank false Open absolute links in a new window or tab ","description":"","tags":["Hugo"],"title":"Learning Hugo(1)","uri":"/posts/others/learning-hugo/"},{"categories":null,"content":"What's Gremlin? Gremlin is used as the graph traversal language on Apache TinkerPop, which is a graph computing framework.\nBasic Concepts Graph:\nCollection of Vertex and Edge.\nElement:\nCollection of Property.\nVertex:\nInherits from Element. Vertex is generally used to store the Property of entities.\nEdge:\nInherits from Element. Edge is generally used to store the Property of relationship.\nProperty\nA k-v pair, where the key is just like the column name in relational database, and the value is stored data in the Edge or Vertex.\nNote that the key must be a string.\nLabel\nThe class for Vertex or Edge. Vertexes and Edges having the same label means they have the same properties. Label can be regarded as the table name in a workplace in relational database.\nUsage of Gremlin Graph, Edge and Vertex All query must start with a graph.\n1 2 3 4 5 6 7 8 // g represents the whole graph // Get all edges g.E() // Get all vertexes g.V() // Get 100 vertexes g.V().limit(100) Property 1 2 3 4 5 6 7 // Get all properties' names for all vertexes g.V().properties().key() // Get property value of all vertexes with property called \"time\" g.V().properties('time').value() // Same, but simpler g.V().values('time') ID Get vertex or edge by id, or get id\n1 2 3 4 // Get vertexes using id g.V('id1') // Get id for all vertexes g.V().id() Label Get vertex or edge by label\n1 2 3 4 // Get vertexes which have label \"software\" g.V().hasLabel('software') // Get out-edge with label \"develops\" of vertex with id \"46\" g.V('46').outE('develops') Filter In Gremlin, we can use hasXXX to filter specific edges or vertexes. Gremlin provides the following commands:\nhas(key, value): filter by property's key and value\nhas(label, key, value): filter by label, and property's key and value\nhas(key, expr): filter by label and an expr. For example, using g.V().has('age', gt(20)) we can get vertexes which have age property and whose value of age is larger than 20.\nhasLabel(label1, label2, ...): filter by label. Elements with label in any one of the label list will pass the filter.\nhasId(id1, id2, ...): filter by id, similar with hasLabel\nhasKey(key1, key2, ...) and has(key): filter by key\n","description":"","tags":["Gremlin","TinkerPop","Graph Database"],"title":"Get Started With Gremlin","uri":"/posts/others/get-started-with-gremlin/"},{"categories":null,"content":"Learning Alfred workflow python sdk.\nInstall Alfred workflow python sdk first: https://www.deanishe.net/alfred-workflow/installation.html\nAdd script runner There are two components in Alfred which can execute a python script: script filter and run script. There are a few differences between script filter and run script, we'll talk about it later.\nThe usage of both components is similar: Note that the Alfred workflow python sdk supports only Python2, since the built-in Python interpreter in Mac OS is Python2.\nYour first python script The following is a simple example of Alfred Python script:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/usr/bin/python # -*- coding: utf-8 -*- from __future__ import print_function import sys import os def main(wf): # If the input {query} is not null if len(wf.args): # pass {query} to downstream print(wf.args) if __name__ == '__main__': wf = Workflow3() sys.exit(wf.run(main)) This script passes the input {query} to downstream. In Alfred, all contents which is printed to stdout will be passed to downstream as {query}.\nVariables If we want to pass more information to downstream, Alfred variable is what we need.\nSet variables There are two types of variables: general variable and environment variable. Alfred Python sdk provides difference methods to set different variables. What's more, variable setting is different in run script component and script filter component.\nSet environment variable Alfred Python provides set_config in workflow.util to set environment variables. Here is the API doc, which is quite easy to understand and use.\n1 2 3 4 5 6 7 from workflow.util import set_config, unset_config # Set environment variable set_config(var_name, var_value, bundleid, exportable=False) # Unset environment variable unset_config(var_name, bundleid) Set variables in script filter component In the upstream script filter, root variable and item variable can be used to pass variables to downstream.\n1 2 3 4 5 6 # Add item variable it = wf.add_item(title, subtitle, arg, valid) it.setvar(var_name, var_value) # Add root variable wf.setvar(var_name, var_value) wf.add_item adds an item, which is a collection of variables as well as one candidate in alfred window. When the candidate is chosen, the item is actioned.\nWhen an item is actioned, all variables in arg is passed as {query} to downstream\ntitle and subtitle is the text appears in the alfred window.\nSet variables in run script component In run script component, Variables should be used to store and pass variables.\nHere is an example, we initialize a variable called fail, and pass it to downstream:\n1 2 3 4 5 6 from workflow import Variables if not success: v = Variables() v[\"fail\"] = \"Fail to run script\" print(v) return Get variables In the downstream script, os.getenv(var_name) is used to get variables:\n1 2 import os var_value = os.getenv(var_name) In other components of Alfred, we can use {var:var_name}.\nThe following is an example:\nIf the variable called fail is empty, the downstream component \"open file\" will be executed with the input {query}, where {query} is the file path.\nLogger In Alfred Python, we cannot use print to debug because the printed content will be passed to downstream as {query}. But don't worry, the sdk provdes a logger wf.logger\nLogger initialization:\n1 2 3 4 5 6 7 8 from workflow import Workflow3 log = None if __name__ == '__main__': wf = Workflow3() log = wf.logger sys.exit(wf.run(main)) Then we can use log.error(), log.warn(), log.debug(), log.info() to debug. All info will be printed in debug window when the script is running:\n","description":"","tags":["alfred"],"title":"Alfred Workflow Python Development","uri":"/posts/others/alfred-workflow-python/"},{"categories":null,"content":"Create SSH key First, create ssh key:\n1 ssh-keygen -t rsa -C \"yourmail@mail.com\" Then add the created private key to ssh-agent, and store it in keychain:\n1 ssh-add -K ~/.ssh/id_rsa -K can make sure that the ssh key will always be valid after restart. You can use ssh-add -l to check added ssh keys\nConfig We can use different ssh keys to connect different servers.\nThe configuration file is ~/.ssh/config\nHere is an example config file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Use id_rsa_github to connect to git@github.com Host myhost HostName github.com User git PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa_github # Use default ssh key to connect to git@gitlab.com Host myhost2 HostName gitlab.com User git PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa # Use id_rsa_myserver to connect root@12.34.56.78 Host myhost3 HostName 12.34.56.78 User root PreferedAuthentications publickey IdentityFile ~/.ssh/id_rsa_myserver If the server is not in config file, the default ssh key is ~/.ssh/id_rsa\nGithub setting Please refer to Github Help\n","description":"","tags":["linux"],"title":"SSH on Mac OS","uri":"/posts/engineering/ssh/"},{"categories":null,"content":"Before we start to develop an Alfred workflow, we should learn some basic concepts first.\nPrerequisites: https://www.alfredapp.com/help/workflows/\nWorkflow development basics There are five categories of workflow node in Alfred: Triggers, Inputs, Actions, Utilities, Outputs\nThe two most important categories are Inputs and Actions. The first one read inputs from various sources, and the latter one are what does the workflow do.\nInputs There are 5 types of inputs:\nKeyword\nFile filter\nFind a file, pass file_path as the {query}\nDictionary Filter\nList filter\nScript Filter\nFile Filter File types in file filter is used to narrow the scope of searched files. For example, a png file's file types tree (from precise classification to broadest classification) can be:\npublic.png -\u003e public.image -\u003e public.data -\u003e public.item -\u003e public.content\nScript Filter The most used input filter. A bash script is used to process the input, including execute a python/ruby/perl/go/... program. Hence, in most cases, we can do all what we want to do in script filter.\nActions There are 12 actions in Alfred, including open file/app/url, run system command, terminal command, etc. If the given actions cannot satisfy your requirement, Alfred also provide an action called run script. Just like a script filter. But there are a few differences between script filter and run script. We'll talk about it latter.\nVariables Please read https://www.alfredapp.com/help/workflows/advanced/variables/ as the prerequisites first.\nVariables are just like variables in programming, which stores values, results and informations.\nIn Alfred, we can use {var:var_name} to get the value of variable var_name. BTW, {query} can be also regarded as a special variable.\nA general variable is valid for node in downstream, and the environment variable is always valid in the workflow.\nPlaceholder Alfred offers dynamic placeholders, which allow you to insert dynamically-created content. All variables can be accessed using placeholder. The placeholder's format is {placeholder:variation}, where variation is used to specify the advanced options when Alfred replaces the placeholder with the dynamic content.\nThere are some useful placeholders, all of them have variations, for those details, pls refer to this page\n{query} {var:var_name}, {allvars} {date}, {time}, {datetime} {clipboard:0}, {clipboard:n}: n-th item in clipboard {random} What's more, placeholders have modifiers, which provides more possibilities. The modifiers follow this format: {placeholder:variation.modifier}\nThe following are allowed modifiers:\nuppercase, lowercase, captitals, capitalcase\ntrim\nreverse\nstripdiacritics: remove accented characters\nstripnonalphanumeric: remove non-alphanumeric characters\nDebugger Debugger is quite useful in the workflow development. It prints values you want to debug window. By default, it prints {query} and {allvars}.\n","description":"","tags":["alfred"],"title":"Alfred Workflow Basics","uri":"/posts/others/alfred-workflow-basics/"},{"categories":null,"content":"常用命令 Normal Mode 按键 含义 \u003e\u003e 向右缩进 \u003c\u003c 向左缩进 ctrl w + hjkl 跳转窗口到方向 ctrl w + w 下一个窗口 Command Mode 命令 含义 :sp 上下拆分窗口 :vsp 左右拆分窗口 搜索和替换 替换的语法：范围s/original_text/replacement/替换选项\n替换当前行：s/orginal_text/replacement/g，g为global全局替换，即替换范围内的所有目标\n替换范围 替换所有行只需要在前面加个%，即%s/orginal_text/replacement/g\n选择区域替换：先ctrl+v进入Visual Mode，选择完区域后输入:进入命令行模式，范围就自动添加了，后面一样。也可以手动指定，如：2,12s/original_text/replacement/g搜索2到12行；.,+5s/original_text/replacement/g表示搜索当前行.和接下来5行+5\n替换选项 g：global替换\ni/I：大小写不敏感/敏感\nc：需要确认\nCoC 一般来说，在coc.nvim中，tab选择，回车或者继续打字确定\n一些其他的增强功能如下表：\n注意，其中\\为\u003cleader\u003e键，在vimrc中，\u003cleader\u003er即为\\r。Leader键一般用来表示自定义的一些快捷键\n按键 含义 gd 转到定义 gy 转到类型的定义 gi 转到实现 gr 转到reference \\r 替换下一个单词 \\f format选定的区域 ","description":"","tags":["vim"],"title":"Vim 命令","uri":"/posts/others/vim/"},{"categories":null,"content":"What's Hugo? Hugo is a popular open-source static site generator. After my painful journey on trying Hexo and Jekyll, Hugo makes me happy.\nGet Started Install Hugo on Mac OS. 1 brew install hugo Initialize the first website. 1 hugo new site your-folder-name Set Github repo I'm using two repos for my blog: one stores all source files of hugo and the other stores generated websites.\nFirst, create a repo for source files in Github.\nFor example, my source repo is github.com/HaoboGu/source-haobogu.github.io.\nAfter the repo for source files is created, set local hugo folder to track the repo.\n1 2 3 4 5 6 7 8 9 10 cd your-folder-name # Git initialization git init # Commit hugo files git add . git commit -m \"first commit\" # Track your remote repo git remote add origin git@github.com:HaoboGu/source-haobogu.github.io.git git push -u origin master Add a theme Hugo doesn't have a built-in theme, so I decided to use hugo-nuo(my edited version). You can choose one from themes.gohugo.io/.\n1 2 3 4 5 # Import the hugo-nuo theme as a git submodule git submodule add git@github.com:HaoboGu/hugo-nuo.git themes/hugo-nuo # Add theme to config file echo 'theme = \"hugo-nuo\"' \u003e\u003e config.toml Add first page 1 hugo new posts/Hello-Hugo.md You may notice that there is a generated header in the markdown file. This header is called front matter. By default, this page will not be publised because draft = true option in front matter template.\nTest your site locally Run\n1 2 # -D means draft will be publised hugo server -D Then navigate to your site at http://localhost:1313/.\nDeploy site to Github Pages Create your Github Pages repo for hosting generated site.\nSuppose the site repo is github.com/HaoboGu/haobogu.github.io, then run\n1 2 3 4 # Delete generated public folder rm -rf public # Set public folder as a submodule which tracks Github Pages repo git submodule add -b master git@github.com:\u003cUSERNAME\u003e/\u003cUSERNAME\u003e.github.io.git public Create script deploy.sh, then run this script to deploy the website\n1 2 3 chmod +x deploy.sh ./deploy.sh DONE!\n","description":"","tags":["hugo"],"title":"Get Started with Hugo","uri":"/posts/others/get-started-with-hugo/"},{"categories":null,"content":"Blender 常用快捷键 最常用 g：移动 r：旋转 s：缩放 xyz：轴向。比如想要按照x轴移动，那么就gx即可；如果想要x轴不变，在yz轴向上移动，则用g shift+x shift+r：重复上一步操作 模式切换 tab：进入网格编辑模式 游标 shift + s：设定游标到当前位置 编辑模式 在物体模式（默认模式）下按tab可以进入编辑模式之后，可以在左上角选择点线面模式。左上角会显示当前所处的模式，也可以手动切换：\n可以按shift同时选择点、线、面模式。\n统计信息 如果想看当前视图中选择了多少个点线面的统计，可以在视图叠加层选择统计信息：\n如果没有物体被选择显示的是所有物体的和。如果有物体被选择，那么进入编辑模式，就显示当前选中的所有物体的统计信息。\n挤出 进入编辑模式，然后按快捷键E，即可挤出。\n快速选择挤出的选项：alt(option)+E\n倒角 默认快捷键是 ctrl/cmd + B，鼠标滚轮可以控制分段数。注意可以在边模式中单独对某个边做倒角，在点模式可以对某个点/角做倒角。\n在左键确认之后，会弹出菜单。菜单里面有一些倒角的选项。可以注意有3大主要参数：宽度、段数和形状。宽度就是倒角的范围，段数就是分成多少段，形状是0~1的参数。\n参数：\n宽度类型：看下图\n材质：倒角的材质。默认-1会选择相邻的面的材质，大概是这样：\n材质可以选择0123这样的数字，这个数字代表在材质界面的顺序。\n硬化倒角：光滑倒角。类似右键-平滑着色。可以让面发现大于xx度的倒角光滑起来\n外斜接、内斜接：倒角和其他面相邻的时候的相交形状\n在倒角的时候，需要注意物体是否被缩放了。缩放有可能导致导致倒角的不均匀。如果确实需要有缩放的，在调整完之后可以右键-全部应用，这样可以看到左边的缩放都变成了1，这样再倒角就均匀了。\n循环切割 一些基础概念 并排边 首先，只有四边形有并排边（即对边）。三角形没有，而多边形没有办法确定唯一的对边。因此在考虑并排边的时候只考虑四边形（四边面）。\n循环边 在网格棋盘中，一条边可以往下传递，为循环边。一个边的循环边不能和当前边直接组成四边形（里面不包含其他边，如下图垂直的边都不行）。循环边遇到极点或边界边的时候会停止：\n白色光标的位置就是极点，可以看到从极点没办法往下传递了，因为系统不知道是选择直着传还是往右上角走。\n循环边切割 在本质上，循环切割就是在对一对并排边进行细分（分成两个边），然后连接两个点对当前面进行切割，然后再对切割的这条边进行循环传递就是循环切割。如下示意图：\n可以看到橘黄色标注的边都是并排边，而亮黄色的就是循环边切割。切割的都是四边形。\n环切工具 首先还是快捷键：ctrl/cmd + R。按左键确认，然后可以调整边线。\n在blender中，循环边切割工具还有其他的一些额外选项：\n切割次数：就是切割多少次，切几刀。在预切割状态（按完cmd+R），可以使用鼠标的滚轮进行调整 平滑度、衰减、系数：切完之后会对形状有影响。这些参数就是调整切割后的形状的 钳制：强制系数在-1~1，即不会超过当前的物体边缘。否则可能会超过 均匀、翻转：有时候并排边并不是垂直的，使用这两个选项可以让切割线和左边或者右边一样。 坐标系 在blender中，坐标系的选择在顶部这里（快捷键为,）\n在这里我们有若干选择，下面分别说明。\n全局：就是固定不变的坐标系，类似于东南西北\n局部：针对选中物体的朝向的坐标系，类似前后左右\n法向：某个面的法线方向为主的坐标系\n万向：其中两个轴是固定的，第三个轴可以旋转。想象一下万向轮，把x轴当成旋转轴，轮子就绕着x轴转；然后z轴是转向轴，转动的方向可以变\n对于万向坐标系来说，在右侧的变换-模式中可以选择XYZ的类型。YXZ的意思就是，y轴绕x轴旋转，而xy轴绕z轴旋转。\n视图：以当前视窗（屏幕）为主的坐标轴。左右为x，上下为y，前后为z\n游标：就是blender刚开始最中心的那个游标的坐标系。游标是可以移动的且有朝向的。如果选择了游标坐标系，那么当前物品的坐标系就和游标是一样的了\n原点 每个物品都有一个原点。对模型的操作，如缩放、旋转、归零等实际上都是基于原点的。一般来说是物体的质心。在Blender中可以随意移动原点，只需要在右上角的选项里面勾选仅影响原点即可：\n当然，如果你不小心移动了原点，也可以通过 选中物体 - 右键菜单 - 设置远点来重设原点\n边界框中心 在blender里面，还有一个边界框中心比较容易和原点混淆。边界框中心是每个物体的边界框（立方体）的中心。\n轴心点 吸附 开关 打开吸附：单击顶部菜单栏的磁铁图标，或者快捷键shift+tab:\n临时打开/关闭：在移动模式下，按住ctrl，就可以切换吸附模式的开关；松开恢复。\n吸附选项 blender有如下若干个吸附选项。需要注意的是，吸附至选项都是针对被吸附对象来说的：\n而吸附基准点，是吸附对象的点如质心、边缘等。\n","description":"","tags":null,"title":"Haobo's Blog","uri":"/posts/keyboard/blender/"},{"categories":null,"content":"Keeb 一些 Essentials 对于键盘来说，从用户的角度，是由按键组成的（编码器/虚拟按键等，暂不考虑）。\n一个一个的按键，组成了键盘的布局，用户根据布局操作每一个按键。 因此，layout是最上层的概念。对于软件来说，layout包含如下特性：\nlayout共有多少行、列 layout的每一个位置和PCB中的矩阵的对应关系，哪些位置有按键，哪些没有。 layout存的是键位的设置，而每个键位的具体功能，则存在和layout对应的keymap中。 在Keymap中，存储的是每个按键的按下后的具体功能 Action 。一个keymap可以有多个layer，keymap和layer的关系，可以参考 https://docs.qmk.fm/#/keymap 。\n在实际用户使用的时候，首先是要获取用户触发的键位 position ，然后判断用户的 TriggerType（触发方式）。 对于键盘来说，每一个按键可以有如下的触发方式：\n按下/抬起：最普通的触发方式 tap：把快速按下+抬起组合起来，看做是一个动作。只有完成了整个的tap动作，才会触发。tap一般都有一个时间上限，在时间上限内完成 按下+抬起，可以识别为tap tapN：连续N次tap hold：长按，有一个最低时间。 NO：没有动作，默认的 然后，根据对应 keymap + position 里面存储的 Action，再加上 TriggerType，就能得到键盘最终执行的动作。\n即 Position + TriggerType + Action = What's executed by keyboard(aka KeyboardAction).\n然后，键盘 dispatch 所有的 KeyboardAction，去执行相关操作。\nDefinitions keycode：键码\n键码应该有 2 套： 系统支持的键码 内部的键码，包含系统支持键码 + 内部功能（比如切层、RGB、音频控制等等） layout：布局\n需要支持KLE 要点：键位、PCB矩阵位置、形状 \u0026 角度 layout需要和具体的键位position绑定。然后，每个键位有X种触发方式，每一种触发方式有Y种功能。 matrix：PCB矩阵，和layout存在有限的对应\nlayer：层 QMK中，一共支持32层，可以同时激活多个层，多个层同时激活的时候，会从高往低逐个扫描，如果是KC_TRANS，就进到低层，直到在某层有一个有效的按键。支持如下切层功能：\nDF(layer): 设置默认层，默认层默认一直是激活状态。。默认层一般都是第0层，也就是最下面的一层 MO(layer): 临时激活某一层，比如MO(1)就是临时激活第1层，松开这个按键的时候失效 LM(layer, mod): 也是临时激活某层，区别是LM自带某个modifier（不用手再去按了） LT(layer, kc): 双功能，长按是激活某一层，单击是触发对应的keycode OSL(layer): 按一下临时激活某层，激活状态在下次按键失效。和MO(layer)的区别就是，MO(layer)需要保持按下激活某一层，而OSL(layer)是一次性的，只对下一个按键有效 TG(layer): 切换某层的激活状态。上面的都是临时激活层，而TG是永久切换层的激活状态(开 -\u003e 关/关 -\u003e 开) TO(layer): 激活某一层，关闭其他所有层。这个在按下的那一刻起就生效了。 TT(layer): 按下临时激活某层（和MO一样），但是增加了一个功能是，如果连续单击TT，则可以把这一层永久激活或关闭（相当于TG）。换句话说，TT是双功能，等于MO(按下) + TG(多击)。多击次数可以通过 TAPPING_TOGGLE设置，如在QMK中，设置#define TAPPING_TOGGLE 2，就是把TT设置成双击激活。 macro：宏\naction：键盘的动作 除了基础的发送键码之外，键盘的按键还有其他很多功能，如：\n层操作：临时切层/激活\u0026失活某层/层+modifier/层的tap触发 modifier tap 媒体控制（播放、暂停、音量） 系统控制（关机） 这些功能并不能被键盘的keycode覆盖，统称为 keyboard action 另外，再做一层抽象：人操作键盘有几种方式：\n按下/抬起：最正常的方式，按下和抬起分别算 tap：也就是按下 + 抬起触发 hold：长按 连续tap：qmk中的tap dance 组合键：同时按下多个按键，可以是 modifier + key，也可以是key + key 在上面的这些，可以理解成是human action\ndebounce\n协议层：支持via、vial\n驱动层：trait for GPIO、LED、RGB、Encoder等等\ntools：快捷生成、快捷接入等\ncli：命令行工具\nKeycode System keycode 首先是系统接受的键码，这些键码是国际标准定义好的，可以参见QMK的文档。\ntap key: 按一下就触发一次\nLayer QMK一共支持32层，层的操作上面有写。在具体实现上，\n由于每层都是独立的，因此层的开启和关闭，主要使用bitwise operation来实现。 层 + modifier需要单独实现 层的 tap 操作需要单独实现 About Rust 变量 rust中变量默认不可变，要可变需要加mut\nrust中，还可以用 cosnt 去声明一个常量。声明const的时候，必须指定类型。const一般用大写字母表示。\n基础类型 数字 对于数字和基础的变量，rust支持多种进制的表示。还可以添加_作为虚拟分隔符（主要是为了可读性）。\nNumber literals Example Decimal 98_222 Hex 0xff Octal 0o77 Binary 0b1111_0000 Byte (u8 only) b'A' 浮点数、bool f32 \u0026 f64 \u0026 bool，这些通用的，基本都差不多\nTuple Rust里面也有tuple，用 () 括起来\n1 let tup: (i32, f64, u8) = (500, 6.4, 1); 还能解组\n1 let (x, y, z) = tup; Tuple取值，使用 .：let x = tup.0\narray Array 是一个序列，和tuple的区别是，Array中的数据类型必须一致。\nArray还有一个特点：它被分配在stack上，其长度是固定的\n1 let a: [i32; 5] = [3; 5]; Array的取值，和tuple也不同：let x = a[0]\n直接取的话，数组越界会直接panic，需要注意\n","description":"","tags":null,"title":"Haobo's Blog","uri":"/posts/keyboard/keeb/"},{"categories":null,"content":"openocd配置 在openocd里面配置ospi，只能直接写寄存器。因此需要首先了解一下寄存器的配置\nospi1的起始位置是 0x58005000\nOCTOSPI_CR寄存器 CR即control register。是ospi最基础的配置。\nAddress offset: 0x0000 Reset value: 0x0000 0000\n一共32Bit\n其中比较重要的：\nFMODE：28-29，配置了ospi的模式，有 00： Indirect-write mode 01: Indirect-read mode 10: Automatic status-polling mode 11: Memory-mapped mode 在使用ospi flash的时候，使用11\n配置：\n1 mww 0x52005000 0x30400003 ;# OCTOSPI_CR: FMODE=0x11, APMS=1, FTHRES=0, FSEL=0, DQM=0, TCEN=1 (OCTOSPI_DCR1 Device configuration register 1，设备配置寄存器1\n1 mww 0x52005008 0x01160100\t;# OCTOSPI_DCR1: MTYP=0x1, FSIZE=0x19, CSHT=0x01, CKMODE=0, DLYBYP=0 MTYP: 001: Macronix mode\nDEVSIZE: Number of bytes in device = 2[DEVSIZE+1]. 因此，在我们这里使用8M的情况下，应该是2^23，即devsize=22=0x16\nCTOSPI communication configuration register (OCTOSPI_CCR) DMODE: 011 data 4line mode\n","description":"","tags":null,"title":"Haobo's Blog","uri":"/posts/keyboard/ospi/"},{"categories":null,"content":"RM67162 OLED 使用RM67162驱动OLED屏幕的各种方案。\n4线SPI 首先，4线SPI比3线多了一个D/C（数据、命令）选择线，即一共4个IO\n时钟SCLK/SCL 主机输出从机输入 MOSI（在从机端叫SDI） 从机输出主机输入 MISO（在从机端叫SDO） 命令、数据选择：DCX 另外，再加上一个片选信号CS（或者叫NCS），一共5条数据线。\n在STM32H7中，可以使用OSPI外设作为4线SPI的主机。无论是选择3线SPI还是4线SPI，在CubeMX中都应选择Single SPI：\n由于我们使用了OSPI驱动外置Flash，而STM32H7的外设中，没有办法同时使用OSPI1和OSPI2的片选、时钟两条线，因此直接使用普通SPI，然后手动设置片选和D/C选择线成为了唯一的可行方式（普通SPI会自动配置好SCLK、MOSI和MISO）。\nTODO：普通SPI的硬件NSS（即片选）是否能用？\n","description":"","tags":null,"title":"Haobo's Blog","uri":"/posts/keyboard/rm67162/"},{"categories":null,"content":"STM32CubeMX 配置 STM32CubeMX新工程配置备忘。\n步骤 新建工程，选择MCU\n配置DEBUG端口\n配置RCC晶振\n配置时钟树\n配置USB\n配置其他外设\n配置工程信息，使用Makefile、为每个外设生成源文件\n生成整个工程\n然后，去其他工程下把openocd.cfg、STM32H7xxxx.svd复制到工程目录下。\n在VSCode下配置，task.json：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"make\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"make all -j 16\", \"problemMatcher\": { \"owner\": \"cpp\", \"fileLocation\": \"autoDetect\", \"pattern\": { \"regexp\": \"^(.*):(\\\\d+):(\\\\d+):\\\\s+(warning|error):\\\\s+(.*)$\", \"file\": 1, \"line\": 2, \"column\": 3, \"severity\": 4, \"message\": 5 } } }, { \"label\": \"make clean\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"make clean\" }, { \"label\": \"flash\", \"group\": \"build\", \"type\": \"shell\", \"command\": \"openocd -f openocd.cfg -c \\\"program build/注意这里是你的工程名.elf preverify verify reset exit\\\"\" } ] } 和launch.json：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Cortex Debug\", \"cwd\": \"${workspaceFolder}\", \"executable\": \"./build/f411.elf\", \"request\": \"launch\", \"type\": \"cortex-debug\", \"runToEntryPoint\": \"main\", \"servertype\": \"openocd\", \"showDevDebugOutput\": \"parsed\", \"configFiles\": [ \"openocd.cfg\" ], \"svdFile\": \"这里是你的SVD文件.svd\", \"device\": \"stlink\", \"preLaunchTask\": \"make all\" } ] } ","description":"","tags":null,"title":"Haobo's Blog","uri":"/posts/keyboard/stm32cubemx/"}]
